<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.8/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.8/ http://www.mediawiki.org/xml/export-0.8.xsd" version="0.8" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <base>http://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.23wmf19</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="446" case="first-letter">Education Program</namespace>
      <namespace key="447" case="first-letter">Education Program talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Apache Ant</title>
    <ns>0</ns>
    <id>438891</id>
    <revision>
      <id>601064944</id>
      <parentid>597520350</parentid>
      <timestamp>2014-03-24T17:59:30Z</timestamp>
      <contributor>
        <username>Ayush3292</username>
        <id>20379848</id>
      </contributor>
      <comment>Changed the stable release date and version</comment>
      <text xml:space="preserve" bytes="16004">{{more footnotes|date=August 2010}}
{{Infobox software
| name                   = Apache Ant (Another Neat Tool)
| logo                   = [[File:Apache-Ant-logo.svg|200px|Apache Ant Logo]]
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| released               = {{Start date and age|df=yes|2000|7}}
| latest release version = 1.9.3
| latest release date    = {{release date|2013|12|29}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Build tool]]
| license                = [[Apache License]] 2.0
| website                = {{url|http://ant.apache.org}}
}}
'''Apache Ant''' is a software tool for [[build automation|automating software build]] processes.  It is similar to [[make (software)|Make]] but is implemented using the [[Java (programming language)|Java]] language, requires the Java platform, and is best suited to building Java projects.

The most immediately noticeable difference between Ant and Make is that Ant uses [[XML]] to describe the build process and its dependencies, whereas Make uses [[Makefile#Makefiles|Makefile format]]. 
By default the XML file is named &lt;code&gt;build.xml&lt;/code&gt;.

Ant is an [[Apache Software Foundation|Apache]] project. It is [[open source software]], and is released under the [[Apache Software License]].

==History==
Ant (&quot;Another Neat Tool&quot;&lt;ref&gt;[http://ant.apache.org/faq.html#ant-name ''Why do you call it Ant?''], Apache Ant FAQ&lt;/ref&gt;) was conceived by [[James Duncan Davidson]] while preparing [[Sun Microsystems|Sun]]'s [[reference implementation|reference]] [[JavaServer Pages|JSP]]/[[Servlet]] engine, later [[Apache Tomcat]], for release as [[open source]]. A [[proprietary software|proprietary]] version of ''make'' was used to build it on the [[Solaris Operating Environment]], but in the open source world there was no way of controlling which platform was used to build Tomcat; so Ant was created as a simple platform-independent tool to build Tomcat from directives in an XML &quot;build file&quot;. Ant (version 1.1) was officially released as a stand-alone product on July 19, 2000.

Several proposals for an Ant version 2 have been made, such as AntEater by [[James Duncan Davidson]], Myrmidon by [[Peter Donald (programmer)|Peter Donald]] and Mutant by [[Conor MacNeill (programmer)|Conor MacNeill]], none of which were able to find large acceptance with the developer community.&lt;ref&gt;[http://codefeed.com/blog/?p=98 Conor MacNeill -- The Early History of Ant Development]&lt;/ref&gt;

At one time (2002), Ant was the build tool used by most Java development projects.&lt;ref&gt;Java Tools for eXtreme Programming, Wiley, 2002: 76&lt;/ref&gt; For example, most [[open source]] Java developers include build.xml files with their distribution.{{Citation needed|date=March 2010}}

Because Ant made it trivial to integrate [[JUnit]] tests with the build process, Ant made it easy for willing developers to adopt [[test-driven development]], and even [[Extreme Programming]].

==Sample &lt;code&gt;build.xml&lt;/code&gt; file==
Below is listed a sample build.xml file for a simple Java &quot;Hello, world&quot; application.  It defines four targets - ''clean'', ''[[clobber (computing)|clobber]]'', ''compile'' and ''jar'', each of which has an associated description.  The ''jar'' target lists the ''compile'' target as a dependency.  This tells Ant that before it can start the ''jar'' target it must first complete the ''compile'' target.
&lt;syntaxhighlight lang=&quot;xml&quot;&gt;
&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;project name=&quot;Hello&quot; default=&quot;compile&quot;&gt;
    &lt;target name=&quot;clean&quot; description=&quot;remove intermediate files&quot;&gt;
        &lt;delete dir=&quot;classes&quot;/&gt;
    &lt;/target&gt;
    &lt;target name=&quot;clobber&quot; depends=&quot;clean&quot; description=&quot;remove all artifact files&quot;&gt;
        &lt;delete file=&quot;hello.jar&quot;/&gt;
    &lt;/target&gt;
    &lt;target name=&quot;compile&quot; description=&quot;compile the Java source code to class files&quot;&gt;
        &lt;mkdir dir=&quot;classes&quot;/&gt;
        &lt;javac srcdir=&quot;.&quot; destdir=&quot;classes&quot;/&gt;
    &lt;/target&gt;
    &lt;target name=&quot;jar&quot; depends=&quot;compile&quot; description=&quot;create a Jar file for the application&quot;&gt;
        &lt;jar destfile=&quot;hello.jar&quot;&gt;
            &lt;fileset dir=&quot;classes&quot; includes=&quot;**/*.class&quot;/&gt;
            &lt;manifest&gt;
                &lt;attribute name=&quot;Main-Class&quot; value=&quot;HelloProgram&quot;/&gt;
            &lt;/manifest&gt;
        &lt;/jar&gt;
    &lt;/target&gt;
&lt;/project&gt;
&lt;/syntaxhighlight&gt;
Within each target are the actions that Ant must take to build that target; these are performed using built-in ''tasks''.  For example, to build the ''compile'' target Ant must first create a [[directory (file systems)|directory]] called ''classes'' (which Ant will do only if it does not already exist) and then invoke the Java compiler. Therefore, the ''tasks'' used are ''mkdir'' and ''javac''. These perform a similar task to the command-line utilities of the same name.

Another task used in this example is named ''jar'':
&lt;syntaxhighlight lang=&quot;xml&quot;&gt;
 &lt;jar destfile=&quot;hello.jar&quot;&gt;
&lt;/syntaxhighlight&gt;
This Ant task has the same name as the common Java command-line utility, [[JAR (file format)|JAR]], but is really a call to the Ant program's built-in JAR/ZIP file support. This detail is not relevant to most end users, who just get the JAR they wanted, with the files they asked for.

Many Ant tasks delegate their work to external programs, either native or Java. They use Ant's own &lt;exec&gt; and &lt;java&gt; tasks to set up the command lines, and handle all the details of mapping from information in the build file to the program's arguments and interpreting the return value. Users can see which tasks do this (e.g. &lt;cvs&gt;, &lt;signjar&gt;, &lt;chmod&gt;, &lt;rpm&gt;), by trying to execute the task on a system without the underlying program on the path, or without a full [[Java Development Kit]] (JDK) installed.

==Extensions==

WOProject-Ant&lt;ref&gt;[http://www.objectstyle.org/confluence/display/WOL/WOProject-Ant WOProject-Ant - WOProject / WOLips - Confluence&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; is just one of many examples of a ''task'' extension written for Ant. These extensions are put to use by copying their jar files into ant's ''lib'' directory. Once this is done, these extension tasks can be invoked directly in the typical ''build.xml'' file. The WOProject extensions allow [[WebObjects]] developers to use ant in building their frameworks and applications, instead of using [[Apple Computer|Apple's]] [[Xcode]] suite.

Antcontrib&lt;ref&gt;[http://ant-contrib.sourceforge.net Ant-Contrib]&lt;/ref&gt; provides a collection of tasks such as conditional statements and operations on properties as well as other useful tasks.&lt;ref&gt;[http://ant-contrib.sourceforge.net/tasks/tasks/index.html Ant-Contrib Tasks]&lt;/ref&gt;

Ant-contrib.unkrig.de&lt;ref&gt;[http://ant-contrib.unkrig.de ant-contrib.unkrig.de]&lt;/ref&gt; implements tasks and types for networking, [[Swing_(Java)|Swing]] user interfaces, [[JSON]] processing and other.

Other task extensions exist for [[Perforce]], [[Microsoft .NET|.Net]], [[EJB]], and filesystem manipulations, just to name a few.&lt;ref&gt;[http://ant.apache.org/manual/tasksoverview.html Overview of Ant Tasks&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

==Portability==
One of the primary aims of Ant was to solve make's portability problems. The first portability issue in a Makefile is that the actions required to create a target are specified as [[operating system shell|shell]] commands which are specific to the [[Platform (computing)|platform]] on which Make runs. Different platforms require different shell commands. Ant solves this problem by providing a large amount of built-in functionality that is designed to behave the same on all platforms. For example, in the sample &lt;code&gt;build.xml&lt;/code&gt; file above the ''clean'' target deletes the &lt;code&gt;classes&lt;/code&gt; directory and everything in it. In a Makefile this would typically be done with the command:
 rm -rf classes/
&lt;code&gt;[[rm (Unix)|rm]]&lt;/code&gt; is a [[Unix]]-specific command unavailable in some other environments. [[Microsoft Windows]], for example, would use:
 rmdir /S /Q classes
In an Ant build file the same goal would be accomplished using a built-in command:
&lt;syntaxhighlight lang=&quot;xml&quot;&gt;
 &lt;delete dir=&quot;classes&quot;/&gt;
&lt;/syntaxhighlight&gt;

A second portability issue is a result of the fact that the symbol used to delimit elements of file system directory path components differs from one platform to another. Unix uses a forward slash (/) to delimit components whereas Windows uses a backslash (\). Ant build files let authors choose their favorite convention: forward slash or backslash for directories; semicolon or colon for path separators. It converts each to the symbol appropriate to the platform on which it executes.

==Limitations==
{{criticism section|date=September 2011}}
{{original research|section|date=September 2011}}

*Ant build files, which are written in [[XML]], can be complex and verbose. The complex structure (hierarchical, partly ordered, and pervasively cross-linked) of Ant documents can be a barrier to learning. (A GUI called Antidote was available for a time, but never gained a following and has been retired from the Apache project.) The build files of large or complex projects can become unmanageably large. Good design and modularization of build files can improve readability but not necessarily reduce size. Other build tools, such as [[Apache Maven|Maven]], use more concise scripts at the expense of generality and flexibility.
*Many of the older tasks&amp;mdash;the core ones that are used every day, such as &lt;tt&gt;&lt;javac&gt;&lt;/tt&gt;, &lt;tt&gt;&lt;exec&gt;&lt;/tt&gt; and &lt;tt&gt;&lt;java&gt;&lt;/tt&gt;&amp;mdash;use default values for options that are not consistent with more recent versions of the tasks. Changing those defaults would break existing Ant scripts.
*When expanding properties in a string or text element, undefined properties are not raised as an error, but left as an unexpanded reference (e.g. &lt;tt&gt;${unassigned.property}&lt;/tt&gt;).
*Ant has limited fault handling rules, and no persistence of state, so it cannot be used as a workflow tool for any workflow other than classic build and test processes.
*Lazy property evaluation is not supported. For instance, when working within an Antcontrib &lt;for&gt; loop, a property cannot be re-evaluated for a sub-value which may be part of the iteration. (Some third-party extensions facilitate a workaround; AntXtras flow-control tasksets do provide for cursor redefinition for loops.)
*In makefiles, any rule to create one file type from another can be written inline within the makefile. For example, you may transform a document into some other format by using rules to execute another tool. Creating a similar task in Ant is more complex: a separate task must be written in Java and included with the Ant build file in order to handle the same type of functionality. However, this separation can enhance the readability of the Ant script by hiding some of the details of how a task is executed on different platforms.

There exists a myriad of third-party Ant extensions (called ''antlibs'') that provide much of the missing functionality. Also, the Eclipse [[integrated development environment|IDE]] can build and execute Ant scripts, while the [[NetBeans]] IDE uses Ant for its internal build system. As both these IDEs are very popular development platforms, they can simplify Ant use significantly. (As a bonus, Ant scripts generated by NetBeans can be used outside that IDE as standalone scripts.)

==See also==
*[[Build automation]]
*[[Apache Maven|Maven]], a project management and build automation tool primarily for Java.
*[[Apache Jelly]], a tool for turning XML into executable code.
*[[Nant]], Ant-like tool targeted at the .NET environment rather than Java.
*[[Apache Ivy|Ivy]], a dependency manager which integrates tightly with Ant, subproject of Ant.
*[[List of build automation software]]

==References==
{{Reflist}}

== Bibliography ==
{{refbegin}}
*{{cite book
| first1    = Steve
| last1     = Loughran
| first2    = Erik
| last2     = Hatcher
| title     = Ant in Action
| publisher = [[Manning Publications]]
| edition   = 2nd
| pages     = 600
| date      = July 12, 2007
| isbn      = 978-1-932394-80-1
| url       = 
}}
*{{cite book
| first1    = Steven
| last1     = Holzner
| title     = Ant - The Definitive Guide
| publisher = [[O'Reilly Media]]
| edition   = 2nd
| pages     = 334
| date      = April 13, 2005
| isbn      = 978-0-596-00609-9
| url       = http://oreilly.com/catalog/9780596006099/
}}
*{{cite book
| first1    = Matthew 
| last1     = Moodie
| title     = Pro Apache Ant
| publisher = [[Apress]]
| edition   = 1st
| pages     = 360 
| date      = November 16, 2005
| isbn      = 978-1-59059-559-6
| url       = http://www.apress.com/book/view/9781590595596
}}
*{{cite book
| first1    = Alexis T.
| last1     = Bell
| title     = ANT Java Notes: An Accelerated Intro Guide to the Java ANT Build Tool
| publisher = [[Virtualbookworm.com Publishing]]
| edition   = 1st
| pages     = 268 
| date      = July 7, 2005
| isbn      = 978-1-58939-738-5
| url       = http://www.virtualbookworm.com/mm5/merchant.mvc?Screen=PROD&amp;Store_Code=bookstore&amp;Product_Code=antjava
}}
*{{cite book
| first1    = Erik 
| last1     = Hatcher
| first2    = Steve 
| last2     = Loughran
| title     = Java Development with Ant
| publisher = [[Manning Publications]]
| edition   = 1st
| pages     = 672 
| date      = August 2002
| isbn      = 978-1-930110-58-8
| url       = 
}}
*{{cite book
| first1    = Glenn 
| last1     = Niemeyer
| first2    = Jeremy 
| last2     = Poteet
| title     = Extreme Programming with Ant: Building and Deploying Java Applications with JSP, EJB, XSLT, XDoclet, and JUnit
| publisher = [[SAMS Publishing]]
| edition   = 1st
| pages     = 456 
| date      = May 29, 2003
| isbn      = 978-0-672-32562-5
| url       = http://www.informit.com/store/product.aspx?isbn=0672325624
}}
*{{cite book
| first1    = Alan
| last1     = Williamson
| title     = Ant - Developer's Handbook
| publisher = [[SAMS Publishing]]
| edition   = 1st
| pages     = 456 
| date      = November 1, 2002
| isbn      = 978-0-672-32426-0
| url       = http://www.informit.com/store/product.aspx?isbn=0672324261
}}
*{{cite book
| first1    = Bernd 
| last1     = Matzke
| title     = ANT: The Java Build Tool In Practice
| publisher = [[Charles River Media]]
| edition   = 1st
| pages     = 280 
| date      = September 2003
| isbn      = 978-1-58450-248-7
| url       = http://www.powells.com/biblio?isbn=9781584502487
}}
{{refend}}

==External links==
{{Wikibooks|Apache Ant}}
*[http://ant.apache.org/ Official website of Apache Ant].
*[http://ant.apache.org/manual/ Apache Ant manual] ([http://ant.apache.org/manual/tasklist.html tasks], [http://ant.apache.org/manual/conceptstypeslist.html types]).
*[http://wiki.apache.org/ant/FrontPage Apache Ant wiki].
*[http://code.google.com/p/winant/ WinAnt - Windows installer for Apache Ant].
*[http://www.exubero.com/ant/antintro-s5.html Introduction to Ant] (slide show).
*[http://www.softwaresecretweapons.com/jspwiki/Wiki.jsp?page=LinguineMapsForApacheAnt Linguine Maps visualization library will automatically produce easy to read diagrams from Ant build files.]
*[http://sourceforge.net/projects/antro antro - a profiler for Ant scripts].
*[[b:Programming:Apache Ant|Wiki Book on learning Apache Ant]].
*[http://ideoplex.com/focus/java#ant Ant tutorial].
*[http://hbtechs.blogspot.com/2007/06/automation-using-innovative-tools.html Ant Automation], a good handy example of automation with Ant.
*[http://visualdrugs.net/antrunner/ A simple Windows GUI for running Ant.]
{{apache}}

[[Category:Apache Software Foundation|Ant]]
[[Category:Free software programmed in Java]]
[[Category:Java development tools]]
[[Category:Compiling tools|Ant]]
[[Category:XML software]]
[[Category:Build automation|Ant]]
[[Category:Java libraries|Ant]]
[[Category:Cross-platform software]]
[[Category:Android (operating system) development software]]
[[Category:Software using the Apache license]]</text>
      <sha1>a909uwd37eiir2i0r5illwnbjkondjx</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Portable Runtime</title>
    <ns>0</ns>
    <id>1825377</id>
    <revision>
      <id>598260411</id>
      <parentid>591453278</parentid>
      <timestamp>2014-03-05T14:48:53Z</timestamp>
      <contributor>
        <username>ScotXW</username>
        <id>19568210</id>
      </contributor>
      <text xml:space="preserve" bytes="3505">{{refimprove|date=July 2010}}
{{ Infobox software
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 1.5.0
| latest release date    = November 18, 2013
| operating system       = [[Cross-platform]]
| programming language   = [[C (programming language)|C]]
| genre                  = Development [[Library (computer science)|Library]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://apr.apache.org/}}
}}

The '''Apache Portable Runtime''' (APR) is a supporting library for the [[Apache HTTP Server|Apache]] [[web server]]. It provides a set of [[API]]s that map to the underlying operating system (OS). Where the OS does not support a particular function, APR will provide an emulation. Thus programmers can use the APR to make a program truly portable across platforms.

APR originally formed a part of [[Apache HTTP Server]], but the [[Apache Software Foundation]] spun it off into a separate project. Other applications can use it to achieve platform independence.

== Functionality ==
The range of platform-independent functionality provided by APR includes:
* [[Memory allocation]] and [[memory pool]] functionality
* [[Atomic operations]]
* Dynamic [[Library (computer science)|library]] handling
* File [[I/O]]
* Command-argument parsing
* [[Lock (software engineering)|Locking]]
* [[Hash table]]s and [[array data structure|array]]s
* [[Mmap]] functionality
* [[Berkeley sockets|Network sockets]] and protocols
* [[Thread (computer science)|Thread]], [[Process (computing)|process]] and [[Mutual exclusion|mutex]] functionality
* [[Shared memory]] functionality
* Time routines
* User and group ID services

== Similar projects ==
* [[GLib]] – provides similar functionality. It supports many more data structures and OS-independent functions, but fewer [[Inter-process communication|IPC]]-related functions. (GLib lacks local and global locking and shared-memory management.)
* [[Netscape portable runtime]] (NSPR) is a cross-platform abstraction library used by the [[Mozilla]] project. It is used by another subproject of [[Mozilla application framework]] (XPFE) to provide cross-platform [[graphical user interface]] (GUI) functionality.
* [[Adaptive Communication Environment]] (ACE) is an object-oriented library written in C++ similar in functionality to APR. It is widely deployed in commercial products.&lt;ref&gt;{{cite web|title=ACE and TAO Success Stories|url=http://www.cs.wustl.edu/~schmidt/ACE-users.html|accessdate=2008-07-31| archiveurl= http://web.archive.org/web/20080829164836/http://www.cs.wustl.edu/%7Eschmidt/ACE-users.html| archivedate= 29 August 2008 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;
* [http://www.hyperrealm.com/main.php?s=commoncpp commonc++] is a cross-platform C++ class library for systems programming, with much of the same functionality as APR.
* [[POCO C++ Libraries|POCO]] is a modern C++ framework similar in concept but more extensive than APR.
* [[WxWidgets]] is an object-oriented cross-platform GUI library that also provides abstraction classes for database communication, [[Inter-process communication|IPC]] and networking functionality.
* [[KDE Frameworks]] – used by [[KDE SC]]

== References ==
{{reflist}}

== External links ==
{{wikibooks|APR tutorial||APR wikibooks tutorial}}
* {{official website|http://apr.apache.org/}}

{{Apache}}

[[Category:Apache Software Foundation|Portable Runtime]]
[[Category:Application programming interfaces]]</text>
      <sha1>a0p16kbxrmc2lrdkple9qpge33wqsah</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Beehive</title>
    <ns>0</ns>
    <id>1789039</id>
    <revision>
      <id>590786461</id>
      <parentid>590786059</parentid>
      <timestamp>2014-01-15T07:56:57Z</timestamp>
      <contributor>
        <ip>93.57.73.110</ip>
      </contributor>
      <comment>/* History */</comment>
      <text xml:space="preserve" bytes="7447">{{About|a Java application framework|the [[Hadoop]]-based data warehouse infrastructure|Apache Hive}}
{{Infobox software
| name                   = Apache Beehive
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Retired
| latest release version = 1.0.2
| latest release date    = {{release date|2006|12|04}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = Java Application Framework
| license                = [[Apache License]] 2.0
| website                = http://beehive.apache.org
}}
'''Apache Beehive''' is a Java Application Framework designed to make the development of [[Java EE]] based applications quicker and easier. It makes use of various open-source projects at [[Apache Software Foundation|Apache]] like [[XMLBeans]]. It leverages the latest innovations in Java 5 which includes [[Java Community Process|JSR-175]] which is a facility for annotating fields, methods and classes so that they can be treated in special ways by runtime tools. It builds on the framework developed for [[BEA Systems]] [[Weblogic]] Workshop for its 8.1 series. BEA later decided to donate the code to Apache, so that a wider audience could get an opportunity to use Beehive.

==History==
Version 8.1 of BEA's Weblogic Workshop includes a number of significant enhancements to version 7.0. The previous version was more focused on creating industrial-strength [[web services]] quickly. However, 7.0 did not have many customers, and it failed to create a stir in the market. However, for version 8.1, BEA created a whole new [[Integrated development environment|IDE]] which helped programmers to develop [[Java EE]] based applications more quickly. This was significantly better than 7.0 with more advanced features and also won several awards and gained a lot of critical acclaim. However, a new revolution was brewing in the [[Java (programming language)|Java]] universe in the form of [[Eclipse (computing)|Eclipse]] and it seemed like everyone was moving towards it. Although Workshop 8.1 did not succeed as much as it intended to, the Weblogic Workshop Framework which was developed for 8.1 version Workshop was recognized as a good solid framework. In order that it can be used with other [[Java EE]] based application servers, BEA decided to open-source the project under the purview of the [[Apache Software Foundation]].
Latest version of Beehive was released in December 4, 2006; its lifetime has ended in January 2010, when it has been retired and moved to [[Apache Attic]].

==Beehive components==
===Netui Page Flows===
This is an application framework built on top of [[Jakarta Struts|Apache Struts]] which allows easier tooling and automatic updating of the various Struts configuration files.

===Controls===
This is the heart of the Beehive framework. A control can be defined as a program which can be used by the developer to quickly gain access to enterprise-level resources such as [[Enterprise Java Beans]] (EJBs), [[web services]] etc. For example consider accessing an old [[Enterprise_JavaBean#Legacy|legacy EJB 2]] bean. It involved a lot of boiler-plate code like getting access to an home interface, then creating/finding an EJB using finder methods and then accessing the remote methods of the bean. Using a control simplified this because it did most of the boiler-plate or routine coding for the developer, who could then concentrate more on business logic rather than worrying about the inner-details of [[Java EE]] technology. If the developer was sufficiently advanced, even then it was useful because then the developer could concentrate on more useful things like constructing a [[Facade pattern|Facade]] to a complex set of application APIs. In essence a control to a legacy EJB 2 bean ensured that the developer could simply use the control and call any business method of the EJB, using it in the same way as any other [[Java (programming language)|Java]] class. When EJB 3 came around, such simplification was already provided by the EJB specification itself,&lt;ref&gt;&quot;This release made it much easier to write EJBs, using 'annotations' rather than the complex 'deployment descriptors' used in version 2.x. The use of home and remote interfaces and the ejb-jar.xml file were also no longer required...&quot; [[Enterprise_JavaBean##EJB_3.0.2C_final_release_.282006-05-11.29|EJB]]&lt;/ref&gt;&lt;ref&gt;[[Enterprise_JavaBean#Example|EJB 3 example]]&lt;/ref&gt;&lt;ref&gt;&quot;Enterprise Java Beans (EJB) 3.0 is a deep overhaul and simplification of the EJB specification.&quot; http://www.jboss.org/ejb3&lt;/ref&gt;&lt;ref&gt;&quot;... the heavyweight programming paradigm in EJB 2.x, the flawed persistence model in EJB 2.x entity beans...&quot; &quot;In our view, one of the most important changes in EJB 3.1 is the redefinition of EJBs as simple managed bean POJOs with additional services.&quot;  http://blog.caucho.com/?p=384&lt;/ref&gt; and Beehive controls were of little further use here.&lt;ref&gt;&quot;... the EJB 3 client model has essentially standardized much of the value-add that the [Beehive] EJB control offered in terms of simplifying the EJB 2.1 client model&quot; http://markmail.org/message/mh43akcleflzes3r&lt;/ref&gt;&lt;ref&gt;Andre McCulloch, &quot;OK, these are great points that lead me to believe that and [sic] EJB3 control does not provide much value add for Beehive right now.&quot; http://markmail.org/message/ktec5f4gsbw22ijb&lt;/ref&gt; The Controls come with a standard set of controls wiz EJB Control, Webservice Control, Database Control and JMS Control. Custom controls can also be developed which in turn could make use of the controls already built-in.

===Webservices===
This is the third component of Beehive and it enables a developer to create webservices using meta-data/annotations quickly. In essence by using meta-data/annotations one can create complex [[web services]] utilizing features like conversation, state etc quickly and since all the meta-data/annotations are in one file, it is easier to debug and maintain. Using this approach any plain Java class can be converted into a web service just by the addition of annotations into the Java source files. This is based on [[Java Community Process|JSR-181]] which builds on [[Java Community Process|JSR-175]].

==See also==
{{Portal|Java}}
&lt;!-- formatting; please do not remove until some more text lines are added to compensate spacing --&gt;

==References==
{{Reflist}}

==Bibliography==
{{Refbegin}}
*{{citation
| first1     = Kunal
| last1      = Mittal
| first2     = Srinivas
| last2      = Kanchanavally
| date       = August 15, 2005
| title      = Pro Apache Beehive
| edition    = 1st
| publisher  = [[Apress]]
| pages      = 240
| isbn       = 978-1-59059-515-2
| url        = http://www.apress.com/book/view/9781590595152
}}
{{Refend}}

==External links==
*[http://beehive.apache.org Apache Beehive home site]
*[http://www.bea.com/framework.jsp?CNT=index.htm&amp;FP=/content/products/weblogic/workshop/ Weblogic Workshop]
*[http://archive.eclipse.org/technology/archives/pollinate-project.tar.gz Pollinate Project] (An Eclipse plugin for Apache Beehive, now [http://www.eclipse.org/technology/archived.php archived] and inactive)
{{apache}}

== References ==
&lt;references/&gt;

{{DEFAULTSORT:Beehive}}
[[Category:Apache Software Foundation]]
[[Category:Java platform software]]</text>
      <sha1>c7ir49mmimxbqsmjtp104w6vteyzngj</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Cocoon</title>
    <ns>0</ns>
    <id>795882</id>
    <revision>
      <id>540315497</id>
      <parentid>503209398</parentid>
      <timestamp>2013-02-25T18:12:26Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 13 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q616663]] ([[User talk:Addbot|Report Errors]])</comment>
      <text xml:space="preserve" bytes="5842">{{ Infobox Software
| name                   = Apache Cocoon
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version = 2.2.0
| latest release date    = {{release date|2008|05|15}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[web application framework]]
| license                = [[Apache License]] 2.0
| website                = http://cocoon.apache.org
}}
'''Apache Cocoon''', usually just called '''Cocoon''', is a [[web application framework]] built around the concepts of [[Pipeline (software)|pipeline]], [[separation of concerns]] and component-based web development.  The framework focuses on [[XML]] and [[XSLT]] publishing and is built using the [[Java (programming language)|Java programming language]].  The flexibility afforded by relying heavily on XML allows rapid content publishing in a variety of formats including [[HTML]], [[PDF]], and [[Wireless Markup Language|WML]].  The [[content management system]]s [[Apache Lenya]] and [[Daisy (software)|Daisy]] have been created on top of the framework.  Cocoon is also commonly used as a [[data warehousing]] [[Extract, transform, load|ETL]] tool or as [[Middleware (distributed applications)|middleware]] for transporting data between systems.

==Sitemap==

The sitemap is at the core of Cocoon. It's here that the web site developer configures the different Cocoon components, and defines the [[client–server model|client–server]] interactions in what Cocoon refers to as the ''[[XML pipeline|Pipelines]]''.

==Components==

The components within Cocoon are grouped by function.

===Matchers===

Matchers are used to match user requests such as [[Uniform Resource Locator|URL]]s or [[http cookie|cookie]]s against [[Wildcard character|wildcard]] or [[regular expression]] patterns. Each user request is tested against matchers in the sitemap until a match is made. It is within a matcher that the response to a particular request is specified.

===Generators===

Generators create a [[stream (computing)|stream]] of data for further processing. This stream can be generated from an existing XML document or there are generators that can create XML from scratch to represent something on the server, such as a directory structure or image data.

====XSP====
One type of generator is an XML Server Page ('''XSP''' [http://cocoon.apache.org/1.x/xsp.html]), an XML document containing tag-based directives that specify how to generate dynamic content at request time. Upon Cocoon processing, these directives are replaced by generated content so that the resulting, augmented XML document can be subject to further processing (typically an XSLT transformation). XSPs are transformed into Cocoon producers, typically as Java classes, though any scripting language for which a Java-based processor exists could also be used.

Directives can be either built-in (&quot;XSP&quot;) or user-defined processing tags, both of which are defined in ''logicsheets''.  Tags are defined using XSLT templates that describe how the tags (represented as XML nodes) are transformed into other XML nodes or into procedural code such as Java. The tags are used to embed procedural logic, substitute expressions, retrieve information from the web server environment, and other operations.

Note that XSP is deprecated in recent releases of Cocoon.

===Transformers===

Transformers take a stream of data and change it in some way. The most common transformations are performed with XSLT to change one xml format into another. But there are also transformers that take other forms of data ([[SQL]] commands for example).

===Serializers===

A serializer turns an XML event stream into a sequence of bytes (such as HTML) that can be returned to the client. There are serializers that allow you to send the data in many different formats including [[HTML]], [[XHTML]], [[portable document format|PDF]], [[Rich Text Format|RTF]], [[Scalable Vector Graphics|SVG]], [[Wireless Markup Language|WML]] and [[plain text]], for example.

===Selectors===
Selectors offer the same capabilities as a switch statement. They are able to select particular elements of a request and choose the correct pipeline part to use.

===Views===

Views are mainly used for testing. A view is an exit point in a pipeline. You can put out the XML-Stream which is produced till this point. So you can see if the application is working right.

===Readers===
Publish content without parsing it (no [[XML]] processing). Used for images and such.

===Actions===
Actions are Java classes that execute some business logic or manage new content production.

==The Pipeline==

A ''[[XML pipeline|pipeline]]'' is used to specify how the different Cocoon components interact with a given request to produce a [[Output|response]]. A typical pipeline consists of a generator, followed by zero or more transformers, and finally a serializer.

==See also==
* [[Reactor pattern]] - the design pattern that Cocoon is based on.
* [[XProc]] - a W3C Standard for modelising of XML pipeline.

==External links==

* [http://cocoon.apache.org/ The Apache Cocoon Project]
* [http://cocoon.apache.org/2.1/ Cocoon 2.1 Documentation]
* [http://www.apache.org/ The Apache Software Foundation]
* [http://code.google.com/p/pycoon/ Pycoon] - A Python port of cocoon.
* [http://www.nexista.org/ Nexista] - A PHP cocoon clone
* [http://www.paloose.org/pp/index.html/ Paloose] - Cocoon without Java (A PHP Clone)
* [http://cocoon.zones.apache.org/cocoon21/samples/ Cocoon Sample programs]
{{Application frameworks}}
{{apache}}

[[Category:Apache Software Foundation|Cocoon]]
[[Category:Web application frameworks|Cocoon]]
[[Category:Java platform software|Cocoon]]</text>
      <sha1>0ikgh0450ewd6v0paffow6r1kbsycps</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Directory</title>
    <ns>0</ns>
    <id>3672620</id>
    <revision>
      <id>601393077</id>
      <parentid>593224644</parentid>
      <timestamp>2014-03-26T18:54:44Z</timestamp>
      <contributor>
        <username>Arvindraivns</username>
        <id>20271636</id>
      </contributor>
      <comment>/* External links */</comment>
      <text xml:space="preserve" bytes="3091">{{ Infobox Software
| name                   = Apache Directory Server
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 2.0.0-M15
| latest release date    = {{release date|2013|08|18}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[LDAP]]
| license                = [[Apache License]] 2.0
| website                = http://directory.apache.org/
}}
'''Apache Directory''' is an [[open source]] project of the [[Apache Software Foundation]]. Its flagship product, the '''Apache Directory Server''', is an embeddable [[directory server]] entirely written in [[Java (programming language)|Java]]. It was certified [[Lightweight Directory Access Protocol|LDAP]]v3-compatible by [[The Open Group]] in 2006.&lt;ref&gt;{{Cite web |title=The Apache Software Foundation Is Latest Technology Leader To Embrace LDAP Standard And Achieve Open Group Certification |url=http://opengroup.org/comm/press/11oct06.html |publisher=The Open Directory |date=11 October 2006 |accessdate=30 June 2009 }}&lt;/ref&gt;&lt;ref&gt;{{Cite web |title=What Apache Directory Server is |url=http://directory.apache.org/apacheds/1.0/11-what-apache-directory-server-is.html |accessdate=30 June 2009 |quote=Please note that the server has been successfully certified by the Open Group in September 2006 (&quot;LDAP certified&quot;).| archiveurl= http://web.archive.org/web/20090531195522/http://directory.apache.org/apacheds/1.0/11-what-apache-directory-server-is.html| archivedate= 31 May 2009 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; Besides LDAP, the server supports other protocols as well, and a [[Kerberos (protocol)|Kerberos]] server.&lt;ref&gt;{{Cite web |title=What Apache Directory Server is |url=http://directory.apache.org/apacheds/1.0/11-what-apache-directory-server-is.html |accessdate=30 June 2009 |quote=Other network protocols like Kerberos and NTP are supported| archiveurl= http://web.archive.org/web/20090531195522/http://directory.apache.org/apacheds/1.0/11-what-apache-directory-server-is.html| archivedate= 31 May 2009 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

A subproject implements an Eclipse-based directory tool called '''Apache Directory Studio'''. It includes an LDAP browser/editor, a schema browser, an [[LDAP Data Interchange Format|LDIF]] editor, a [[Directory Service Markup Language|DSML]] editor, and more. 

== See also ==
{{Portal|Java}} 
*[[List of LDAP software]]

== References ==
{{Reflist}}

== External links ==
*[http://www.concretepage.com/java-ee/jndi/create-local-ldap-server-in-eclipse-with-apache-directory-studio Create Local LDAP Server in Eclipse with Apache Directory Studio]
*[http://directory.apache.org/ Apache Directory Server]
*[http://directory.apache.org/studio/ Apache Directory Studio]

{{apache}}

[[Category:Apache Software Foundation|Directory]]
[[Category:Directory services]]
{{network-software-stub}}</text>
      <sha1>8ym5vxf1tj72hpszk0zxbebv6j6cltt</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Forrest</title>
    <ns>0</ns>
    <id>5252792</id>
    <revision>
      <id>541411811</id>
      <parentid>511667933</parentid>
      <timestamp>2013-03-01T03:30:08Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>Bot: Migrating 4 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q616749]]</comment>
      <text xml:space="preserve" bytes="1577">{{ Infobox Software
| name                   = Apache Forrest
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version = 0.9
| latest release date    = {{release date|2011|02|07}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[web framework|Web Framework]]
| license                = [[Apache License]] 2.0
| website                = http://forrest.apache.org
}}
'''Apache Forrest''' is a web-publishing framework based on [[Apache Cocoon]]. It is an XML publishing framework that allows multiple types of data-files as input, such as various popular word processing and spreadsheet files, as well as two wiki dialects.  Plugins are available to support additional formats, both for input as well as output (such as [[PDF]]).

Forrest is not a [[content management system|content management system (CMS)]], as it lacks the full workflow and admin functions of a CMS. Its primary use is in integrating and aggregating content from various sources and presenting them in a unified format for human consumption.

==See also==
* [[Apache Software Foundation]]
* [[Apache Cocoon]]
* [[Apache Lenya]]

==External links==
* [http://forrest.apache.org/ Apache Forrest website]
{{apache}}

[[Category:Apache Software Foundation|Forrest]]
[[Category:Web design]]
[[Category:Technical communication tools]]

{{web-software-stub}}</text>
      <sha1>7noxs0mbhfpppts5gbzaqceyesw8bjn</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Geronimo</title>
    <ns>0</ns>
    <id>1614341</id>
    <revision>
      <id>577528294</id>
      <parentid>540370914</parentid>
      <timestamp>2013-10-17T04:56:12Z</timestamp>
      <contributor>
        <username>Peterl</username>
        <id>266404</id>
      </contributor>
      <comment>No current preview</comment>
      <text xml:space="preserve" bytes="9051">{{Infobox Software
| name                   = Apache Geronimo
| logo                   = [[File:Apache Geronimo Logo Large.png|100px|Apache Geronimo Logo]]
| screenshot             = [[File:Apache Geronimo Administration Console Screenshot.png|300px]]
| caption                = Apache Geronimo Web Administration Console
| collapsible            = yes
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = {{Latest stable software release/Geronimo Application Server}}
| latest release date    = 
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]] ([[JVM]])
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[application server|Web Application Server]]
| license                = [[Apache License]] 2.0
| website                = http://geronimo.apache.org
}}
'''Apache Geronimo''' is an [[open source]] [[application server]] developed by the [[Apache Software Foundation]] and distributed under the [[Apache license]]. Geronimo 2 is currently compatible with the [[Java Platform, Enterprise Edition|Java Enterprise Edition]] (Java EE) 5.0 specification such as [[JDBC]], [[Remote Method Invocation|RMI]], [[e-mail]], [[Java Message Service|JMS]], [[web service]]s, [[XML]], [[Enterprise JavaBean]]s, [[Java EE Connector Architecture|Connector]]s, [[servlets]], [[portlets]] and [[JavaServer Page]]s. This allows developers to create enterprise applications that are portable and scalable, and that integrate with legacy technologies. Geronimo 3 is compatible with Java EE 6.0.

[[IBM]] has provided considerable support to the project through marketing, code contributions, and the funding of several project committers. In October 2005, IBM announced a free edition of its [[WebSphere]] application server named [[IBM WebSphere Application Server Community Edition|Websphere Application Server Community Edition]], which is based on Geronimo.&lt;ref&gt;http://www14.software.ibm.com/webapp/iwm/web/preLogin.do?lang=en_US&amp;source=wsced&lt;/ref&gt; Other commercial supporters include [[AMD]], Chariot Solutions, Simula Labs, and Virtuas.

== Components ==
Like an enterprise [[operating system]], Geronimo is built on a [[Kernel (computer science)|kernel]]—a [[microkernel]] that lays the foundation for everything above it. Geronimo's kernel is Java EE agnostic. Its sole purpose is to manage Geronimo's building blocks. Geronimo is marked by an architectural design that is based on the concept of [[Inversion of Control]] (IoC) (sometimes called [[Dependency Injection]]), which means that the kernel has no direct dependency on any of its [[Component-based software engineering|components]]. The kernel is a framework for services that controls the service life cycle and [[metadata registry|registry]]. The kernel is based on Java EE. It works with Java EE services and components to build specific configurations—one of which is a full Java EE [[solution stack]].

A majority of the Geronimo services are added and configured through GBeans to become a part of the overall application server. A ''GBean'' is the interface that connects the component to the kernel. Each GBean can maintain state, depend on, and interrelate with other GBeans, and operate on events from the kernel and other GBeans. The GBeans interface makes it possible to switch between two [[servlet container]]s, for example [[Jetty (web server)|Jetty]] or [[Apache Tomcat|Tomcat]], without affecting the whole architecture using a GBeans interface. This flexible architecture makes it possible for the Geronimo developers to integrate several existing field-tested [[open source software]] projects.

Here a list of the open source components that are included in the Geronimo project.

{| class=&quot;wikitable&quot;
|-
! Component
! Description
|-
| [[Apache Tomcat]] 
| HTTP server and Servlet container supporting [[Java Servlet]] 2.5 and [[JavaServer Pages]] (JSP) 2.1.
|-
| [[Jetty (web server)|Jetty]] 
| HTTP server and Servlet container supporting Java Servlet 2.5 and JavaServer Pages 2.1—an alternative to the Tomcat server.
|-
| [[Apache ActiveMQ]] 
| Open source [[Java Message Service|Java Message Service (JMS)]] 1.1 applications provider and supporter of message-driven beans (MDBs).
|-
| [[Apache OpenEJB]] 
| Open source [[Enterprise JavaBean]]s (EJB) Container System and EJB Server that supports Enterprise JavaBeans at the 3.0 level, including [[Java Persistence API|Container Managed Persistence]] 2 (CMP2) and [[EJB Query Language]] (EJBQL).
|-
| [[Apache OpenJPA]] 
| Open source [[Java Persistence API]] (JPA) 1.0 implementation.
|-
| [[Apache ServiceMix]] 
| Open source [[Enterprise Service Bus]] (ESB) and component suite based on the [[Java Business Integration]] (JBI) standard on JSR 208.
|-
| [[Apache Axis]] and [[Apache Scout]] 
| Axis is a Simple Object Access Protocol ([[SOAP]]) implementation, while Scout is a JSR 93 ([[JAXR]]) implementation. These provide support for [[Web Services]] and [[Web Services Interoperability]] Organization (WS-I) Basic Profile support.
|-
| [[Apache CXF]] 
| [[Web Services]] frameworks with variety of protocols such as SOAP, XML/HTTP, [[Representational State Transfer|RESTful]] [[HTTP]], or [[CORBA]] and work over a variety of transports such as [[HTTP]], [[Java Message Service|JMS]] or [[Java Business Integration|JBI]].
|-
| [[Apache Derby]] 
| Full-fledged [[relational database management system]] (RDBMS) with native [[Java Database Connectivity]] (JDBC) support.
|-
| [[Apache WADI]] 
| [[Cluster (computing)|Cluster]]ing, [[Load balancing (computing)|load balancing]] and [[failover]] solution for the [[Web application framework|web application container tier]]. (The project is currently in incubation under the [[Apache Incubator]].)
|-
| [[MX4J]] 
| [[Java Management Extensions]] that supplies tools for managing and monitoring applications, system objects, devices and service oriented networks.
|}

==References==
{{reflist}}

==Bibliography==
{{refbegin}}
*{{citation
| first     = Aaron
| last      = Mulder
| year      = 2007
| title     = Apache Geronimo Development and Deployment
| publisher = [[Addison-Wesley Professional]]
| isbn      = 0-321-33483-3
| url       = http://www.chariotsolutions.com/geronimo/index.html
}}
*{{citation
| first     = Kishore
| last      = Kumar
| year      = 2006
| title     = Pro Apache Geronimo
| publisher = [[Apress]]
| isbn      = 1-59059-642-0
| url       = http://www.apress.com/book/view/9781590596425
}}
*{{citation
| first1    = Jeff 
| last1     = Genender
| first2    = Bruce
| last2     = Snyder
| first3    = Sing
| last3     = Li
| year      = 2006
| title     = Professional Apache Geronimo
| publisher = [[Wrox Press|Wrox]]
| isbn      = 0-471-78543-1
| url       = http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471785431.html
}}
*{{citation
| first     = David
| last      = Blevins
| year      = 2004
| title     = Geronimo: A Developer's Notebook
| publisher = [[O'Reilly Media]]
| isbn      = 0-596-00671-3
}}
{{refend}}

== See also ==
Other Java EE application servers:
* [[Apache TomEE]]
* [[JBoss AS]]
* [[IBM WebSphere Application Server|WebSphere AS]]
* [[WebLogic|WebLogic Server]]
* [[Comparison of application servers#Java EE|Comparison of application servers]]
* [[GlassFish]]

==External links==
*[http://geronimo.apache.org Apache Geronimo]
*[http://www-128.ibm.com/developerworks/opensource/top-projects/geronimo.html Geronimo resources area at IBM developerWorks]
*[http://www.ibm.com/developerworks/websphere/zones/was/wasce.html WebSphere Application Server Community Edition resources area at IBM developerWorks]
*[http://www-306.ibm.com/software/info1/websphere/index.jsp?tab=landings/was-ce Announcing IBM WebSphere Application Server Community Edition]
*[http://people.apache.org/~hogstrom/performance/geronimo/2.0/Geronimo2.0.2PerformanceReport-v01draft.pdf Geronimo 2.0.2 vs 1.1.1 Performance report]
*[http://www.ibm.com/developerworks/websphere/library/techarticles/0709_jain/0709_jain.html What's new in WebSphere Application Server Community Edition V2.0]
*[http://www.ibm.com/developerworks/websphere/library/techarticles/0807_jain/0807_jain.html?S_TACT=105AGX10&amp;S_CMP=WASCE What’s new in WebSphere Application Server Community Edition V2.1]
*[http://publib.boulder.ibm.com/wasce/V3.0.0/en/whats-new-in-ce-30.html What’s new in WebSphere Application Server Community Edition V3.0]
=== Presentations ===
*[http://parleys.com/display/PARLEYS/Apache+Geronimo+Unleashed?showComments=true Apache Geronimo Unleashed at javapolis 2006]
* [http://www-128.ibm.com/developerworks/forums/servlet/JiveServlet/download/541-215779-14114573-327254/Impact-2486-WAS-CE.pdf Impact 2008 IBM Websphere CE compared to Jboss]
*[http://cwiki.apache.org/geronimo/presentations.html Presentations listed on Geronimo Wiki]

{{apache}}

[[Category:Apache Software Foundation|Geronimo]]
[[Category:Java enterprise platform]]
[[Category:Free software application servers]]</text>
      <sha1>djd0j1wir2pqeqowabx6w41k8k6jvca</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Derby</title>
    <ns>0</ns>
    <id>2328658</id>
    <revision>
      <id>597633582</id>
      <parentid>586756806</parentid>
      <timestamp>2014-03-01T07:07:47Z</timestamp>
      <contributor>
        <username>DocWatson42</username>
        <id>38455</id>
      </contributor>
      <comment>Performed clean up.</comment>
      <text xml:space="preserve" bytes="6952">{{Infobox Software
| name                   = Apache Derby
| logo                   = [[File:Derby Logo.png|200px|The Apache Derby Project]]
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| author                 = Cloudscape Inc (Later IBM)
| latest release version = 10.10.1.1
| latest release date    = {{release date|2013|4|15}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Relational Database Management System]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://db.apache.org/derby/}}
}}

'''Apache Derby''' (previously distributed as '''IBM Cloudscape''') is a [[relational database management system]] (RDBMS) developed by the [[Apache Software Foundation]] that can be embedded in [[Java (programming language)|Java]] programs and used for [[online transaction processing]]. It has a 2.6 [[Megabyte|MB]] disk-space footprint.&lt;ref&gt;{{cite web | title=Apache Derby | publisher=Apache.org| url=http://db.apache.org/derby/}}&lt;/ref&gt;

Apache Derby is developed as an [[open source]] project under the [[Apache License|Apache 2.0 license]]. [[Oracle Corporation|Oracle]] distributes the same binaries under the name '''Java DB'''.&lt;ref&gt;[http://www.oracle.com/technetwork/java/javadb/overview/faqs-jsp-156714.html#1q2 Java DB - FAQs: &quot;Is Java DB a fork of Apache Derby?&quot;]&lt;/ref&gt;

== Derby technologies ==

=== Derby embedded database engine ===
The core of the technology, Derby's database engine, is a full-functioned relational embedded database-engine, supporting [[JDBC]] and [[SQL]] as programming APIs. It uses [[IBM DB2]] [[SQL]] syntax.

=== Derby Network Server ===
The Derby network server increases the reach of the Derby database engine by providing traditional client server functionality. The network server allows clients to connect over TCP/IP using the standard [[DRDA]] protocol. The network server allows the Derby engine to support networked [[JDBC]], [[ODBC]]/[[Call Level Interface|CLI]], [[Perl]] and [[PHP]].

=== Embedded Network Server ===
An embedded database can be configured to act as a hybrid server/embedded RDBMS; to also accept TCP/IP connections from other clients in addition to clients in the same JVM.&lt;ref&gt;see Embedded Server Example in [http://db.apache.org/derby/docs/10.4/adminguide/ http://db.apache.org/derby/docs/10.4/adminguide/]&lt;/ref&gt;

=== Database Utilities ===
* ij: a tool that allows SQL scripts to be executed against any JDBC database.
* dblook: Schema extraction tool for a Derby database.
* sysinfo: Utility to display version numbers and class path.

== History ==
Apache Derby originated at Cloudscape Inc, an [[Oakland, California|Oakland]], [[California]], start-up founded in 1996 by Nat Wyatt and Howard Torf to develop Java [[database]] technology. The first release of the database engine, then called JBMS, was in 1997. Subsequently the product was renamed Cloudscape and releases were made about every six months.

In 1999 [[Informix]] Software, Inc., acquired Cloudscape, Inc. In 2001 [[IBM]] acquired the database assets of Informix Software, including Cloudscape. The database engine was re-branded to IBM Cloudscape and releases continued, mainly focusing on embedded use with IBM's Java products and middleware.

In August 2004 IBM contributed the code to the [[Apache Software Foundation]] as Derby, an incubator project sponsored by the [[Apache DB]] project.&lt;ref&gt;{{cite web | title=Why IBM is open sourcing Cloudscape as Derby | publisher= IBM | url=http://www.ibm.com/developerworks/data/library/techarticle/dm-0410prial/}}&lt;/ref&gt; In July 2005 the Derby project graduated from the Apache incubator and is now being developed as a sub-project of the [[Apache DB|DB]] Top Level Project at Apache. Prior to Derby's graduation from incubation, Sun joined the Derby project with an intent to use Derby as a component in their own products,&lt;ref&gt;{{cite web | title=Apache Derby graduates with Sun onboard | publisher= CNET news.com | url=http://news.com.com/Apache+Derby+graduates+with+Sun+onboard/2100-7344_3-5818473.html|archiveurl=http://archive.is/cuS0|archivedate=2012-07-14}}&lt;/ref&gt; and with the release of Java 6 in December 2006, Sun started packaging Derby in the [[Java Development Kit|JDK]] branded as Java DB.

In March 2007 IBM announced that they would withdraw marketing and support for the Cloudscape product, but would continue to contribute to the Apache Derby project.&lt;ref&gt;{{cite web | title=Changes in Cloudscape Availability and Support | publisher=IBM | url=http://www-1.ibm.com/support/docview.wss?rs=636&amp;uid=swg21256502}}&lt;/ref&gt;

Currently (December 2013) Derby comes with Java 7 and has been branded as &quot;JavaDB&quot; but is exactly the same bit-for-bit as Derby is.  For developers wanting to use Java 6, they can still download Derby as before, but for developers requiring JRE 7 or later, Derby is included in the Java API.

== See also ==
{{Portal|Free software}}
* [[List of relational database management systems]]
* [[Comparison of relational database management systems]]

==References==
{{reflist}}

==Bibliography==
{{refbegin}}
*{{Cite journal
| format    = 
| first1    = Paul C. 
| last1     = Zikopoulos
| first2    = George 
| last2     = Baklarz
| first3    = Dan 
| last3     = Scott
| date      = November 6, 2005
| title     = Apache Derby—Off to the Races: Includes Details of IBM Cloudscape
| edition   = First
| publisher = [[IBM Press]]
| pages     = 600
| isbn      = 0-13-185525-5
| url       = http://www.ibmpressbooks.com/bookstore/product.asp?isbn=0131855255
| postscript    = &lt;!--None--&gt;
}} {{dead link|date=July 2010}}
{{refend}}

==External links==
* {{official website|http://db.apache.org/derby/}}
* [http://www-306.ibm.com/software/data/cloudscape/ IBM Cloudscape Site]
* [http://db.apache.org/derby/binaries/ApacheDerbyInternals_1_1.pdf Internals of Derby, An Open Source Pure Java Relational Database Engine] deployable in an embedded [[OSGi]] environment
* [http://www.oracle.com/technetwork/java/javadb/overview/index.html Oracle Java DB Site]
* [http://www-106.ibm.com/developerworks/db2/library/techarticle/dm-0410prial/ Why IBM is open sourcing Cloudscape as Derby], IBM developerWorks site
* [http://folk.ntnu.no/andersmo/derby_project_report.pdf Apache Derby SMP scalability]
* [http://www.jpab.org/Derby.html Apache Derby performance results in the JPA Benchmark]
* [http://wiki.apache.org/db-derby/SQLvsDerbyFeatures Compliance matrix with SQL 2003]
* [http://www.mamstricks.com/2013/09/a-simple-java-program-with-derby.html Derby Database connection using netbeans with screenshots ]

{{apache}}

[[Category:Apache Software Foundation|Derby]]
[[Category:Free database management systems]]
[[Category:Free software programmed in Java]]</text>
      <sha1>sdvycwmbrppw0odilcmt5ufjoxsk9kq</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>IBATIS</title>
    <ns>0</ns>
    <id>3946076</id>
    <revision>
      <id>598729514</id>
      <parentid>578159973</parentid>
      <timestamp>2014-03-08T19:04:38Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>/* History */[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (9979)</comment>
      <text xml:space="preserve" bytes="8532">{{lowercase|iBATIS}}
{{Infobox Software
| name                   = Apache iBATIS
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Inactive (see [[MyBatis]])
| latest release version = 
| latest release date    = 
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]], [[.NET Framework|.NET]] and [[Ruby (programming language)|Ruby]]
| genre                  = [[persistence framework]]
| license                = [[Apache License]] 2.0
| website                = http://ibatis.apache.org
}}

'''iBATIS''' is a [[persistence framework]] which automates the mapping between [[SQL]] databases and objects in Java, .NET, and Ruby on Rails.  In Java, the objects are POJOs ([[Plain Old Java Object]]s).  The mappings are decoupled from the application logic by packaging the SQL statements in [[XML]] configuration files.  The result is a significant reduction in the amount of code that a developer needs to access a relational database using lower level APIs like [[JDBC]] and [[ODBC]].

Other persistence frameworks such as [[Hibernate (Java)|Hibernate]] allow the creation of an object model (in Java, say) by the user, and create and maintain the relational database automatically.  iBATIS takes the reverse approach: the developer starts with a SQL database and iBATIS automates the creation of the Java objects.  Both approaches have advantages, and iBATIS is a good choice when the developer does not have full control over the SQL database schema.  For example, an application may need to access an existing SQL database used by other software, or access a new database whose schema is not fully under the application developer's control, such as when a specialized database design team has created the schema and carefully optimized it for high performance.

On May 21, 2010 the development team forked the code creating a new project called [[MyBatis]] and making new releases there. As a consequence the Apache iBATIS project became inactive and was moved to the Apache Attic in June 2010.

== Usage ==
For example, assume there is a database table '''PRODUCT''' (PROD_ID ''INTEGER'', PROD_DESC ''VARCHAR(64)'') and a Java class '''com.example.Product''' (id: ''int'', description: ''String'').  To read the product record having the key '''PROD_ID''' into a new '''Product''' POJO, the following mapping is added into an iBATIS XML mapping file:

&lt;source lang=&quot;xml&quot;&gt;
    &lt;select id=&quot;getProduct&quot; parameterClass=&quot;java.lang.Long&quot; resultClass=&quot;com.example.Product&quot;&gt;
 	select PROD_ID as id,
               PROD_DESC as description
          from PRODUCT
         where PROD_ID = #value#
    &lt;/select&gt;
&lt;/source&gt;

A new Java '''Product''' object can then be retrieved from the database for product number 123 as follows:

&lt;source lang=&quot;java&quot;&gt;
    Product resultProduct = (Product) sqlMapClient.queryForObject(&quot;getProduct&quot;, 123);
&lt;/source&gt;

In the mapping file example, &lt;code&gt;#value#&lt;/code&gt; refers to the long integer value passed into the query. If the parameter is a Java object, then values from properties on that object can be inserted into the query using a similar &lt;code&gt;#&lt;/code&gt; notation. For example, if the parameter class is a &lt;code&gt;com.example.Product&lt;/code&gt; which has a property called &lt;code&gt;id&lt;/code&gt;, then &lt;code&gt;#value#&lt;/code&gt; can be replaced with &lt;code&gt;#id#&lt;/code&gt;. The &lt;code&gt;sqlMapClient&lt;/code&gt; object is an instance of class &lt;code&gt;com.ibatis.sqlmap.client.SqlMapClient&lt;/code&gt;.

== Availability ==
The founder of iBATIS has [http://clintonbegin.blogspot.com/2008/02/clintons-java-5-rant.html publicly stated his dismay with Java 5], but has continued to release new versions of iBATIS for Java.  Versions 2.3.1 and 2.3.2 came out in April 2008, and 2.3.3 in July.

The framework is currently available in [[Java (programming language)|Java]], [[.NET Framework|.NET]], and [[Ruby (programming language)|Ruby]] (RBatis) versions. The [http://code.google.com/p/jbati/ jBati] project is a JavaScript [[ORM]] inspired by iBATIS.

The Apache [[iBator]] tool is closely related: it connects to your database and uses its metadata to generate iBATIS mapping files and Java classes.

==History==
In 2001 a project called iBATIS was started by Clinton Begin. Originally the focus was on the development of cryptographic software solutions. The first product to be released by iBATIS was Secrets,&lt;ref&gt;[http://sourceforge.net/projects/ibatissecrets iBATIS Secrets]&lt;/ref&gt; a personal data encryption and signing tool much like PGP. Secrets was written entirely in Java and was released under an open source license.

That year [[Microsoft]] published a paper&lt;ref&gt;[http://onjava.com/pub/a/onjava/2001/11/28/catfight.html Cat Fight in a Pet Store: J2EE vs. .NET]&lt;/ref&gt; to demonstrate that its recent [[.NET Framework|.NET]] 1.0 framework was more productive than [[Java (programming language)|Java]]. For that purpose Microsoft built its own version of Sun's Web &quot;Pet Store&quot;, a Web project that Sun had used to show Java best practices ([[Java BluePrints]]). [[Microsoft]] claimed that [[.NET Framework|.NET]] was 10 times faster and 4 times more productive than [[Java (programming language)|Java]].

In 2002 Clinton developed an application called JPetStore&lt;ref&gt;[http://www.clintonbegin.com/downloads/JPetStore-1-2-0.pdf JPetStore 1.0]&lt;/ref&gt; to demonstrate that [[Java (programming language)|Java]] could be more productive than [[.NET Framework|.NET]] and could also do so while achieving a better architecture than was used in the [[Microsoft]] implementation.

JPetStore 1.0 had a big impact&lt;ref&gt;[http://www.theserverside.com/news/thread.tss?thread_id=14243 JPetStore 1.0 announcement on TheServerside.com]&lt;/ref&gt; and the [[Database abstraction layer|database layer]] that Clinton used attracted the attention of the community. Soon, iBATIS Database Layer 1.0 project started, composed by two components: iBATIS DAO and iBATIS SQL Maps.

iBATIS 2.0 was released in June 2004.&lt;ref&gt;[http://www.theserverside.com/news/thread.tss?thread_id=26844 iBATIS 2.0 announcement]&lt;/ref&gt; It was a complete redesign while keeping the same features. Clinton donated the iBATIS name and code to [[Apache Software Foundation]] and the project stayed in the ASF for six years.

Eventually iBATIS DAO was deprecated, considering that better DAO frameworks were available, such as [[Spring Framework]].

On May 19, 2010 iBATIS 3.0 was published and simultaneously the development team decided to continue the development of the framework at [[Google Code]].&lt;ref&gt;[http://mail-archives.apache.org/mod_mbox/ibatis-user-java/201005.mbox/%3CAANLkTimXoLiHwI-3kbW6It7mH0771xJP4RqT609VKCXC@mail.gmail.com%3E iBATIS Project Team Moving to Google Code]&lt;/ref&gt; under a new project called [[MyBatis]].

On June 16, 2010 Apache announced that iBATIS was retired and moved to the Apache's attic.

==See also==
*[[MyBatis]]
*[[Java Persistence API]]
*[[Hibernate (Java)|Hibernate]]
*[[EclipseLink]]
*[[Apache Cayenne]]
*[[Spring Framework (Java)|Spring Framework]]
*[[pureQuery]]
*[[NHydrate]]
*[[OpenJPA]]
*[http://www.orbroker.org O/R Broker], similar framework for [[Scala (programming language)|Scala]]

== References ==
{{reflist}}

== Bibliography ==
{{refbegin}}
* {{cite book 
| last      = Begin
| first     = Clinton
| title     = iBATIS in Action
| coauthors = Brandon Goodin, Larry Meadors
| publisher = [[Manning Publications|Manning]] 
| edition   = 1st
| date      = January 17, 2007
| pages     = 384
| isbn      = 1-932394-82-5 {{Please check ISBN|reason=Check digit (5) does not correspond to calculated figure.}}
| url       = 
}}
* {{cite book 
| last      = Richardson
| first     = Chris
| title     = POJOs In Action 
| publisher = [[Manning Publications|Manning]] 
| edition   = 1st
| date      = January 23, 2006
| pages     = 456 
| isbn      = 1-932394-58-3
| url       = 
}}
{{refend}}

==External links==
*{{Official website|ibatis.apache.org}}
*[http://ibatis.apache.org/ibator The iBator project at Apache]
*[http://www.linksdaddy.com/blogs/show-blog/Integrating-iBatis-in-Spring-based-enterprise-applications Integrating iBatis in Spring based enterprise applications]: A simple step by step guide for using iBatis in Spring framework

{{Apache}}

[[Category:Apache Software Foundation|iBATIS]]
[[Category:Computer languages]]
[[Category:Object-relational mapping]]
[[Category:Persistence frameworks]]</text>
      <sha1>hax5u08k24djdexnvbp8dcb5ikfkog1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Jackrabbit</title>
    <ns>0</ns>
    <id>2889426</id>
    <revision>
      <id>596299355</id>
      <parentid>593801664</parentid>
      <timestamp>2014-02-20T06:28:34Z</timestamp>
      <contributor>
        <username>Billrobo</username>
        <id>1090643</id>
      </contributor>
      <minor/>
      <comment>/* See also */ Removed Nuxeo from list as it appears to no longer use Jackrabbit (Ref: http://www.nuxeo.com/blog/development/2011/01/why-nuxeo-dropped-jcr/)</comment>
      <text xml:space="preserve" bytes="3066">{{Infobox Software
| name                   = Apache Jackrabbit
| logo                   = [[File:Jackrabbitlogo.png|200px|Apache Jackrabbit Logo]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 2.6.3
| latest release date    = {{Release date|2013|08|01}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Content repository]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://jackrabbit.apache.org/}}
}}
'''Apache Jackrabbit''' is an [[open source]] [[content repository]] for the [[Java platform]]. The Jackrabbit project was started on August 28, 2004, when [[Day Software]] licensed an initial implementation of the [[Content repository API for Java|Java Content Repository API (JCR)]]. Jackrabbit was also used as the [[reference implementation]] of [[JSR-170]], specified within the [[Java Community Process]]. The project graduated from the [[Apache Incubator]] on March 15, 2006, and is now a Top Level Project of the [[Apache Software Foundation]].

JCR specifies an API for application developers (and application frameworks) to use for interaction with modern content repositories that provide content services such as searching, versioning, transactions, etc.

==Features==
* Fine and coarse-grained content access
* Hierarchical content
* Structured content 
* Node types and mixins
* Property types - text, number, date
* Binary properties
* XPath queries
* SQL queries
* Unstructured content
* Import and export
* Referential integrity
* Access control
* Versioning
* JTA support
* Observation
* Locking
* Clustering
* Multiple persistence models

==See also==
{{Portal|Free software}}
* [[Apache Sling]] - a web framework for building applications on top of Apache Jackrabbit
* [[Hippo CMS]] - an Open Source content management system based on Apache Jackrabbit
* [[eXo Platform|eXo JCR]] - another Open Source JCR implementation
* [[Jahia]] - Open Source [[Enterprise Content Management|ECM]] based on Apache Jackrabbit
* [[LogicalDOC]] - Open Source, Enterprise Document Management that uses Apache Jackrabbit
* [[Magnolia (CMS)]] - an Open Source content management system based on Apache Jackrabbit
* [[OpenKM]] - Open Source [[Knowledge Management|KM]] based on Apache Jackrabbit
* [[Sakai Project]] - Open Source Collaboration and Learning Environment based on Apache Sling and Apache Jackrabbit

==External links==
*[http://jackrabbit.apache.org/ Jackrabbit's Home Page]
*[http://www.apachenews.org/archives/000849.html Jackrabbit 1.0 released]
*[http://www.jcp.org/en/jsr/detail?id=170 JSR-170: Content Repository for Java(TM) Technology API]
*[http://www.jcp.org/en/jsr/detail?id=283 JSR-283: Content Repository for Java(TM) Technology API, version 2.0]
{{apache}}

[[Category:Apache Software Foundation|Jackrabbit]]
[[Category:Java enterprise platform]]
[[Category:Structured storage]]</text>
      <sha1>i5ko3yiiic2q936jsc0ka50uq9cq6uq</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Lenya</title>
    <ns>0</ns>
    <id>795725</id>
    <revision>
      <id>540474264</id>
      <parentid>519457069</parentid>
      <timestamp>2013-02-26T03:08:15Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 8 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q371501]] ([[User talk:Addbot|Report Errors]])</comment>
      <text xml:space="preserve" bytes="2421">{{No footnotes|date=August 2008}}
{{Infobox software
| name                   = Apache Lenya
| logo                   =
| screenshot             = &lt;!--  Commented out because image was deleted: [[Image:Apache-lenya-menubar.png|250px]] --&gt;
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| released               = ?
| frequently updated     = yes&lt;!-- Release version update? Don't edit this page, just click on the version number! --&gt;
| programming language   = [[Java (programming language)|Java]]/[[XML]]
| operating system       = [[Java platform|Java 2 Platform]] (OS-independent)
| language               = ?
| genre                  = [[Content management system]]
| license                = [[Apache License]] 2.0
| website                = http://lenya.apache.org
}}
'''Apache Lenya''' is a [[Java (programming language)|Java]]/[[XML]] [[open-source]] [[content management system]] based on the [[Apache Cocoon]] [[content management framework]].  Features include [[revision control]], scheduling, search capabilities, [[workflow]] support, and browser-based [[WYSIWYG]] editors.

Lenya was originally started by [[Michael Wechner]] in early 1999 to manage the content of the journal of pattern formation. Michael previously did basic research in physics by writing computer simulations on dendritic growth.

In early 2000 Michael co-founded [[Wyona]], which continued to develop Lenya on the basis of the interactive newspaper edition of [[Neue Zürcher Zeitung]]. The name Lenya is a combination of the names of his two sons Levi and Vanya.

In the spring of 2003, Wyona donated Lenya to the [[Apache Software Foundation]], where Lenya was incubated and became a Top Level Project in September 2004.

In 2006 Michael has started a new CMS called [[Yanel]] (an [[anagram]] of Lenya) featuring versioned interfaces as an approach to provide backwards compatibility at all times and hence replacing the classical approach of periodical releases by continuous deployment.

==See also==
{{Portal|Free software}}
*[[List of content management systems]]

==External links==
*[http://lenya.apache.org/ Apache Lenya website]
*[http://wiki.apache.org/lenya/ Apache Lenya wiki]

{{apache}}

[[Category:Free content management systems]]
[[Category:Apache Software Foundation|Lenya]]
[[Category:Free software programmed in Java]]

{{cms-software-stub}}</text>
      <sha1>a9wtmlwo9xcxsp5bblce4b9jnnlfyzg</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Maven</title>
    <ns>0</ns>
    <id>1333305</id>
    <revision>
      <id>600636914</id>
      <parentid>600636746</parentid>
      <timestamp>2014-03-21T19:25:13Z</timestamp>
      <contributor>
        <ip>108.18.140.134</ip>
      </contributor>
      <comment>/* Dependencies */</comment>
      <text xml:space="preserve" bytes="21075">{{Refimprove|date=March 2012}}
{{Infobox software
| name                   = Apache Maven
| logo                   = [[File:Maven logo.svg|220px]]
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| founder                = [[Jason van Zyl]]
| status                 = Active
| latest release version = 3.2.1&lt;ref&gt;[http://maven.apache.org/docs/3.2.1/release-notes.html Maven 3.2.1]&lt;/ref&gt;
| latest release date    = {{release date|2014|02|21}}&lt;ref&gt;[http://maven.apache.org/docs/history.html Maven Releases History]&lt;/ref&gt;
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Build tool]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://maven.apache.org/}}
}}
'''Maven''' is a [[build automation]] tool used primarily for [[Java (programming language)|Java]] projects. Maven addresses two aspects of building software: First, it describes how software is built, and second, it describes its dependencies. Contrary to preceding tools like [[Apache Ant]] it uses conventions for the build procedure, and only exceptions need to be written down. An [[XML]] file describes the software project being built, its dependencies on other external modules and components, the build order, directories, and required [[Plug-in (computing)|plug-ins]]. It comes with pre-defined targets for performing certain well-defined tasks such as compilation of code and its packaging. Maven dynamically downloads [[Java (programming language)|Java]] libraries and Maven plug-ins from one or more repositories such as the Maven 2 [http://central.sonatype.org Central Repository], and stores them in a local cache.&lt;ref name=&quot;maven2repo&quot;&gt;[http://repo1.maven.org/maven2/ Maven 2 Central Repository]&lt;/ref&gt;  This local cache of downloaded artifacts can also be updated with artifacts created by local projects.  Public repositories can also be updated.

Maven can be used to build and manage projects written in [[C Sharp (programming language)|C#]], [[Ruby (programming language)|Ruby]], [[Scala (programming language)|Scala]], and other languages. The Maven project is hosted by the [[Apache Software Foundation]], where it was formerly part of the [[Jakarta Project]].

Maven is built using a plugin-based architecture that allows it to make use of any application controllable through standard input.  Theoretically, this would allow anyone to write plugins to interface with build tools (compilers, unit test tools, etc.) for any other language.  In reality, support and use for languages other than Java has been minimal. Currently a plugin for the .NET framework exists and is maintained,&lt;ref&gt;[http://doodleproject.sourceforge.net/mavenite/dotnet-maven-plugin/index.html .NET Maven Plugin]&lt;/ref&gt; and a [[C (programming language)|C]]/[[C++]] native plugin is maintained for Maven 2.&lt;ref&gt;[http://mojo.codehaus.org/maven-native/native-maven-plugin/index.html maven-native C/C++ plugin] and [http://duns.github.com/maven-nar-plugin/ maven-nar C/C++ plugin]&lt;/ref&gt;

Superseding technologies like [[gradle]] and [[sbt]] as build tools do not rely on XML any more, but keep the key concepts Maven introduced. With [[Apache Ivy]] a dedicated dependency manager was developed as well.

==Example==
Maven projects are configured using a [[Project Object Model]], which is stored in a &lt;code&gt;pom.[[XML|xml]]&lt;/code&gt;-file. Here's a minimal example:

&lt;syntaxhighlight lang=&quot;xml&quot;&gt;
&lt;project&gt;
  &lt;!-- model version is always 4.0.0 for Maven 2.x POMs --&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;
  
  &lt;!-- project coordinates, i.e. a group of values which
       uniquely identify this project --&gt;
  
  &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt;
  &lt;artifactId&gt;my-app&lt;/artifactId&gt;
  &lt;version&gt;1.0&lt;/version&gt;

  &lt;!-- library dependencies --&gt;
  
  &lt;dependencies&gt;
    &lt;dependency&gt;
    
      &lt;!-- coordinates of the required library --&gt;
      
      &lt;groupId&gt;junit&lt;/groupId&gt;
      &lt;artifactId&gt;junit&lt;/artifactId&gt;
      &lt;version&gt;3.8.1&lt;/version&gt;
      
      &lt;!-- this dependency is only used for running and compiling tests --&gt;
      
      &lt;scope&gt;test&lt;/scope&gt;
      
    &lt;/dependency&gt;
  &lt;/dependencies&gt;
&lt;/project&gt;
&lt;/syntaxhighlight&gt;

This POM only defines a unique identifier for the project (''coordinates'') and its dependency on the [[JUnit]] framework. However, that is already enough for building the project and running the [[Unit testing|unit tests]] associated with the project. Maven accomplishes this by embracing the idea of [[Convention over Configuration]], that is, Maven provides default values for the project's configuration. The directory structure of a normal idiomatic Maven project has the following directory entries:
[[File:Maven CoC.svg|thumb|right|The Maven software tool auto-generated this directory structure for a Java project.]]
{| class=&quot;wikitable&quot;
|-
! Directory name
! Purpose
|-
| project home
| Contains the pom.xml and all subdirectories.
|-
| src/main/java
| Contains the deliverable Java sourcecode for the project.
|-
| src/main/resources
| Contains the deliverable resources for the project, such as property files.
|-
| src/test/java
| Contains the testing classes (JUnit or TestNG test cases, for example) for the project.
|-
| src/test/resources
| Contains resources necessary for testing.
|}

Then the command
&lt;syntaxhighlight lang=&quot;text&quot;&gt;
 mvn package
&lt;/syntaxhighlight&gt;
will compile all the Java files, run any tests, and package the deliverable code and resources into &lt;code&gt;target/my-app-1.0.jar&lt;/code&gt; (assuming the artifactId is my-app and the version is 1.0.)

Using Maven, the user provides only configuration for the project, while the configurable plug-ins do the actual work of compiling the project, cleaning target directories, running unit tests, generating API documentation and so on. In general, users should not have to write plugins themselves. Contrast this with [[Apache Ant|Ant]] and [[make (software)|make]], in which one writes imperative procedures for doing the aforementioned tasks.

==Concepts==

===Project Object Model===
A Project Object Model (POM) provides all the configuration for a single project. General configuration covers the project's name, its owner and its dependencies on other projects. One can also configure individual phases of the build process, which are implemented as [[plug-in (computing)|plugin]]s. For example, one can configure the compiler-plugin to use Java version 1.5 for compilation, or specify packaging the project  even if some unit test fails.

Larger projects should be divided into several modules, or sub-projects, each with its own POM. One can then write a root POM through which one can compile all the modules with a single command. POMs can also inherit configuration from other POMs. All POMs inherit from the Super POM&lt;ref&gt;[http://maven.apache.org/guides/introduction/introduction-to-the-pom.html#Super_POM Super POM]
&lt;/ref&gt; by default. The Super POM provides default configuration, such as default source directories, default plugins, and so on.

===Plugins===
Most of Maven's functionality is in [[Plug-in (computing)|plugins]]. A plugin provides a set of goals that can be executed using the following syntax:
&lt;syntaxhighlight lang=&quot;text&quot;&gt;
 mvn [plugin-name]:[goal-name]
&lt;/syntaxhighlight&gt;
For example, a Java project can be compiled with the compiler-plugin's compile-goal&lt;ref&gt;[http://maven.apache.org/plugins/maven-compiler-plugin/ Maven Compiler Plugin]&lt;/ref&gt; by running &lt;code&gt;mvn compiler:compile&lt;/code&gt;.

There are Maven plugins for building, testing, source control management, running a web server, generating [[Eclipse (software)|Eclipse]] project files, and much more.&lt;ref&gt;[http://maven.apache.org/plugins/index.html Maven - Available Plugins]&lt;/ref&gt; Plugins are introduced and configured in a &lt;plugins&gt;-section of a &lt;code&gt;pom.xml&lt;/code&gt; file. Some basic plugins are included in every project by default, and they have sensible default settings.

However, it would be cumbersome if the archetypical build sequence of building, testing and packaging a software project required running each respective goal manually:
&lt;syntaxhighlight lang=&quot;text&quot;&gt;
 mvn compiler:compile
 mvn surefire:test
 mvn jar:jar
&lt;/syntaxhighlight&gt;
Maven's lifecycle-concept handles this issue.

Plug-ins are the primary way to extend Maven.  Developing a Maven plug-in can be done by extending the org.apache.maven.plugin.AbstractMojo class.  Example code and explanation for a Maven plug-in to create a cloud-based virtual machine running an application server is given in the article ''Automate development and management of cloud virtual machines''.&lt;ref&gt;{{Cite journal| last=Amies| first=Alex| coauthors=Zou P X, Wang Yi S| title=Automate development and management of cloud virtual machines|journal=IBM developerWorks|publisher=IBM|date=29 Oct 2011| url=http://www.ibm.com/developerworks/cloud/library/cl-automatecloud/index.html}}&lt;/ref&gt;

===Build lifecycles===

Build lifecycle is a list of named ''phases'' that can be used to give order to goal execution. One of Maven's standard lifecycles is the ''default lifecycle'', which includes the following phases, in this order:&lt;ref&gt;[http://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html#Lifecycle_Reference Maven Build Lifecycle Reference]&lt;/ref&gt;
&lt;syntaxhighlight lang=&quot;text&quot; line&gt;
 process-resources
 compile
 process-test-resources
 test-compile
 test
 package
 install
 deploy
&lt;/syntaxhighlight&gt;
Goals provided by plugins can be associated with different phases of the lifecycle. For example, by default, the goal &quot;compiler:compile&quot; is associated with the compile-phase, while the goal &quot;surefire:test&quot; is associated with the test-phase. Consider the following command:
 
 mvn test

When the preceding command is executed, Maven runs all goals associated with each of the phases up to and including the test-phase. In such a case, Maven runs the &quot;resources:resources&quot;-goal associated with the process-resources-phase, then &quot;compiler:compile&quot;, and so on until it finally runs the &quot;surefire:test&quot;-goal.

Maven also has standard phases for cleaning the project and for generating a project site. If cleaning were part of the default lifecycle, the project would be cleaned every time it was built. This is clearly undesirable, so cleaning has been given its own lifecycle.

Standard lifecycles enable users new to a project the ability to accurately build, test and install every Maven-project by issuing the single command:
 
 mvn install

===Dependencies===
A central feature in Maven is [[library dependency|dependency management]].  Maven's dependency-handling mechanism is organized around a coordinate system identifying individual artifacts such as software libraries or modules.  The POM example above references the JUnit coordinates as a direct dependency of the project.  A project that needs, say, the [[Hibernate (Java)|Hibernate]]-library simply has to declare Hibernate's project coordinates in its POM. Maven will automatically download the dependency and the dependencies that Hibernate itself needs (called transitive dependencies) and store them in the user's local repository. Maven 2 [http://central.sonatype.org Central Repository]&lt;ref name=&quot;maven2repo&quot;/&gt; is used by default to search for libraries, but one can configure the repositories to be used (e.g., company-private repositories) within the POM.

There are search engines such as The Central Repository Search Engine&lt;ref&gt;[https://search.maven.org/ The Central Repository Search Engine],&lt;/ref&gt; which can be used to find out coordinates for different open-source libraries and frameworks.

Projects developed on a single machine can depend on each other through the local repository. The local repository is a simple folder structure which acts both as a cache for downloaded dependencies and as a centralized storage place for locally built artifacts. The Maven command &lt;code&gt;mvn install&lt;/code&gt; builds a project and places its binaries in the local repository. Then other projects can utilize this project by specifying its coordinates in their POMs.

== Maven compared with Ant ==
The fundamental difference between Maven and Ant is that Maven's design regards all projects as having a certain structure and a set of supported task work-flows (e.g., getting resources from source control, compiling the project, unit testing, etc.). While most software projects in effect support these operations and actually do have a well-defined structure, Maven requires that this structure and the operation implementation details be defined in the POM file. Thus, Maven [[Convention over configuration|relies on a convention]] on how to define projects and on the list of work-flows that are generally supported in all projects.&lt;ref&gt;{{cite web|title=Maven: The Complete Reference|url=http://www.sonatype.com/books/mvnref-book/reference/installation-sect-compare-ant-maven.html|publisher=Sonatype|accessdate=11 April 2013}}&lt;/ref&gt;

This design constraint resembles the way that an IDE handles a project, and it provides many benefits, such as a succinct project definition, and the possibility of automatic integration of a Maven project with other development tools such as IDEs, build servers, etc.

But one drawback to this approach is that Maven requires a user to first understand what a project is from the Maven point of view, and how Maven works with projects, because what happens when one executes a phase in Maven is not immediately obvious just from examining the Maven project file. In many cases, this required structure is also a significant hurdle in migrating a mature project to Maven, because it is usually hard to adapt from other approaches.

In Ant, projects do not really exist from the tool's technical perspective. Ant works with XML build scripts defined in one or more files. It processes targets from these files and each target executes tasks. Each task performs a technical operation such as running a compiler or copying files around. Targets are executed primarily in the order given by their defined dependency on other targets. Thus, Ant is a tool that chains together targets and executes them based on inter-dependencies and other Boolean conditions.

The benefits provided by Ant are also numerous. It has an XML language optimized for clearer definition of what each task does and on what it depends. Also, all the information about what will be executed by an Ant target can be found in the Ant script.

A developer not familiar with Ant would normally be able to determine what a simple Ant script does just by examining the script. This is not usually true for Maven.

However, even an experienced developer who is new to a project using Ant cannot infer what the higher level structure of an Ant script is and what it does without examining the script in detail. Depending on the script's complexity, this can quickly become a daunting challenge. With Maven, a developer who previously worked with other Maven projects can quickly examine the structure of a never-before-seen Maven project and execute the standard Maven work-flows against it while already knowing what to expect as an outcome.

It is possible to use Ant scripts that are defined and behave in a uniform manner for all projects in a working group or an organization. However, when the number and complexity of projects rises, it is also very easy to stray from the initially desired uniformity. With Maven this is less of a problem because the tool always imposes a certain way of doing things.

Note that it is also possible to extend and configure Maven in a way that departs from the Maven way of doing things.  This is particularly true for Maven 2 and newer releases, such as [http://maven.apache.org/guides/introduction/introduction-to-plugins.html Mojos] or more formally, [[Plug-in (computing)|plugins]] and custom project directory structures.

==IDE integration==
Add-ons to several popular [[Integrated development environment|Integrated Development Environment]]s exist to provide integration of Maven with the IDE's build mechanism and source editing tools, allowing Maven to compile projects from within the IDE, and also to set the classpath for code completion, highlighting compiler errors, etc. Examples of popular IDEs supporting development with Maven include:
* [[Eclipse (software)|Eclipse]]
* [[NetBeans]]
* [[IntelliJ|IntelliJ IDEA]]
* [[JBuilder]]
* [[JDeveloper]] (version 11.1.2)
* [[MyEclipse]]

These add-ons also provide the ability to edit the POM or use the POM to determine a project's complete set of dependencies directly within the IDE.

Some built-in features of IDEs are forfeited when the IDE no longer performs compilation. For example, Eclipse's JDT has the ability to recompile a single java source file after it has been edited. Many IDEs work with a flat set of projects instead of the hierarchy of folders preferred by Maven. This complicates the use of [[Software configuration management|SCM]] systems in IDEs when using Maven.&lt;ref&gt;[http://maven.apache.org/eclipse-plugin.html Eclipse plugins for Maven]&lt;/ref&gt;&lt;ref&gt;[http://www.jetbrains.com/idea/features/ant_maven.html#Maven_Integration IntelliJ IDEA - Ant and Maven support]&lt;/ref&gt;&lt;ref&gt;[http://wiki.netbeans.org/MavenBestPractices Best Practices for Apache Maven in NetBeans 6.x]&lt;/ref&gt;

==History==
Maven, created by [http://www.sonatype.com Sonatype's] Jason van Zyl, began as a subproject of [[Apache Turbine]] in 2002.  In 2003, it was voted on and accepted as a top level [[Apache Software Foundation]] project.  In July 2004, Maven's release was the critical first milestone, v1.0.  Maven 2 was declared v2.0 in October 2005 after about six months in beta cycles. Maven 3.0 was released in October 2010 being mostly backwards compatible with Maven 2.

==Future==
Maven 3.0 information began trickling out in 2008. After eight alpha releases, the first beta version of Maven 3.0 was released in April 2010.
Maven 3.0 has reworked the core Project Builder infrastructure such that the POM's file-based representation is now decoupled from its in-memory object representation.  This has expanded the possibility for Maven 3.0 add-ons to leverage non-XML based project definition files.  Languages suggested include [[Ruby (programming language)|Ruby]] (already in private prototype by Jason van Zyl), [[YAML]], and [[Groovy (programming language)|Groovy]].

Special attention has been paid to ensuring compatibility between Maven 2 and 3. For most projects, an upgrade to Maven 3 will not require any adjustments of their project structure. The first beta of Maven 3 saw the introduction of a parallel build feature which leverages a configurable number of cores on a multi-core machine and is especially suited for large multi-module projects.

==See also==
{{Portal|Free software}}
* [[Apache Continuum]], a continuous integration server which integrates tightly with Maven
* [[Apache Jelly]], a tool for turning XML into executable code
* [[Apache Ivy]], alternative dependency management tool for Java
* [[Gradle]], a build tool based on convention over configuration
* [[Jenkins (software)|Jenkins]]
* [[List of build automation software]]

==References==
{{Reflist}}

==Further reading==
{{Refbegin}}
* A free online book - {{cite web|last=O'Brien, et al.|first=Tim|title=Maven: The Complete Reference|url=http://www.sonatype.com/books/mvnref-book/reference/|work=Sonatype.com|publisher=Sonatype|accessdate=15 March 2013}}
* The anteater book: {{
cite book
| others                = Sonatype Company
| title                 = Maven: The Definitive Guide
| url                   = http://books.google.com/books?id=cBvZ4s72Z0gC
| accessdate            = 2013-04-17
| year                  = 2009
| publisher             = O'Reilly Media, Inc.
| isbn                  = 9780596551780
| pages                 = 470
}}
* A printed book - {{citation
| first = Jason 
| last = Van Zyl
| title = Maven: Definitive Guide
| author-link =
| publication-date =
| date = 2008-10-01
| edition = first
| volume =
| series =
| publication-place =
| place =
| publisher = [[O'Reilly Media]]
| pages = 468 
| page =
| id =
| isbn = 0-596-51733-5
| doi =
| oclc =
| url =
| accessdate =
}}
{{Refend}}

==External links==
*{{Official website|maven.apache.org}}
*[http://docs.codehaus.org/display/MAVENUSER/The+Maven+2+tutorial The Maven 2 tutorial: A practical guide for Maven 2 users] - tutorial at [http://www.codehaus.org Codehaus.org]
*[http://today.java.net/pub/a/today/2007/03/01/building-web-applications-with-maven-2.html Building Web Applications with Maven 2]
*[http://www.javaworld.com/javaworld/jw-05-2006/jw-0529-maven.html The Maven 2 POM demystified] - article at [[JavaWorld]]
*[http://www.php-maven.org/ Maven for PHP]

{{Apache}}

{{DEFAULTSORT:Maven}}
[[Category:Compiling tools]]
[[Category:Java development tools]]
[[Category:Apache Software Foundation]]
[[Category:Build automation]]
[[Category:Software using the Apache license]]</text>
      <sha1>rggq3urygohax497wmokthfxpb8za9a</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Xerces</title>
    <ns>0</ns>
    <id>93465</id>
    <revision>
      <id>586174231</id>
      <parentid>546327729</parentid>
      <timestamp>2013-12-15T11:21:44Z</timestamp>
      <contributor>
        <username>Wickorama</username>
        <id>7705463</id>
      </contributor>
      <comment>/* External links */ Category:Software using the Apache license</comment>
      <text xml:space="preserve" bytes="1824">{{Other uses}}
{{Infobox Software
| name                   = Apache Xerces
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 2.11.0 (Xerces J)&lt;br&gt;3.1.1 (Xerces C++)
| latest release date    = 30 November 2010 (Xerces J)&lt;br&gt;27 April 2010 (Xerces C++)
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   =
| genre                  = [[XML]] parser [[library (computer science)|library]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://xerces.apache.org/}}
}}

'''Xerces''' is [[Apache Software Foundation|Apache]]'s collection of software libraries for parsing, validating, serializing and manipulating [[XML]]. The library implements a number of standard [[application programming interface|API]]s for XML parsing, including [[Document Object Model|DOM]], [[Simple API for XML|SAX]] and SAX2. The implementation is available in [[Java (programming language)|Java]], [[C++]] and [[Perl]] programming languages.

==See also==
* [[Apache Software License]]
* [[Xalan]]
* [[Java XML]]

==External links==
*[http://xerces.apache.org/ Apache Xerces Project home]
**[http://xerces.apache.org/xerces2-j/ Xerces2 Java Project]
***[http://xerces.apache.org/xerces2-j/api.html Xerces2 Javadocs]
**[http://xerces.apache.org/xerces-c/ Xerces C++ Project]
**[http://xerces.apache.org/xerces-p/ Xerces Perl Project]
*[http://totheriver.com/learn/xml/xmltutorial.html Tutorial: XML with Xerces for Java]

{{Apache}}

[[Category:XML parsers]]
[[Category:Java platform]]
[[Category:Apache Software Foundation]]
[[Category:Java libraries]]
[[Category:C++ libraries]]
[[Category:Software using the Apache license]]</text>
      <sha1>2dphz28x686k1x6j1ttc7r78a9dmiri</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache JServ Protocol</title>
    <ns>0</ns>
    <id>4980158</id>
    <revision>
      <id>561120376</id>
      <parentid>554083395</parentid>
      <timestamp>2013-06-22T22:04:20Z</timestamp>
      <contributor>
        <ip>50.53.15.59</ip>
      </contributor>
      <comment>/* History */</comment>
      <text xml:space="preserve" bytes="2830">The '''Apache JServ Protocol''' ('''AJP''') is a [[binary protocol]] that can [[Proxy server|proxy]] inbound requests from a [[web server]] through to an [[application server]] that sits behind the web server.

It also supports some monitoring in that the web server can [[Ping (networking utility)|ping]] the application server. Web implementors typically use AJP in a [[Load balancing (computing)|load-balanced]] deployment where one or more front-end web servers feed requests into one or more application servers. Sessions are redirected to the correct application server using a routing mechanism wherein each application server instance gets a name (called a ''route''). In this scenario the web server functions as a [[reverse proxy]] for the application server.

AJP runs in [[Apache HTTP Server]] 1.x using the [[mod_jk]] [[Plug-in (computing)|plugin]] and in Apache 2.x using the provided Proxy AJP, [[mod_proxy]] and proxy balancer modules together. Implementations exist for the not-yet-released [[lighttpd]] version 1.5,&lt;ref&gt;[http://redmine.lighttpd.net/projects/1/wiki/Docs_ModProxyCore lighttpd ModProxyCore]&lt;/ref&gt; [[nginx]],&lt;ref&gt;[https://github.com/yaoweibin/nginx_ajp_module Weibin Yao: nginx_ajp_module]&lt;/ref&gt; [[GlassFish|Grizzly]] 2.1,&lt;ref&gt;{{cite web | url=https://grizzly.java.net/nonav/docs/docbkx2.3/html/ajp.html | title=AJP | publisher=java.net | work=Grizzly 2.3 User's Guide | accessdate=2013-04-29}}&lt;/ref&gt; and the [[Internet Information Server]].&lt;ref&gt;[http://boncode.net/connector/webdocs/Tomcat_Connector.htm BonCode AJP 1.3 Connector]&lt;/ref&gt;

Both the [[Apache Tomcat]] servlet container as well as the [[Jetty (web server)|Jetty]] servlet container support AJP.

==History==
This protocol was originally created by Alexei Kosut in July 1997.&lt;ref&gt;Kosut, Alexei. &quot;Apache JServ Protocol&quot;, Version 1.0. Java Apache Project. July 1997.&lt;/ref&gt;&lt;ref name=&quot;AJPv2&quot;&gt;[http://isu.ifmo.ru/docs/IAS904/web.904/q20202/protocol/AJPv21.html Apache JServ Protocol Version 2.1 (AJPv2.1) - June 30, 1998 - Draft]&lt;/ref&gt; He also created the first implementations of it in the same month with the releases of the Apache JServ Servlet Engine 0.9 and the Apache mod_jserv 0.9a (released on July 30, 1997).&lt;ref&gt;[http://isu.ifmo.ru/docs/IAS904/web.904/q20202/changes.html Apache mod_jserv - History of Changes]&lt;/ref&gt;

An attempt was made at revamping the protocol to a second version in 1998&lt;ref name=&quot;AJPv2&quot;/&gt; but seemingly it remains with just minor updates at version 1.3.

==References==
{{Reflist}}

==External links==
* [http://tomcat.apache.org/connectors-doc/ajp/ajpv13a.html The Apache Tomcat Connector - AJP Protocol Reference] AJPv13
* [http://tomcat.apache.org/tomcat-3.3-doc/AJPv13.html Apache JServ Protocol version 1.3] Dan Milstein, December 2000.

{{Web interfaces}}

[[Category:Apache Software Foundation|JServ]]</text>
      <sha1>h5zfm94eu88jo0kosg1a9p80tyej3na</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>ApacheBench</title>
    <ns>0</ns>
    <id>3348493</id>
    <revision>
      <id>601479082</id>
      <parentid>595150958</parentid>
      <timestamp>2014-03-27T08:41:55Z</timestamp>
      <contributor>
        <username>Thorwald</username>
        <id>74279</id>
      </contributor>
      <minor/>
      <comment>/* Example Usage */ no need for &quot;blockquote&quot; here</comment>
      <text xml:space="preserve" bytes="2086">{{notability|date=December 2012}}
{{unreferenced|date=December 2012}}
'''ApacheBench''' (&lt;tt&gt;ab&lt;/tt&gt;) is a single-threaded command line computer program for [[Web server benchmarking|measuring the performance]] of [[HyperText Transfer Protocol|HTTP]] [[web server]]s.  Originally designed to test the [[Apache HTTP Server]], it is generic enough to test any web server.

The &lt;tt&gt;ab&lt;/tt&gt; tool comes bundled with the standard Apache source distribution, and like the Apache web server itself, is free, [[open source]] software and distributed under the terms of the [[Apache License]].

==Example Usage==
&lt;source lang=&quot;javascript&quot;&gt;
ab -n 100 -c 10 http://www.yahoo.com/
&lt;/source&gt;

This will execute 100 HTTP GET requests, processing up to 10 requests concurrently, to the specified URL, in this example, &quot;&lt;nowiki&gt;http://www.yahoo.com/&lt;/nowiki&gt;&quot;.

==Concurrency Versus Threads==
Note that ApacheBench will only use one operating system thread regardless of the concurrency level (specified by the &lt;tt&gt;-c&lt;/tt&gt; parameter).  In some cases, especially when benchmarking high-capacity servers, a single instance of ApacheBench can itself be a bottleneck.  When using ApacheBench on hardware with multiple processor cores, additional instances of ApacheBench may be used in parallel to more fully saturate the target URL.

==Detecting ApacheBench==

The ApacheBench [[User agents|User Agent]] string is the following :

&lt;tt&gt;ApacheBench/MAJOR.MINOR&lt;/tt&gt;

where MAJOR and MINOR represent the major and minor version numbers of the program.  It is usually not correctly categorised by web server log analysers such as [[Webalizer]] or [[AWStats]], so running ApacheBench with a great number of requests may skew the results of the reports generated by these programs.

==See also==
{{Portal|Software Testing}}
* [[Web server benchmarking]]

==External links==
* [http://httpd.apache.org/docs/2.4/programs/ab.html Manual page for the 'ab' tool]
* [http://httpd.apache.org/ Apache HTTPD official website]

[[Category:Apache Software Foundation]]
[[Category:Load testing tools]]

{{web-software-stub}}</text>
      <sha1>tffpgjy3nvz5bsrmw7ailct3bvaokvw</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Axis</title>
    <ns>0</ns>
    <id>1888026</id>
    <revision>
      <id>556072459</id>
      <parentid>546510923</parentid>
      <timestamp>2013-05-21T08:25:14Z</timestamp>
      <contributor>
        <ip>151.22.38.72</ip>
      </contributor>
      <comment>fixed url</comment>
      <text xml:space="preserve" bytes="5382">{{Infobox software
| name                   = Apache Axis
| logo                   = [[File:Apache Axis Logo.jpg|150px|Apache Axis Logo]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 1.4
| latest release date    = {{release date|2006|04|22}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]] and [[C++]]
| genre                  = [[Web service]]
| license                = [[Apache License]] 2.0
| website                = http://ws.apache.org/axis
}}

'''Apache Axis''' ('''A'''pache e'''X'''tensible '''I'''nteraction '''S'''ystem) is an [[open source]], [[XML]] based [[Web service]] framework. It consists of a [[Java (programming language)|Java]] and a [[C++]] implementation of the [[SOAP (protocol)|SOAP]] server, and various utilities and [[API]]s for generating and deploying [[WWW|Web]] service applications. Using Apache Axis, developers can create interoperable, distributed computing applications. Axis is developed under the auspices of the [[Apache Software Foundation]].

==Axis for Java==
When using the Java version of Axis there are two ways to expose Java code as Web service. The easiest one is to use Axis native JWS (Java Web Service) files.
Another way is to use custom deployment. Custom deployment enables you to customize resources that should be exposed as Web service.

See also [[Apache Axis2]].

===JWS Web service creation===
JWS files contain Java class source code that should be exposed as Web service. The main difference between an ordinary java file and jws file is the file extension. Another difference is that jws files are deployed as [[source code]] and not compiled [[class file]]s.

The following example is taken from http://axis.apache.org/axis/java/user-guide.html#Publishing_Web_Services_with_Axis .
It will expose methods ''add'' and ''subtract'' of class Calculator.
&lt;source lang=&quot;java&quot;&gt;
 public class Calculator 
 {
   public int add(int i1, int i2) 
   {
     return i1 + i2; 
   }
 
   public int subtract(int i1, int i2) 
   {
     return i1 - i2;
   }
 }
&lt;/source&gt;

====JWS Web service deployment====
Once the Axis servlet is deployed, you need only to copy the jws file to the Axis directory on the server. This will work if you are using an
[[Apache Tomcat]] container. In the case that you are using another web container, custom [[WAR (Sun file format)|WAR]] archive creation will be required  .

====JWS Web service access====
JWS Web service is accessible using the URL &lt;nowiki&gt;http://localhost:8080/axis/Calculator.jws&lt;/nowiki&gt; . If you are running a custom configuration of [[Apache Tomcat]] or a different container, the URL might be different.

===Custom deployed Web service===
Custom Web service [[Software deployment|deployment]] requires a specific deployment descriptor called WSDD (Web Service Deployment Descriptor) syntax. It can be used to specify resources that should be exposed as Web services. Current version (1.3) supports
* [[Remote procedure call|RPC]] services
* EJB - stateless ([[Enterprise Java Bean]])

====Automated generation of WSDL====
When a Web service is exposed using Axis it will generate a [[Web Services Description Language|WSDL]] file automatically when accessing the Web service URL with ''?WSDL'' appended to it.

==Axis for C++==
An example for implementing and deploying a simple web-service with the C++ version of Axis can be found in the Axis-CPP Tutorial (link in the Reference section below).

The steps necessary are:
* Create the wsdl file
* Generate client and server stubs using wsdl2ws
* Provide the server side web service implementation (e.g. the add method of the calculator service)
* Build the server side code and update the generated deploy.wsdd with the .dll path
* Deploy the binaries to the directory specified in the wsdd
* Build client
* Run and enjoy...

For more information on the individual steps go directly to the tutorial.

==Related technologies==
* [[Apache Axis2]] - re-design/write of Axis
* [[Java Web Services Development Pack]] - web services framework
* [[Apache CXF]] - other Apache web services framework (old [[Codehaus XFire|XFire]] &amp; [[Celtix]])
* [[XML Interface for Network Services]] - RPC/web services framework
* [[Web Services Invocation Framework]] - Java API for invoking Web services
* [[webMethods Glue]] - commercial web services enabling product
* AlchemySOAP - open source C++ web services framework

==See also==
{{Portal|Java}}
&lt;!-- formatting; please do not remove until some more text lines are added to compensate spacing --&gt;
*[[Apache Axis2]]

==External links==
* [http://ws.apache.org/axis/ Apache AXIS Homepage] at the Apache Software Foundation
* [http://ws.apache.org/axis/cpp/ Apache AXIS C++ Homepage] at the Apache Software Foundation
* [http://ws.apache.org/axis/cpp/arch/End-2-End-Sample.html] Axis-C++ tutorial at the Apache Software Foundation
* [http://ws.apache.org/axis2/ Apache Axis2/Java] at the Apache Software Foundation
* [http://ws.apache.org/axis2/c/ Apache Axis2/C] at the Apache Software Foundation
* [http://www.stylusstudio.com/open_ws_framework.html Stylus Studio Tools for Apache AXIS], see also Stylus Studio

{{Apache}}

[[Category:Apache Software Foundation|Axis]]
[[Category:Web services]]
[[Category:Web service specifications]]</text>
      <sha1>qor97xoy3l89ruix9teyxv6w2tdal0m</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Batik (software)</title>
    <ns>0</ns>
    <id>5145427</id>
    <revision>
      <id>540944344</id>
      <parentid>527510916</parentid>
      <timestamp>2013-02-27T13:55:34Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 5 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q810829]] ([[User talk:Addbot|Report Errors]])</comment>
      <text xml:space="preserve" bytes="3998">{{Infobox software
| name                   = Batik
| logo                   = [[Image:Batik (software) logo.png|190px]]
| screenshot             = [[Image:Batik Screenshot.png|250px]]
| caption                = Batik running Solitaire Sample
| collapsible            = yes
| developer              = [[Apache Software Foundation]]
| latest release version = 1.7
| latest release date    = {{release date and age|2008|01|10}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| language               =
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[SVG|Scalable Vector Graphics (SVG)]]
| license                = [[Apache License]] 2.0
| website                = http://xmlgraphics.apache.org/batik
}}

'''Batik''' is a pure-[[Java (programming language)|Java]] library that can be used to render, generate, and manipulate [[Scalable Vector Graphics|SVG]] graphics.  (SVG is an [[XML]] markup language for describing two-dimensional [[vector graphics]].) IBM supported the project and then donated the code to the [[Apache Software Foundation]], where other companies and teams decided to join efforts.
Batik provides a set of core modules that provide functionality to:

* render and dynamically modify SVG content,
* transcode SVG content to some raster [[Graphics file format]]s, such as [[Portable Network Graphics|PNG]], [[JPEG]] and [[TIFF]],
* transcode [[Windows Metafile]]s to SVG (WMF or Windows Metafile Format is the vector format used by [[Microsoft Windows]] applications),
* and manage scripting and user events on SVG documents.

The Batik distribution also contains a ready-to-use SVG browser (called Squiggle) making use of the above modules.

The name of the library comes from the [[Batik|Batik painting technique]].

==Status==
Batik was long the most conformant existing [http://www.w3.org/TR/SVG11/ SVG 1.1] implementation&lt;ref&gt;[http://xmlgraphics.apache.org/batik/status.html Batik 1.7 Status]&lt;/ref&gt;&lt;ref&gt;{{cite web
|url=http://codedread.com/svg-support.php
|title=Welcome To CodeDread 1.1
|last=Schiller
|first=Jeff
|date=2009-01-18
|accessdate=2009-02-08
| archiveurl= http://web.archive.org/web/20090216041528/http://www.codedread.com/svg-support.php| archivedate= 16 February 2009 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;&lt;ref&gt;almost 94% of the official SVG tests are OK with the 1.7 version.&lt;/ref&gt; and {{as of | 2011 | lc = on }} is just a small fraction behind [[Opera (web browser)|Opera]].{{Citation needed|date=April 2011}}

The latest 1.7 version, made available on January 10, 2008, has an &quot;almost full&quot; implementation of the current state of the [[sXBL]] specification,&lt;ref&gt;[http://www.apache.org/dist/xmlgraphics/batik/README.txt Batik 1.7 Readme]&lt;/ref&gt; a nearly complete implementation of SVG [http://www.w3.org/TR/SVG11/animate.html#AnimateElementsIntro declarative animation] [[Synchronized Multimedia Integration Language|SMIL]] features, and some of the [http://www.w3.org/TR/2004/WD-SVG12-20041027/ SVG 1.2] late October 2004 working draft (see [[Scalable Vector Graphics#Development history|SVG's Development history]]).

==See also==
{{Portal|Free software}}
*[[Scalable Vector Graphics]]
*[[Synchronized Multimedia Integration Language]]
*[[sXBL]] : a mechanism for defining the presentation and interactive behavior of elements described in a namespace other than SVG's
*[[Comparison of layout engines (SVG)]]

==References==
{{Reflist|2}}

==External links==
*[http://xmlgraphics.apache.org/batik/ Apache Batik Project]
*[http://wiki.apache.org/xmlgraphics-batik/SupportedSVG12Features Current status of Batik's sXBL implementation]
*[http://www.w3.org/Graphics/SVG/ The official SVG page at W3C], SVG Working Group

{{Apache}}
{{SVG Plugins}}

[[Category:Apache Software Foundation|Batik]]
[[Category:Graphics libraries]]
[[Category:Java platform]]
[[Category:Free software programmed in Java]]
[[Category:Scalable Vector Graphics]]
[[Category:Java libraries]]</text>
      <sha1>dt34oz87yn0a65bk6nbwi9l3qczgo4u</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Incubator</title>
    <ns>0</ns>
    <id>1641276</id>
    <revision>
      <id>576186571</id>
      <parentid>568610845</parentid>
      <timestamp>2013-10-07T19:48:21Z</timestamp>
      <contributor>
        <username>AdamGomaa</username>
        <id>1839867</id>
      </contributor>
      <comment>Remove stale project listing per talk page.</comment>
      <text xml:space="preserve" bytes="1383">{{Refimprove|date=February 2012}}

'''Apache Incubator''' is the gateway for [[open-source]] projects intended to become fully fledged [[Apache Software Foundation]] projects.

The Incubator project was created in October 2002 to provide an entry path to the Apache Software Foundation for projects and codebases wishing to become part of the Foundation's efforts. All code donations from external organizations and existing external projects wishing to move to Apache must enter through the Incubator.

The Apache Incubator project serves on the one hand as a temporary container project until the incubating project is accepted and becomes a top-level project of the [[Apache Software Foundation]] or becomes subproject of a proper project such as the [[Jakarta Project]] or [[Apache XML]]. On the other hand, the Incubator project documents how the Foundation works, and how to get things done within its framework. This means documenting process, roles and policies within the [[Apache Software Foundation]] and its member projects.


==External links==
* [http://incubator.apache.org/ Apache Incubator]
** [http://incubator.apache.org/projects/ All projects in incubator]
* [http://wiki.apache.org/incubator/OpenOfficeProposal Apache Incubator OpenOfficeProposal]

{{apache}}

&lt;!--Interwikies--&gt;

[[Category:Apache Software Foundation|Incubator]]
[[Category:Computing websites]]</text>
      <sha1>fr4l658h35zytm523f5jrndlcsy4c3w</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache POI</title>
    <ns>0</ns>
    <id>169055</id>
    <revision>
      <id>596416851</id>
      <parentid>596304050</parentid>
      <timestamp>2014-02-21T00:09:52Z</timestamp>
      <contributor>
        <username>Meters</username>
        <id>3632083</id>
      </contributor>
      <comment>Undid revision 594837545 by [[Special:Contributions/159.53.174.140|159.53.174.140]] ([[User talk:159.53.174.140|talk]])dubious,  restore original</comment>
      <text xml:space="preserve" bytes="7323">{{Infobox software
| name                   = Apache POI
| logo                   = [[Image:Jakarta POI Logo.gif|170px|Jakarta POI Logo]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 3.10
| latest release date    = {{release date|2014|02|08}}&lt;ref&gt;{{cite web |url=http://www.apache.org/dist/poi/release/RELEASE-NOTES.txt |title= Apache POI 3.10 released |date=2013-02-08 |accessdate=2014-02-20}}&lt;/ref&gt;
| latest preview version = 3.10 beta2
| latest preview date    = {{release date|2013|09|19}}&lt;ref&gt;{{cite web |url=https://mail-archives.apache.org/mod_mbox/poi-user/201309.mbox/%3C1379602209.87112.YahooMailNeo%40web160705.mail.bf1.yahoo.com%3E |title=[ANNOUNCE] Apache POI 3.10 Beta 2 released |author=Timothy Allison |date=2013-09-19 |accessdate=2014-02-03}}&lt;/ref&gt;
| operating system       = [[Cross-platform]]
| programming language   =
| genre                  = [[API]] to access [[Microsoft Office]] [[file format|formats]]
| license                = [[Apache License]] 2.0
| website                = http://poi.apache.org
}}
'''Apache POI''', a project run by the [[Apache Software Foundation]], and previously a sub-project of the [[Jakarta Project]], provides pure [[Java platform|Java]] libraries for reading and writing files in [[Microsoft Office]] [[file format|formats]], such as [[Microsoft Word|Word]], [[Microsoft PowerPoint|PowerPoint]] and [[Microsoft Excel|Excel]].

==History and roadmap==
The name was originally an [[acronym]] for &quot;Poor Obfuscation Implementation&quot;,&lt;ref&gt;{{citation | url = http://www.javaworld.com/javaworld/jw-03-2004/jw-0322-poi.html | title = Excelling in Excel with Java | first = Elango | last = Sundaram | publisher = Java World | date = 2004-03-22}}.&lt;/ref&gt; referring humorously to the fact that the file formats seemed to be deliberately [[Obfuscated code|obfuscated]], but poorly, since they were successfully [[reverse engineering|reverse-engineered]]. This explanation – and those of the similar names for the various sub-projects – were removed from the official web pages in order to better market the tools to businesses who would not consider such humor appropriate.  The original authors ([[Andrew C. Oliver]] and [[Marc Johnson]]) also noted the existence of the Hawaiian [[poi (food)|poi]] dish, made of mashed [[taro root]], which had similarly derogatory connotations.&lt;ref&gt;{{citation | url = http://web.archive.org/web/20041015142404/http://www.coyotesong.com/poi/ | title = POI homepage from October 2004 | publisher = Coyote Song}}, showing original explanations for naming.&lt;/ref&gt;

===Office Open XML support===
POI supports the ISO/IEC 29500:2008 [[Office Open XML]] file formats since version 3.5.  A significant contribution for OOXML support came from Sourcesense,&lt;ref&gt;{{citation | url = http://www.sourcesense.com/ | title = SourceSense}}.&lt;/ref&gt; an [[open source]] company which was commissioned by [[Microsoft]] to develop this contribution.&lt;ref&gt;{{cite web
| url=http://www.informationweek.com/news/windows/showArticle.jhtml?articleID=206905858
| title= Microsoft Eyes Open Source Components for Office 2007
| publisher= Information Week
| date=26 March 2008
| accessdate=1 March 2009}}&lt;/ref&gt;  This link spurred controversy, some POI contributors questioning POI OOXML patent protection regarding Microsoft's [[Open Specification Promise]] patent license.&lt;ref&gt;{{citation | url = http://mail-archives.apache.org/mod_mbox/poi-dev/200803.mbox/%3c47EAE71C.6040603@buni.org%3e | title = POI development mailing list archives | date = March 2008}}.&lt;/ref&gt;

==Architecture==
The Apache POI project contains the following subcomponents (meaning of acronyms is taken from old documentation):

* POIFS (Poor Obfuscation Implementation File System) – This component reads and writes [[Microsoft]]'s [[OLE 2]] [[COM Structured storage|Compound document]] format.  Since all [[Microsoft Office]] files are [[OLE 2]] files, this component is the basic building block of all the other POI elements. POIFS can therefore be used to read a wider variety of files, beyond those whose explicit decoders are already written in POI.
* HSSF (Horrible SpreadSheet Format) – reads and writes [[Microsoft Excel]] (XLS) format files. It can read files written by  [[Microsoft Excel|Excel]] 97 onwards; this [[file format]] is known as the ''BIFF 8'' format. As the Excel file format is complex and contains a number of tricky characteristics, some of the more advanced features cannot be read.
* XSSF (XML SpreadSheet Format) – reads and writes [[Office Open XML]] (XLSX) format files.  Similar feature set to HSSF, but for Office Open XML files.
* HPSF (Horrible Property Set Format) – reads &quot;Document Summary&quot; information from [[Microsoft Office]] files. This is essentially the information that one can see by using the ''File|Properties'' menu  item within an [[Microsoft Office|Office]] application.
* HWPF (Horrible Word Processor Format) – aims to read and write [[Microsoft Word|Microsoft Word 97]] (DOC) format files. This component is in initial stages of development.
* HSLF (Horrible Slide Layout Format) – a pure Java implementation for [[Microsoft PowerPoint]] files. This provides the ability to read, create and edit presentations (though some things are easier to do than others)
* HDGF (Horrible DiaGram Format) – an initial pure Java implementation for [[Microsoft Visio]] binary files. It provides an ability to read the low level contents of the files.
* HPBF (Horrible PuBlisher Format) – a pure Java implementation for Microsoft Publisher files.
* HSMF (Horrible Stupid Mail Format&lt;ref&gt;{{citation | url = http://npoi.codeplex.com/discussions/233518 | publisher = Microsoft | title = Codeplex NPOI}}.&lt;/ref&gt;{{Better source|date=January 2012}}) – a pure Java implementation for Microsoft Outlook MSG files.&lt;ref&gt;{{citation | title = POI-HSMF | publisher = Apache | url = http://poi.apache.org/hsmf/}}.&lt;/ref&gt;
* DDF (Dreadful Drawing Format) – a package for decoding the Microsoft Office Drawing format.

The HSSF component is the most advanced feature of the library.&lt;ref&gt;{{citation | url = http://poi.apache.org/hssf/ | title = POI-HSSF | publisher = Apache}}.&lt;/ref&gt;  Other components (HPSF, HWPF, and HSLF) are usable, but less full-featured.&lt;ref&gt;{{citation | url = http://poi.apache.org/hwpf/ | title = POI-HWPF | publisher = Apache}}.&lt;/ref&gt;&lt;ref&gt;{{citation | url = http://poi.apache.org/hslf/ | title = POI-HSLF | publisher = Apache}}.&lt;/ref&gt;

The POI library is also provided as a [[Ruby (programming language)|Ruby]]&lt;ref&gt;{{citation | title = POI-Ruby | publisher = Apache | url = http://poi.apache.org/poi-ruby.html}}.&lt;/ref&gt; or [[ColdFusion]] extension.

==See also==
*[[Open Packaging Conventions]]
*[[Office Open XML software]]

==References==
{{Reflist|2}}

==External links==
* {{citation | url = https://poi.apache.org/ | title = Apache POI}} – the official Apache POI project page.
* {{citation | url = http://oscon2006.sourceforge.net/ | title = POI presentation at OSCON2006 | publisher = Source Forge}}.

{{Apache}}

{{DEFAULTSORT:Apache Poi}}
[[Category:Apache Software Foundation|POI]]
[[Category:Microsoft Office-related software]]
[[Category:Java platform]]
[[Category:Java libraries|POI]]
[[Category:Cross-platform free software]]</text>
      <sha1>msxsid4zyy7nxhfq6i7eu8e71c47zls</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Rivet</title>
    <ns>0</ns>
    <id>2253508</id>
    <revision>
      <id>581938804</id>
      <parentid>510187262</parentid>
      <timestamp>2013-11-16T17:50:32Z</timestamp>
      <contributor>
        <ip>123.202.98.244</ip>
      </contributor>
      <text xml:space="preserve" bytes="1013">'''Apache Rivet''' is an open source programming system that allows developers of web sites to use [[Tcl]] as a scripting language for creating dynamic web sites. Rivet is similar to [[PHP]], [[Active Server Pages|ASP]], and [[JavaServer Pages|JSP]].  Rivet was primarily developed by [[Damon Courtney]], [[David Welton]], and [[Karl Lehenbauer]]. Current developers are [[Massimo Manghi]], [[Harald Oehlmann]] and [[Karl Lehenbauer]]

Rivet can use any of the thousands of publicly available [[Tcl]] packages that offer countless features such as database interaction ([[Oracle database|Oracle]], [[PostgreSQL]], [[MySQL]], [[SQLite]], etc.), or interfaces to popular applications such as the [[GD Graphics Library]].

==Notable Web Sites using Rivet==
*[[FlightAware]]

==External links==
*[http://tcl.apache.org/rivet/ Official Rivet web site]
{{Apache}}
[[Category:Apache Software Foundation|Rivet]]
[[Category:Scripting languages]]
[[Category:Dynamic programming languages]]
[[Category:Apache httpd modules]]</text>
      <sha1>m1zurptr9bdydbn4r1nu2qf1c3iymtw</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Jakarta Slide</title>
    <ns>0</ns>
    <id>332045</id>
    <revision>
      <id>499629928</id>
      <parentid>472883414</parentid>
      <timestamp>2012-06-27T17:26:57Z</timestamp>
      <contributor>
        <username>Edward</username>
        <id>4261</id>
      </contributor>
      <minor/>
      <comment>link [[JavaBeans]] using [[User:Edward/Find link|Find link]]</comment>
      <text xml:space="preserve" bytes="2138">{{Primary sources|date=August 2008}}
{{Infobox software
| name                   = Jakarta Slide
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Retired
| latest release version =  5.0 M4
| latest release date    = {{release date|2007|12|19}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Content management system]]
| license                = [[Apache License]] 2.0
| website                = http://jakarta.apache.org/slide
}}
'''Slide''' is an open-source [[content management system]] from the [[Jakarta Project|Jakarta]] project. It is written in [[Java (programming language)|Java]] and implements the [[WebDAV]] protocol. Slide is a set of APIs to implement the WebDAV client. Because of this, Slide can also be seen as a Content Management Framework. The use of WebDAV, which is a superset of [[HTTP]], makes Slide an ideal candidate for web-based content management. Among the applications of Slide are its use as a file server, in intranet applications, and as an excellent repository for XML both as properties and versioned files for persistence of [[JavaBeans]]. It also has an extensible storage mechanism that can be used for Integration and adaptation.

The Apache Jakarta PMC has announced the retirement
of the Jakarta Slide subproject at 2007-11-03. An alternative implementation that is actively maintained is the WebDAV component of the [[Apache Jackrabbit]] project that provides Java-based content repository software.

==External links==
{{Portal|Free software}}
*[http://jakarta.apache.org/slide/ Official Slide website at apache.org]
*[http://jackrabbit.apache.org/doc/components/webdav.html Apache Jackrabbit library at apache.org]
{{Apache}}

{{DEFAULTSORT:Slide}}
[[Category:Free content management systems]]
[[Category:Apache Software Foundation]]
[[Category:Java platform software]]
[[Category:Cross-platform software]]

{{cms-software-stub}}</text>
      <sha1>eoclycqdvwky5hnlyv65li8k668gx79</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Tapestry</title>
    <ns>0</ns>
    <id>2052309</id>
    <revision>
      <id>598849256</id>
      <parentid>597601837</parentid>
      <timestamp>2014-03-09T15:54:51Z</timestamp>
      <contributor>
        <ip>86.213.189.6</ip>
      </contributor>
      <comment>removed dead link - Page not found</comment>
      <text xml:space="preserve" bytes="16325">{{Infobox software
| name                   = Apache Tapestry
| logo                   = [[File:Tapestry.png]]
| screenshot             = 
| caption                = &quot;Tapestry 5: Code Less, Deliver More&quot;
| author                 = [[Howard Lewis Ship]]
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 5.3.7
| latest release date    = {{release date|2013|04|29}}
| operating system       = [[Cross-platform]] ([[Java Virtual Machine]])
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Web application framework|Web Framework]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://tapestry.apache.org/}}
}}
'''Apache Tapestry''' is an [[open-source]] component-oriented [[Java (programming language)|Java]] [[web application framework]] conceptually similar to [[JavaServer Faces]] and [[Apache Wicket]].&lt;ref&gt;{{cite web|url=http://devrates.com/post/show/345948/howard-lewis-ship-of-tapestry-interview-%5Bpart-1%5D|title=Howard Lewis Ship of Tapestry interview [part 1] (2012-10-22)|accessdate=2013-01-28 }}&lt;/ref&gt; Tapestry was created by Howard Lewis Ship, and was adopted by the [[Apache Software Foundation]] as a top-level project in 2006.&lt;ref&gt;Drobiazko 2012, p. 1.&lt;/ref&gt;

Tapestry emphasizes simplicity, ease of use, and developer productivity. It adheres to the [[Convention over Configuration]] paradigm, eliminating almost all XML configuration.&lt;ref&gt;http://tapestryjava.blogspot.com/2006/07/tapestry-5-updates.html&lt;/ref&gt; Tapestry uses a modular approach to web development, by having a strong [[UI data binding|binding]] between each [[user interface]] component (object) on the web page and its corresponding [[Java (programming language)|Java]] class. This component-based architecture borrows many ideas from [[WebObjects]].&lt;ref&gt;Tapestry in Action - Preface by Howard Lewis Ship&lt;/ref&gt;

== Notable Features ==

; Live Class Reloading : Tapestry monitors the file system for changes to Java page classes, component classes, service implementation classes, HTML templates and component property files, and it hot-swaps the changes into the running application without requiring a restart. This provides a very short code-save-view feedback cycle that is claimed to greatly improve developer productivity.&lt;ref&gt;http://tapestry.apache.org/class-reloading.html&lt;/ref&gt;
; Component-based : Pages may be constructed with small nestable components, each having a template and component class. Custom components are purportedly trivial to construct.&lt;ref&gt;Drobiazko 2012, p. 20.&lt;/ref&gt;
; Convention over configuration : Tapestry uses naming conventions and annotations, rather than XML, to configure the application.&lt;ref&gt;Drobiazko 2012, p. 7.&lt;/ref&gt;
; Spare use of HTTPSession : By making minimal use of the HTTPSession, Tapestry is designed to be highly efficient in a clustered, session-replicated environment.&lt;ref&gt;http://tapestry.apache.org/performance-and-clustering.html&lt;/ref&gt;
; Post/Redirect/Get : Most form submissions follow the [[Post/Redirect/Get]] (PRG) pattern, which reduces multiple form submission accidents and makes URLs friendlier and more bookmarkable, along with enabling the browser Back and Refresh buttons to operate normally.&lt;ref&gt;http://tapestry.apache.org/forms-and-validation.html&lt;/ref&gt;
; Inversion of Control (IOC) : Tapestry is built on a lightweight [[Inversion of Control]] layer with similarities to [[Google Guice]] but designed to make nearly all aspects of Tapestry's behavior configurable and replaceable.&lt;ref&gt;Drobiazko 2012, p. 7.&lt;/ref&gt;

== Hello World Example ==

A minimal, templated, Tapestry application needs only three files:

; HelloWorld.tml
: The (X)HTML template for the /helloworld page. Tapestry templates can contain any well-formed (X)HTML markup.
 &lt;source lang=&quot;xml&quot;&gt;
&lt;!DOCTYPE html&gt;
&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; 
      xmlns:t=&quot;http://tapestry.apache.org/schema/tapestry_5_3.xsd&quot;&gt;
&lt;body&gt;
    &lt;p&gt;Hello, ${username}&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;/source&gt;

; HelloWorld.java
: The page class associated with the template.  Here it merely provides a *username* property that the template can access.
&lt;source lang=&quot;java&quot;&gt;
package org.example.demo.pages;

/** a page class (automatically associated with the template file of the same name) */
public class HelloWorld {

    /** an ordinary getter */
    public String getUsername() {
        return &quot;world&quot;;
    }
}
&lt;/source&gt;

; web.xml
: The [[Java servlet|servlet]] application [[Deployment Descriptor]], which installs Tapestry as a servlet filter.
&lt;source lang=&quot;xml&quot;&gt;
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;!DOCTYPE web-app
        PUBLIC &quot;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN&quot;
        &quot;http://java.sun.com/dtd/web-app_2_3.dtd&quot;&gt;
&lt;web-app&gt;
    &lt;display-name&gt;Tapestry Example&lt;/display-name&gt;
    &lt;context-param&gt;
        &lt;!-- tell Tapestry 5 where to look for pages, components and mixins --&gt;
        &lt;param-name&gt;tapestry.app-package&lt;/param-name&gt;
        &lt;param-value&gt;org.example.demo&lt;/param-value&gt;
    &lt;/context-param&gt;
    &lt;filter&gt;
        &lt;!-- define the Tapestry servlet filter --&gt;
        &lt;filter-name&gt;app&lt;/filter-name&gt;
        &lt;filter-class&gt;org.apache.tapestry5.TapestryFilter&lt;/filter-class&gt;
    &lt;/filter&gt;
    &lt;filter-mapping&gt;
        &lt;!-- tell the servlet container which requests to send to the Tapestry servlet filter --&gt;
        &lt;filter-name&gt;app&lt;/filter-name&gt;
        &lt;url-pattern&gt;/*&lt;/url-pattern&gt;
    &lt;/filter-mapping&gt;
&lt;/web-app&gt;
&lt;/source&gt;

== Class Transformation ==

Tapestry uses bytecode manipulation to transform page and component classes at runtime. This approach allows the page and component classes to be written as simple [[Plain Old Java Object|POJOs]], with a few naming conventions and annotations potentially triggering substantial additional behavior at class load time. Tapestry versions 5.0, 5.1 and 5.2 used the [[Javassist]] bytecode manipulation library. Subsequent versions replaced Javassist with a new bytecode manipulation layer called ''Plastic'' which is based on [[ObjectWeb ASM]].&lt;ref&gt;http://tawus.wordpress.com/2011/04/18/meeting-plastic/&lt;/ref&gt;&lt;ref&gt;http://mail-archive.ow2.org/asm/2011-04/msg00033.html&lt;/ref&gt;

== Client-side Support ==

Tapestry 5 versions up through 5.3 bundle the [[Prototype JavaScript Framework|Prototype]] and [[script.aculo.us]] JavaScript frameworks, along with a Tapestry-specific library, so as to support Ajax operations as first-class citizens. Third party modules are available to integrate jQuery instead of, or in addition to, Prototype/Scriptaculous.

Starting with version 5.4, Tapestry includes a new JavaScript layer that removes built-in components' reliance on Prototype, allowing jQuery or another JavaScript framework to be plugged in.&lt;ref&gt;http://tapestryjava.blogspot.com/2012/10/zeroing-in-on-tapestry-54.html&lt;/ref&gt;

Version 5.4 also introduces support for JavaScript ''modules'' using the RequireJS module loading system.

== Core Principles ==

The Tapestry project documentation cites four &quot;principles&quot; that govern all development decisions for Tapestry, starting with version 5 in 2008:&lt;ref&gt;{{cite web|url=http://tapestry.apache.org/principles.html |title=Principles |accessdate=2012-10-12 |date=2010-12-21 | archiveurl= http://web.archive.org/web/20121012021755/http://tapestry.apache.org/principles.html| archivedate= 12 October 2012 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

* Static Structure, Dynamic Behavior—page and component structure is essentially static, eliminating the need to construct (and store in session memory) large page and component trees.
* Adaptive API—the framework is designed to adapt to the code, rather than having the code adapt to the framework
* Differentiate Public vs. Internal APIs—all APIs are explicitly &quot;internal&quot; (private) except those that are necessarily public.
* Ensure Backwards Compatibility—The Tapestry developers are reportedly committed to ensuring that upgrading to the latest version of Tapestry is always easy.

== Criticism ==

Tapestry has been criticized as not being backward-compatible across major versions, especially noted in the transition from version 4 to version 5, where no clean migration path was available for existing applications.&lt;ref&gt;{{cite web|url=http://tapestry.1045711.n5.nabble.com/Tapestry5-future-compatiblity-td2431085.html |title=Tapestry5 future compatiblity?  |accessdate=2013-01-21 |date=2009-04-30}}&lt;/ref&gt; Project team members have acknowledged this as a major problem for Tapestry's users in the past, and backward compatibility was made a major design goal for Tapestry going forward. Indeed, from early on in the development of version 5, backward compatibility was listed as one of Tapestry's four new &quot;Core Principles&quot;, and two of the other three were intended to make the evolution of the framework possible without sacrificing backward compatibility. Project team members claim that all Tapestry releases since 5.0 have been highly backward compatible.

Early criticisms of Tapestry 5 also mentioned documentation as a shortcoming. Project members now claim that this deficiency has been largely addressed with a thoroughly revised and updated User's Guide and other documentation.

Since version 5.0, Tapestry has bundled the Prototype and Scriptaculous JavaScript libraries. According to Howard Lewis Ship, in the 2008-2009 timeframe these were reasonable choices. Since then, however, Prototype's popularity has declined, and jQuery's has risen dramatically. In response, the Tapestry community developed modules that allowed jQuery to be used in addition to, or instead of, Prototype. Meanwhile, the next version of Tapestry, 5.4, is expected to remove the dependency on Prototype entirely, replacing it with a compatibility layer into which either jQuery or Prototype (or potentially any other JavaScript framework) can be plugged.

== Relation to other frameworks ==

According to Howard Lewis Ship, Tapestry was initially conceived as an attempt to implement in Java some of the general concepts and approaches found in WebObjects, which was at that time written in [[Objective-C]] and closed-source.&lt;ref&gt;http://devrates.com/post/show/345948/howard-lewis-ship-of-tapestry-interview-%5Bpart-1%5D&lt;/ref&gt;

[[Apache Wicket]] was developed as a response to the complexity of early versions of Tapestry, according to Wicket originator Jonathan Locke.&lt;ref&gt;http://web.archive.org/web/20040909074534/http://www.theserverside.com/news/thread.tss?thread_id=28162&lt;/ref&gt;

[[Facelets]], the default view technology in [[JavaServer Faces]], was reportedly inspired by early versions of Tapestry, as an attempt to fill the need for &quot;a framework like Tapestry, backed by JavaServer Faces as the industry standard&quot;.&lt;ref&gt;http://web.archive.org/web/20070706220453/https://facelets.dev.java.net/&lt;/ref&gt;&lt;ref&gt;http://web.archive.org/web/20130113100928/http://www.jsfcentral.com/articles/facelets_1.html&lt;/ref&gt;

== History ==

{| class=&quot;wikitable sortable&quot;
|-
! Version !! Date !! Description
|-
| {{Version  |o  |1.0}}
| 2000 || Developed by Howard Lewis Ship for internal use
|-
| {{Version  |o  |2.0}}
| 2002-04 || First made available on [[SourceForge]] under the [[GNU Lesser General Public License]].&lt;ref&gt;{{cite web|url=http://www.theserverside.com/discussions/thread.tss?thread_id=12750|title=Tapestry: Java Web Components Release 2.0 is Out|accessdate=2013-01-20 }}&lt;/ref&gt;
|-
| {{Version  |o  |3.0}}
| 2004-04 || The first release under Apache, as a Jakarta sub-project.&lt;ref&gt;{{cite web|url=http://www.theserverside.com/discussions/thread.tss?thread_id=25354|title=Tapestry 3.0 Final Release|accessdate=2013-01-20 }}&lt;/ref&gt;
|-
| {{Version  |o  |4.0}}
| 2006-01 || Introduced support for JDK 1.5 annotations, a new input validation subsystem, and improved error reporting &lt;ref&gt;{{cite web|url=http://www.theserverside.com/news/thread.tss?thread_id=38407|title=Tapestry 4.0 Released|accessdate=2013-01-20 }}&lt;/ref&gt;
|-
| {{Version  |co |5.0}}
| 2008-12 || A nearly complete rewrite from Tapestry 4, introducing a new POJO-based component model emphasizing configuration over convention, and replaced Hivemind with a new no-XML Inversion of Control layer.
|-
| {{Version  |co |5.1}}
| 2009-04 || Performance and memory improvements, automatic GZIP compression, JavaScript aggregation, but remained backwards compatible to Tapestry 5.0. 
|-
| {{Version  |co |5.2}}
| 2010-12 || Added [[Bean Validation|JSR 303 Bean Validation]].&lt;ref&gt;{{cite web|url=http://blog.tapestry5.de/index.php/2010/01/04/tapestry-and-jsr-303-bean-validation-api/ |title=Tapestry and JSR-303 Bean Validation API |accessdate=2010-03-13 |date=2010-01-04 | archiveurl= http://web.archive.org/web/20100416154003/http://blog.tapestry5.de/index.php/2010/01/04/tapestry-and-jsr-303-bean-validation-api/| archivedate= 16 April 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; Extended live class reloading to service implementations. Removed page pooling.&lt;ref&gt;{{cite web|url=http://tapestry.apache.org/2010/12/17/announcing-tapestry-52.html |title=Announcing Tapestry 5.2 |accessdate=2012-11-14 |date=2010-12-17 | archiveurl= http://web.archive.org/web/20121114182121/http://tapestry.apache.org/2010/12/17/announcing-tapestry-52.html/| archivedate= 12 November 2012 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;
|-
| {{Version  |co |5.3}}
| 2011-11 || Added support for HTML5 doctype, JSR-330 annotations for injection,&lt;ref&gt;http://tapestry.apache.org/using-jsr-330-standard-annotations.html&lt;/ref&gt; performance and memory improvements, new components, switched from JavaAssist to ASM bytecode manipulation
|-
| {{Version  |co |5.3.1 - 5.3.6}}
| 2012-2013 || Bug fixes and minor enhancements
|-
| {{Version  |c |5.3.7}}
| 2013-04-24 || Current stable version
|-
| {{Version  |cp |5.4 Beta}}
| 2014 || Major client-side enhancements. New JavaScript layer for switchable jQuery/Prototype support, uses Require.js for its JavaScript module system, [[Twitter Bootstrap]] for its default styling.&lt;ref&gt;{{cite web|url=http://tapestry.apache.org/javascript-rewrite.html|title=JavaScript Rewrite|accessdate=2013-01-20 | archiveurl= http://web.archive.org/web/20121114180615/http://tapestry.apache.org/javascript-rewrite.html| archivedate=2012-11-14}}&lt;/ref&gt;
|}

== Related projects ==
*[http://tynamo.org/ Tynamo Framework aka Trails 2] is based on Tapestry 5.
*[http://jumpstart.doublenegative.com.au/home.html JumpStart tutorial] Tapestry by examples.

==See also==
* [[Java Platform, Enterprise Edition|Java EE]]
* [[Comparison of web application frameworks]]
* [[Facelets]]
* [[Apache Wicket]]
* [[Java view technologies and frameworks]]

== References ==
{{refbegin}}
* {{citation
| first1      = Igor 
| last1       = Drobiazko
| year        = 2012
| title       = Tapestry 5: Rapid web application development in Java
| publisher   = Igor Drobiazko
| pages       = 482
| url         = http://www.tapestry5book.com/
| postscript      = &lt;!--none--&gt;
}}
* {{citation
| first1      = Alexander 
| last1       = Kolesnikov
| date        = January 15, 2008
| title       = Tapestry 5: Building Web Applications: A step-by-step guide to Java Web development with the developer-friendly Apache Tapestry framework
| publisher   = [[Packt|Packt Publishing]]
| pages       = 280
| isbn        = 1-84719-307-2
| url         = http://www.packtpub.com/tapestry-5/book
| postscript      = &lt;!--none--&gt;
}}
* {{citation
| first      = Ka
| last       = Iok Tong
| date       = January 1, 2007
| title      = Enjoying Web Development with Tapestry
| publisher  = 
| pages      = 497 
| edition    = 3rd 
| asin       = B00262M3HS
| url        =
| postscript      = &lt;!--none--&gt; 
}}
* {{citation
| first      = Howard M. Lewis
| last       = Ship
| year       = 2004
| title      = Tapestry in Action
| publisher  = [[Manning Publications|Manning]]
| pages      = 580
| isbn       = 1932394117
| url        =
| postscript      = &lt;!--none--&gt; 
}}
{{refend}}

=== Notes ===
{{reflist}}

== External links ==
* [http://tapestry.apache.org/ Tapestry Home Page]
* [http://howardlewisship.com/ Howard Lewis Ship]
* [http://tynamo.org/ Tynamo Project]
* [http://jumpstart.doublenegative.com.au/jumpstart/ Getting Started Examples]
{{Application frameworks}}
{{apache}}

[[Category:Web application frameworks]]
[[Category:Java enterprise platform]]
[[Category:Apache Software Foundation|Tapestry]]</text>
      <sha1>kfv9zh11kdqndmiz7bqzakr5y5vr05y</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Wicket</title>
    <ns>0</ns>
    <id>1934392</id>
    <revision>
      <id>601480805</id>
      <parentid>597306035</parentid>
      <timestamp>2014-03-27T09:03:45Z</timestamp>
      <contributor>
        <ip>82.209.214.162</ip>
      </contributor>
      <text xml:space="preserve" bytes="14735">{{Infobox software
| name                   = Apache Wicket
| logo                   = [[File:Apache Wicket logo.png]]
| screenshot             =
| caption                =
| founder                = [[Jonathan Locke]]
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 6.14.0
| latest release date    = {{release date|2014|02|20}}&lt;ref&gt;[http://wicket.apache.org/2014/02/20/wicket-6.14.0-released.html Apache Wicket - Apache Wicket 6.14.0 released]. Wicket.apache.org. Retrieved on 2014-03-27.&lt;/ref&gt;
| operating system       = [[Cross-platform]] ([[Java Virtual Machine]])
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Web application framework]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://wicket.apache.org/}}
}}
'''Apache Wicket''', commonly referred to as '''Wicket''', is a lightweight component-based [[web application framework]] for the [[Java (programming language)|Java programming language]] conceptually similar to [[JavaServer Faces]] and [[Apache Tapestry|Tapestry]].  It was originally written by Jonathan Locke in April 2004.  Version 1.0 was released in June 2005. It graduated into an [[Apache Software Foundation|Apache]] top-level project in June 2007.&lt;ref&gt;{{cite web|url = http://martijndashorst.com/blog/2007/06/20/3-2-1|title = Wicket graduates from Apache Incubation|date = 2007-07-20|accessdate = 2008-03-07|first = Martijn|last = Dashorst}}&lt;/ref&gt;

== Rationale ==
Traditional [[model-view-controller]] (MVC) frameworks work in terms of whole [[HTTP request|requests]] and whole pages.  In each request cycle, the incoming request is mapped to a method on a ''controller'' object, which then generates the outgoing response in its entirety, usually by pulling data out of a ''model'' to populate a ''view'' written in specialised [[web template|template markup]].  This keeps the application's [[control flow|flow-of-control]] simple and clear, but can make [[code reuse]] in the controller difficult.

In contrast, Wicket is closely patterned after [[stateful]] [[GUI]] frameworks such as [[Swing (Java)|Swing]].  Wicket applications are trees of ''components'', which use listener [[delegation (programming)|delegates]] to react to [[HTTP]] requests against links and forms in the same way that Swing components react to mouse and keystroke events. Wicket is categorized as a component-based framework.

== Design ==
Wicket uses plain [[XHTML]] for templating (which enforces a clear separation of presentation and [[business logic]] and allows templates to be edited with conventional [[WYSIWYG]] design tools&lt;ref&gt;{{cite web|first = Daniel|last = Carleton|date = 2007-10-12|accessdate = 2008-03-07|url = http://www.devx.com/Java/Article/35620|title = Java Web Development the Wicket Way|publisher = DevX| archiveurl= http://web.archive.org/web/20080310152030/http://www.devx.com/Java/Article/35620| archivedate= 10 March 2008 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;).  Each component is bound to a named element in the XHTML and becomes responsible for rendering that element in the final output. The ''page'' is simply the top-level containing component and is paired with exactly one XHTML template.  Using a special tag, a group of individual components may be abstracted into a single component called a ''panel'', which can then be reused whole in that page, other pages, or even other panels.

Each component is backed by its own model, which represents the state of the component. The framework does not have knowledge of how components interact with their models, which are treated as [[black box|opaque]] objects automatically [[serialization|serialized]] and [[object persistence|persisted]] between requests. More complex models, however, may be made ''detachable'' and provide [[software hook|hooks]] to arrange their own storage and restoration at the beginning and end of each request cycle.  Wicket does not mandate any particular object-persistence or [[object-relational mapping|ORM]] layer, so applications often use some combination of [[Hibernate (Java)|Hibernate]] objects, [[EJB]]s or [[POJO]]s as models.

In Wicket, all server side state is automatically managed. You should never directly use an HttpSession object or similar wrapper to store state. Instead, state is associated with components. Each server-side page component holds a nested hierarchy of stateful components, where each component’s model is, in the end, a POJO (Plain Old Java Object)

Wicket is all about simplicity. There are no configuration files to learn in Wicket. Wicket is a simple class library with a consistent approach to component structure.

== Example ==
A [[Hello World]] Wicket application, with four files:
; HelloWorld.html
: The XHTML template.
 &lt;source lang=&quot;xml&quot;&gt;
&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD XHTML 1.0 Transitional//EN&quot; 
      &quot;http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd&quot;&gt;
&lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot; 
      xmlns:wicket=&quot;http://wicket.apache.org/dtds.data/wicket-xhtml1.3-strict.dtd&quot;
      xml:lang=&quot;en&quot; lang=&quot;en&quot;&gt; 

&lt;body&gt;
    &lt;span wicket:id=&quot;message&quot; id=&quot;message&quot;&gt;Message goes here&lt;/span&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;/source&gt;

; HelloWorld.java
: The page component that will be bound to the template.  It, in turn, binds a child component (the Label component named &quot;message&quot;).
&lt;source lang=&quot;java&quot;&gt;
package org.wikipedia.wicket;

import org.apache.wicket.markup.html.WebPage;
import org.apache.wicket.markup.html.basic.Label;

public class HelloWorld extends WebPage {
    /**
     * Constructor
     */
    public HelloWorld() {
        add(new Label(&quot;message&quot;, &quot;Hello World!&quot;));
    }
}
&lt;/source&gt;

; HelloWorldApplication.java
: The main application class, which routes requests for the homepage to the HelloWorld page component.
&lt;source lang=&quot;java&quot;&gt;
package org.wikipedia.wicket;

import org.apache.wicket.protocol.http.WebApplication;

public class HelloWorldApplication extends WebApplication {
    /**
     * Constructor.
     */
    public HelloWorldApplication() {
    }

    /**
     * @see org.apache.wicket.Application#getHomePage()
     */
    public Class getHomePage() {
        return HelloWorld.class;
    }
}
&lt;/source&gt;

; web.xml
: The [[Java servlet|servlet]] application [[Deployment Descriptor]], which installs Wicket as the default handler for the servlet and arranges for HelloWorldApplication to be instantiated at startup.
&lt;source lang=&quot;xml&quot;&gt;
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;web-app xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; 
         xmlns=&quot;http://java.sun.com/xml/ns/javaee&quot; 
         xmlns:web=&quot;http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd&quot; 
         xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/javaee 
                             http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd&quot; 
         id=&quot;WebApp_ID&quot; version=&quot;2.5&quot;&gt;
    &lt;display-name&gt;Wicket Example&lt;/display-name&gt;
    &lt;filter&gt;
        &lt;filter-name&gt;HelloWorldApplication&lt;/filter-name&gt;
        &lt;filter-class&gt;org.apache.wicket.protocol.http.WicketFilter&lt;/filter-class&gt;
        &lt;init-param&gt;
            &lt;param-name&gt;applicationClassName&lt;/param-name&gt;
            &lt;param-value&gt;org.wikipedia.wicket.HelloWorldApplication&lt;/param-value&gt;
        &lt;/init-param&gt;
    &lt;/filter&gt;
    &lt;filter-mapping&gt;
        &lt;filter-name&gt;HelloWorldApplication&lt;/filter-name&gt;
        &lt;url-pattern&gt;/*&lt;/url-pattern&gt;
    &lt;/filter-mapping&gt;
&lt;/web-app&gt;
&lt;/source&gt;

== Components ==
*Basic components like form, links, repeaters{{clarify|date=May 2012}}, and so on are built-in
http://wicketstuff.org/wicket14/compref/ {{Dead link|date=May 2013}}
*More are on http://wicketstuff.org/confluence/display/STUFFWIKI/Wiki {{Dead link|date=February 2013}}

== Releases ==
{| class=&quot;wikitable sortable&quot;
|-
! Release
! style=&quot;width:60px;&quot;| Date
! Notes
|-
|1.3.7
|2009-07-30
|&lt;ref&gt;[http://wicket.apache.org/2009/07/30/wicket-1.3.7-released.html Apache Wicket - Apache Wicket 1.3.7 marks end of life for Wicket 1.3]. Wicket.apache.org. Retrieved on 2013-08-13.&lt;/ref&gt;
|-
|1.4
|2009-07-30
|&quot; ... a departure from the past where we leave Java 1.4 behind and we require Java 5 as the minimum JDK version. By moving to Java 5 as the required minimum platform, we were able to utilize Java 5 idioms and increase the type safety of our APIs.&quot; &lt;ref&gt;[http://wicket.apache.org/2009/07/30/wicket-1.4-takes-typesafety-to-the-next-level.html Apache Wicket - Apache Wicket 1.4 takes typesafety to the next level]. Wicket.apache.org. Retrieved on 2013-08-13.&lt;/ref&gt;
|-
|1.4.1
|2009-08-21
|&lt;ref&gt;[http://wicket.apache.org/2009/08/21/wicket-1.4.1-released.html Apache Wicket - Wicket 1.4.1 released]. Wicket.apache.org. Retrieved on 2013-08-13.&lt;/ref&gt;
|-
|1.4.9
|2010-05-24
|&quot;... over fifteen bug fixes and improvements&quot; &lt;ref&gt;[http://wicket.apache.org/2010/05/24/wicket-1.4.9-released.html Apache Wicket - Wicket 1.4.9 released]. Wicket.apache.org. Retrieved on 2013-08-13.&lt;/ref&gt;
|-
|1.4.10
|2010-08-11
|&quot;...  over thirty bug fixes and improvements.&quot; &lt;ref&gt;[http://wicket.apache.org/2010/08/11/wicket-1.4.10-released.html Apache Wicket - Wicket 1.4.10 released]. Wicket.apache.org. Retrieved on 2013-08-13.&lt;/ref&gt;
|-
|1.4.16
|2011-02-25
|&quot;This is primarily a minor bugfix release on the 1.4.x (stable) branch.&quot; &lt;ref&gt;[http://wicket.apache.org/2011/02/25/wicket-1.4.16-released.html Apache Wicket - Wicket 1.4.16 released]. Wicket.apache.org. Retrieved on 2013-08-13.&lt;/ref&gt;
|-
|1.4.17
|2011-04-02
|&quot;This is primarily a minor bugfix release on the 1.4.x (stable) branch.&quot; &lt;ref&gt;[http://wicket.apache.org/2011/04/02/wicket-1.4.17-released.html Apache Wicket - Wicket 1.4.17 released]. Wicket.apache.org. Retrieved on 2013-08-13.&lt;/ref&gt;
|-
|1.4.18
|2011-08-09
|&quot;This is primarily a minor bugfix release on the 1.4.x (stable) branch.&quot; &lt;ref&gt;[http://wicket.apache.org/2011/08/09/wicket-1.4.18-released.html Apache Wicket - Wicket 1.4.18 released]. Wicket.apache.org. Retrieved on 2013-08-13.&lt;/ref&gt;
|-
|1.4.19
|2011-10-19
|&quot;This is primarily a minor bugfix release on the 1.4.x (stable) branch.&quot; &lt;ref&gt;[http://wicket.apache.org/2011/10/17/wicket-1.4.19-released.html Apache Wicket - Wicket 1.4.19 released]. Wicket.apache.org. Retrieved on 2013-08-13.&lt;/ref&gt;
|-
|1.5.0
|2011-09-07
|&quot;Apache Wicket 1.5 has been in development for the last two years and brings many improvements over previous versions.&quot; &lt;ref&gt;[http://wicket.apache.org/2011/09/07/wicket-1.5-released.html Apache Wicket - Apache Wicket releases Wicket 1.5]. Wicket.apache.org. Retrieved on 2013-08-13.&lt;/ref&gt;
|-
|1.5.1
|2011-09-29
|&quot;... over 40 bug fixes and 15 improvements.&quot;&lt;ref&gt;[http://wicket.apache.org/2011/09/29/wicket-1.5.1-released.html Apache Wicket - Wicket 1.5.1 released]. Wicket.apache.org. Retrieved on 2013-08-13.&lt;/ref&gt;
|-
|1.5.2 
|2011-10-24 
|&quot;... over 25 bug fixes and 5 improvements.&quot;&lt;ref&gt;[http://wicket.apache.org/2011/10/24/wicket-1.5.2-released.html Apache Wicket - Wicket 1.5.2 released]. Wicket.apache.org. Retrieved on 2013-08-13.&lt;/ref&gt;
|-
|1.5.3 
|2011-11-14 
|&quot;... over 40 bug fixes and improvements.&quot;&lt;ref&gt;[http://wicket.apache.org/2011/11/14/wicket-1.5.3-release.html ]{{dead link|date=August 2013}}&lt;/ref&gt;
|-
|1.6
|2012-09-05
| Out-of-the box jQuery integration, complete control over AJAX requests, improved event registration in browsers, support for large datasets, dependency management for client side JavaScript libraries, experimental support for websockets
|-
|1.6.3
|2013-01-02
| jQuery 1.8.2; fixed JavaScript errors in IE7 and IE8.
|-
|1.6.4
|2013-01-14
|jQuery 1.8.3, bootstrap 2.2.2, JSR 303 BeanValidation support, Hierarchical feedback panel
|}

==See also==
{{Portal|Free software}}
*[[Vaadin]]
*[[Apache Tapestry|Tapestry]]
*[[Apache Click|Click]]
*[[ZK (framework)|ZK]]
*[[Richfaces]]
*[[Echo (framework)|Echo]]

==References==
{{refbegin}}
* {{cite book
| first1      = Igor 
| last1       = Vaynberg
| date        = May 15, 2011
| title       = Apache Wicket Cookbook 
| publisher   = [[Packt Publishing]]
| edition     = 1st
| page       = 312 
| isbn        = 1-84951-160-8
| url         = https://www.packtpub.com/apache-wicket-cookbook/book
}}
* {{cite book
| first1      = Martijn
| last1       = Dashorst
| first2      = Eelco
| last2       = Hillenius
| date        = September 15, 2008
| title       = Wicket in Action
| publisher   = [[Manning]]
| edition     = 1st
| page       = 392
| isbn        = 1-932394-98-2
| url         = 
}}
* {{cite book
| first      = Karthik
| last       = Gurumurthy
| date       = September 7, 2006
| title      = Pro Wicket
| publisher  = [[Apress]]
| edition    = 1st
| page      = 328
| isbn       = 1-59059-722-2
| url        = http://www.apress.com/book/view/9781590597224
}}
{{refend}}

===Notes===
{{reflist}}

==External links==
===Introductory articles===
*[http://www.ibm.com/developerworks/web/library/wa-aj-wicket/?S_TACT=105AGY82&amp;S_CMP=GENSITE Wicket: A simplified framework for building and testing dynamic Web pages]
*[http://ensode.net/wicket_first_look.html A First Look at the Wicket Framework]
*[http://www.theserverside.com/news/thread.tss?thread_id=34725 The Server Side discussion on Wicket 1.0]
*[http://weblogs.java.net/blog/timboudreau/archive/2005/04/wicket_help_tes_1.html Tim Boudreau's Blog]
*[http://blogs.sun.com/geertjan/entry/wicket_3_3_support_for Kickstart Wicket in NetBeans IDE 6.1]
*[http://www.theserverside.com/news/thread.tss?thread_id=28162 The Server Side discussion]
*[http://www.javalobby.org/java/forums/t105230.html Javalobby interview with Martijn Dashorst (project chairman)]
*[http://www.viddler.com/explore/oredev/videos/61/ Wicket introduction presentation by Martijn Dashorst]
*[http://video.fosdem.org/2011/maintracks/apache-wicket.xvid.avi Wicket at FOSDEM 2011]

===Blogs===
*[http://chillenious.wordpress.com Eelco Hillenius]
*[http://martijndashorst.com Martijn Dashorst]
*[http://web.mac.com/jonathan.locke/iWeb/JonathanLocke/Blog/Blog.html Jonathan Locke]
*[http://www.herebebeasties.com Al Maw]
*[http://blog.brunoborges.com.br Bruno Borges]
*[http://wicketinaction.com/ Wicket in Action]
*[http://mysticcoders.com/ Mystic Coders]

==Documentation==
*[http://wicketstuff.org/ Reusable components and patterns for Wicket]
*[http://wicketstuff.org/wicket14/ Site that has live demos and a repository of components]
*[http://cwiki.apache.org/WICKET Wiki with how-tos, a manual and more]
*[http://wicketbyexample.com Plethora of examples of using Wicket in the real world]
*[http://code.google.com/p/wicket-guide/ A free and comprehensive user guide to Wicket]

{{apache}}
{{Application frameworks}}

[[Category:Java enterprise platform]]
[[Category:Free software programmed in Java]]
[[Category:Web application frameworks]]
[[Category:Apache Software Foundation|Wicket]]</text>
      <sha1>cuspwubxgwthuyy2p98vulcamcj715y</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache XML</title>
    <ns>0</ns>
    <id>1004521</id>
    <revision>
      <id>541095499</id>
      <parentid>538308294</parentid>
      <timestamp>2013-02-28T00:57:49Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 5 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q2662299]] ([[User talk:Addbot|Report Errors]])</comment>
      <text xml:space="preserve" bytes="1891">{{Unreferenced|date=December 2009}}
The '''Apache XML''' project is part of the [[Apache Software Foundation]] and focuses on [[XML]]-related projects. 

It consists of several sub projects:

==Active sub projects==
*'''[[Xerces]]''': An XML parser for [[Java (programming language)|Java]], [[C++]] and [[Perl]]
*'''[[Xalan]]''': An [[XSLT]] stylesheet processor for Java and C++ which implements the [[XPath]] query language.
*'''[[Apache Forrest|Forrest]]''': A standards-based documentation framework
*'''XML-Security''': A project providing security functionality for XML data 
*'''Xindice''': A native [[XML database]] 
*'''XML Commons''': A project focusing on common code and guidelines for XML projects 
*'''[[XMLBeans]]''': An XML-Java binding tool

==Projects related to webservices==
*'''SOAP''': Is an old implementation of the [[SOAP]]. This project based on [[IBM]]'s SOAP4J implementation. It should no longer be used for new projects. Instead you should favour the Axis implementation.
*'''XML-RPC''': Apache XML-RPC is a Java implementation of [[XML-RPC]], a protocol that uses XML over [[HyperText Transfer Protocol|HTTP]] to implement remote procedure calls.
*'''[[Axis (computer program)|Axis]]''': Apache Axis is the current implementation of the [[SOAP]] for Java and C++. It is the successor for the SOAP project.
*'''WSIF''': [[Web Services Invocation Framework]] is a simple Java [[Application programming interface|API]] for invoking [[Web services]].

==No longer developed projects==
*'''[[AxKit]]''': An XML-based web publishing framework in [[mod_perl]] 
*'''Crimson''': A Java XML parser derived from the [[Sun Microsystems|Sun]] Project X Parser 
*'''Xang''': Framework for rapid development of dynamic server pages in [[ECMAScript]] ([[JavaScript]])
{{Apache}}

{{DEFAULTSORT:Apache Xml}}
[[Category:XML software]]
[[Category:Apache Software Foundation|XML]]</text>
      <sha1>t9z10bgh5rte83uwp6kg9jpi1t3wsp5</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache OFBiz</title>
    <ns>0</ns>
    <id>7528242</id>
    <revision>
      <id>585388072</id>
      <parentid>581752576</parentid>
      <timestamp>2013-12-10T04:34:30Z</timestamp>
      <contributor>
        <ip>217.66.215.45</ip>
      </contributor>
      <text xml:space="preserve" bytes="8405">{{Refimprove|June 2011|date=June 2011}}

{{Infobox software
| name                   = Apache OFBiz
| logo                   = [[Image:Ofbiz logo.gif|190px]]
| screenshot             = [[Image:Screenshot Apache OFBiz.PNG|250px|Apache OFBiz Screenshot]]
| caption                = Apache OFBiz Screenshot running
| collapsible            = yes
| developer              = [[Apache Software Foundation]]
| latest release version = 12.04.02
| latest release date    = {{release date|2013|07|20}}&lt;ref&gt;https://ofbiz.apache.org/&lt;/ref&gt;
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[business processes|Business Process]]
| license                = [[Apache License]] 2.0
| website                = http://ofbiz.apache.org
}}
'''Apache Open For Business''' (Apache OFBiz) is an [[open source]] [[enterprise resource planning]] (ERP) system. It provides a suite of enterprise applications that integrate and automate many of the [[business process]]es of an enterprise.

OFBiz is an [[Apache Software Foundation]] top level project.

== Overview ==
Apache OFBiz is a framework, provides a common [[data model]] and a rich set of [[business processes|business process]].
All applications are built around a common architecture using common data, logic and process components.
Beyond the framework itself, Apache OFBiz offers functionality including:
* [[Accounting]] (Agreements, Invoicing, Vendor Management, [[General Ledger]])
* Asset Maintenance
* Catalogue and Product Management
* Facility and Warehouse Management
* Manufacturing
* Order Processing
* Inventory Management, automated stock replenishment etc.
* [[Content management system]] (CMS)
* [[Human resources]] (HR)
* People and Group Management
* Project Management
* [[Sales force management system|Sales Force Automation]]
* Work Effort Management
* Electronic [[point of sale]] (ePOS)
* [[Electronic commerce]] (eCommerce)
* [[Scrum (development)]] (Scrum software development support)

== Technology ==

All of Apache OFBiz functionality is built on a common framework. The functionality can be divided into the following distinct layers:

=== Presentation Layer ===
Apache OFBiz uses the concept of &quot;screens&quot; to represent the Apache OFBiz pages. Each page is, normally, represented as a screen. A page in Apache OFBiz consists of components. A component can be a header, footer, etc. When the page is rendered all the components are combined together as specified in the screen definition. Components might be Java Server Pages ([JSP]s) &lt;deprecated&gt;, FTL pages built around [[FreeMarker]] [[Template engine (web)|template engine]], Forms and Menus Widgets. Widgets are an OFBiz specific technology.

=== Business Layer ===
The business, or application layer defines services provided to the user. The services can be of several types: Java methods, SOAP, simple services, workflow, etc. A service engine is responsible for invocation, transactions and security.
Apache OFBiz uses a set of well established, open source technologies and standards such as  [[Java platform|Java]], [[Java Platform, Enterprise Edition|Java EE]], [[XML]] and [[SOAP]]. Although Apache OFBiz is built around the concepts used by Java EE, many of its concepts are implemented in different ways; either because Apache OFBiz was designed prior to many recent improvements in Java EE or because Apache OFBiz authors didn’t agree with those implementations.

=== Data Layer ===
The data layer is responsible for database access, storage and providing a common data
interface to the Business layer.
Data is accessed not in [[Object Oriented]] fashion but in a [[Relational database|relational]] way.
Each [[entity]] (represented as a row in the database) is provided to the business layer as a set of generic values.
A generic value is not typed, so fields of an entity are accessed by the [[Column (database)|column]] name.

==Open Source Libraries==
Here a list of the existing open source libraries that are used in Apache OFBiz (deprecated see the ref link)
&lt;ref&gt;[https://cwiki.apache.org/confluence/display/OFBADMIN/Libraries+Included+in+OFBiz List of open source libraries used in OFBiz]&lt;/ref&gt;
{| class=&quot;wikitable&quot; style=&quot;width:100%;&quot;
|-
! Component
! Description
! License
|-
| [[Apache Commons]]
| A collection of reusable Java components like (FileUpload, IO, Digester, Logging, Codec, and Validator)
| [[Apache License|Apache]]
|-
| [[ANTLR]]
| Framework for constructing recognizers, interpreters, compilers, and translators from grammatical descriptions
| [[BSD license|BSD]]
|-
| [[ObjectWeb ASM|ASM]]
| Set of Java classes for decomposing, modifying, and recomposing Java [[bytecode]]
| [[BSD license|BSD]]
|-
| [[Bean Scripting Framework|BSF]]
| Set of Java classes which provides scripting language support within Java applications
| [[Apache License|Apache]]
|-
| [[BeanShell]]
| Lightweight [[Scripting language]] for Java [http://www.jcp.org/en/jsr/detail?id=274 JSR 274]. Beanshell is being phased out of OFBiz and eventually will be replaced by Groovy
| [[Sun Public License|SPL]]/[[LGPL]]
|-
| [[Groovy (programming language)|Groovy]]
| An agile dynamic language for the Java Platform
| [[Apache License|Apache]]
|-
| [[Jython]]
| An implementation of the high-level, dynamic, object-oriented language [[Python (programming language)|Python]] written in 100% Pure Java
| [http://www.jython.org/license.html Jython License]
|-
| [[Apache Ant]]
| Software tool for automating software build processes.
| [[Apache License|Apache]]
|-
| [[Apache Avalon]]
| Software framework provide a reusable component framework for container (server) applications
| [[Apache License|Apache]]
|-
| [[FreeMarker]]
| [[Template processor|Template engine]] focusing on generation of text output (anything from HTML to autogenerated source code)
| [[BSD license|BSD]]
|-
| [[Apache Geronimo]]
| A certified Java EE based [[application server]]
| [[Apache License|Apache]]
|-
| [[HttpUnit]]
| [[Software testing]] framework used to perform testing of web sites without the need for a web browser.
| [[BSD license|BSD]]
|-
| [[International Components for Unicode|ICU]]
| Mature C/C++ and Java libraries for [[Unicode]] support, software [[internationalization]] and software [[globalization]].
| [[MIT License|MIT]]
|-
| [[JavaCC]]
| [[Parser generator]] for the Java programming language similar to [[Yacc]]
| [[BSD license|BSD]]
|-
| [[Javolution]]
| Real-time library aiming to make Java applications faster and more time predictable
| [[BSD license|BSD]]
|-
| [[Apache Xerces]]
| An [[XML parser]] library for Java, [[C++]] and [[Perl]]
| [[Apache License|Apache]]
|-
| [[Apache Derby]]
| Full-fledged relational database management system (RDBMS) with native Java Database Connectivity (JDBC) support
| [[Apache License|Apache]]
|-
| [[MX4J]]
| [[Java Management Extensions]] (JMX) tools for managing and monitoring applications, system objects, devices and service oriented networks.
| [[Apache License|Apache]]
|-
| [[Apache Tomcat]]
| Web application server supporting Java Servlet 2.5 and JavaServer Pages ([[JavaServer Pages|JSP]]) 2.1
| [[Apache License|Apache]]
|-
| [[Jetty (web server)|Jetty]]
| Web application server supporting Java Servlet 2.5 and JavaServer Pages 2.1—an alternative to the Tomcat server
| [[Apache License|Apache]]
|-
| [[DataVision]]
| Reporting tool similar to [[Crystal Reports]] and [[BIRT Project]]
| [[Apache License|Apache]] 1.1
|}

==See also==
* [[Comparison of free and open source eCommerce web application framework]]

== References ==
{{reflist}}

== External links ==
*[http://ofbiz.apache.org/ Official Apache OFBiz website]
*[http://www.ofbiz.info/ Additional indexed OFBiz information]
*[http://www.ofbiz.biz/ Apache OFBiz Community (ger)]
*[http://www.cs.ait.ac.th/~mdailey/papers/Nattanicha-ERPCompare.pdf ERP Application Development Frameworks: Case Study and Evaluation] by Nattanicha Rittammanart, Wisut Wongyuedy, and Matthew N. Dailey
{{apache}}

{{DEFAULTSORT:Apache Ofbiz}}
[[Category:Apache Software Foundation|OFBiz]]
[[Category:Free accounting software]]
[[Category:Free e-commerce software]]
[[Category:Free industrial software]] &lt;!-- because of MRP --&gt;
[[Category:Free ERP software]]
[[Category:Free software programmed in Java]]
[[Category:Web applications]]</text>
      <sha1>7fly1jferp69j6hyax9h16yuyfwgf2v</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache ServiceMix</title>
    <ns>0</ns>
    <id>7528444</id>
    <revision>
      <id>598957393</id>
      <parentid>598957246</parentid>
      <timestamp>2014-03-10T08:46:29Z</timestamp>
      <contributor>
        <username>Kbrose</username>
        <id>3938795</id>
      </contributor>
      <comment>/* See also */ trim</comment>
      <text xml:space="preserve" bytes="5002">{{Blacklisted-links|1=
*http://java.dzone.com/articles/pattern-based-development-with-0
*:''Triggered by &lt;code&gt;\bdzone\.com\b&lt;/code&gt; on the global blacklist''|bot=Cyberbot II}}
{{Infobox software
| name                   = Apache ServiceMix
| logo                   = &lt;div style= &quot;background-color:#4D83B1&quot;&gt;[[file:Servicemix-logo.png|200px]]&lt;/div&gt;
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 4.5.1
| latest release date    = {{Release date|2013|05}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[enterprise service bus]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://servicemix.apache.org/}}
}}
'''Apache ServiceMix''' is an enterprise-class [[open source|open-source]] distributed [[enterprise service bus]] (ESB) based on the [[service-oriented architecture]] (SOA) model. It is a project of the [[Apache Software Foundation]] and was built on the semantics and [[application programming interface]]s of the [[Java Business Integration]] (JBI) specification [[Java Community Process|JSR]] 208. The software is distributed under the [[Apache License]].

ServiceMix 4 fully supports the [[OSGi]] framework. ServiceMix is lightweight and easily embeddable, has integrated [[Spring Framework]] support and can be run at the edge of the network (inside a client or server), as a standalone ESB provider or as a service within another ESB. ServiceMix is compatible with [[Java Platform, Standard Edition|Java SE]] or a [[Java Platform, Enterprise Edition|Java EE]] [[application server]].
ServiceMix uses [[Apache ActiveMQ|ActiveMQ]] to provide remoting, clustering, reliability and distributed failover. The basic frameworks used by ServiceMix are Spring and XBean.&lt;ref&gt;{{cite web|last=Irriger|first=Axel |title=Apache ServiceMix|url=http://www.methodsandtools.com/tools/tools.php?servicemix| accessdate = 17 February 2011}}&lt;/ref&gt;

ServiceMix is often used with [[Apache ActiveMQ]], [[Apache Camel]] and [[Apache CXF]] in SOA infrastructure projects. Enterprise subscriptions for ServiceMix is available from independent vendors.

ServiceMix is an [[Enterprise Service Bus]] that provides: {{Citation needed|date=June 2009}}
* Federation, clustering and container provided failover
* Hot deployment and lifecycle management of business objects
* Vendor independence from vendor-licensed products
* Compliance with the JBI specification JSR 208
* Compliance with the OSGi 4.2 specification through Apache Felix &lt;ref&gt;{http://felix.apache.org/ Announcement} by [[Brian Taylor (Software Architect)]]&lt;/ref&gt; 
* Support for OSGi Enterprise through Apache Aries

It was accepted as an official Apache project by the ASF Board of Directors on September 19, 2007.&lt;ref&gt;[http://gnodet.blogspot.com/2007/09/servicemix-has-graduated.html Announcement] by [[Guillaume Nodet]]&lt;/ref&gt; 

==See also==
* [[Apache ActiveMQ]]
* [[Apache Camel]]
* [[Apache CXF]]
* [[Enterprise messaging system]]
* [[Event-driven SOA]]
* [[Message-oriented middleware]]

==References==
{{reflist}}

==Bibliography==
{{Refbegin}}
* {{citation
| first = Christudas
| last = Binildas A
| title = Service Oriented Java Business Integration
| author-link =
| publication-date =
| date = August 13, 2008
| edition = 1st
| volume =
| series =
| publication-place =
| place =
| publisher = [[Packt|Packt Publishers]]
| pages = 436
| page =
| id =
| isbn = 1-84719-440-0
| doi =
| oclc =
| url = http://www.packtpub.com/service-oriented-java-business-integration
| accessdate =
}}
* {{citation
| first1 = Tijs
| last1 = Rademakers
| first2 = Jos
| last2 = Dirksen
| title = Open-Source ESBs in Action
| author-link =
| publication-date =
| date = October 28, 2008
| edition =
| volume =
| series =
| publication-place =
| place =
| publisher = [[Manning Publications]]
| pages = 528
| page =
| id =
| isbn = 1-933988-21-5
| doi =
| oclc =
| url =
| accessdate =
}}
{{Refend}}

==External links==
* [http://servicemix.apache.org/home.html ServiceMix web site]
** [http://servicemix.apache.org/users-guide.html Apache User Guide]
** [http://servicemix.apache.org/documentation.html Apache documentation]
** [http://servicemix.apache.org/discussion-forums.html Apache Forums]
* [http://parleys.com/display/PARLEYS/Home#slide=2;talk=14123079;title=ServiceMix Javapolis 2007 Online ServiceMix Session]
* [http://camelone.org CamelOne Conference]
* [http://jboss.org JBoss community web site]
* [http://java.dzone.com/articles/pattern-based-development-with-0 Pattern Based Development with ServiceMix]

{{apache}}

{{DEFAULTSORT:Apache Servicemix}}
[[Category:Apache Software Foundation|ServiceMix]]
[[Category:Java platform]]
[[Category:Java enterprise platform]]
[[Category:Service-oriented architecture-related products]]
[[Category:Enterprise application integration]]</text>
      <sha1>qq59lc5vhg2x8glqs81msnbra50mlol</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Axis2</title>
    <ns>0</ns>
    <id>9442659</id>
    <revision>
      <id>596491631</id>
      <parentid>596491287</parentid>
      <timestamp>2014-02-21T14:17:26Z</timestamp>
      <contributor>
        <ip>62.132.238.161</ip>
      </contributor>
      <text xml:space="preserve" bytes="7887">{{Infobox Software 
| name                   = Apache Axis2
| logo                   = &lt;!-- Deleted image removed: [[Image:Apache Axis2 Logo.jpg|150px|Apache Axis2 Logo]] --&gt;
| screenshot             = 
| caption                = 
| developer              =  [[Apache Software Foundation]] 
| latest release version = 1.6.2
| latest release date    = {{release date|2012|04|17}}
| operating system       = [[Cross-platform]] 
| programming language   = [[Java (programming language)|Java]] and [[C (programming language)|C]]
| genre                  = [[Web service]]
| license                = [[Apache License]] 2.0
| website                = http://axis.apache.org/
}}

'''Apache Axis2''' is a core engine for [[Web services]]. It is a complete re-design and re-write of the widely used [[Apache Axis]] [[SOAP]] stack. Implementations of Axis2 are available in [[Java (programming language)|Java]] and [[C (programming language)|C]].

Axis2 provides the capability to add Web services interfaces to Web applications. It can also function as a standalone server application.

At time of writing the last stable release is version 1.6.2. A version 1.7 is been under development for some years, but progress is slow without prospect of a release date. Likely reason for this is the fact that Axis2 is being phased out and replaced by [[Apache CXF]].


==Why Apache Axis2==
A new architecture for Axis2 was introduced{{by whom?|date=February 2013}} during the August 2004 Axis2 Summit in Colombo, Sri Lanka. The new architecture on which Axis2 is based is more flexible, efficient and configurable in comparison to Axis1.x architecture. Some well-established concepts from Axis 1.x, like handlers etc., have been preserved in the new architecture.

Apache Axis2 supports SOAP 1.1 and SOAP 1.2, and it has integrated support for the widely popular [[Representational state transfer|REST]] style of Web services. The same business-logic implementation can offer both a WS-* style interface as well as a [[REST]]/[[Plain Old XML|POX]] style interface simultaneously.

Axis2/Java has support for [[Spring Framework]].

Axis2/C seems to be [http://axis.apache.org/axis2/c/core/download.cgi abandoned in 2009].

Axis2 came with many new features, enhancements and industry specification implementations. Key features offered include:

==Axis2 Features==

Apache Axis2 includes support for following standards:

* [[WS-ReliableMessaging|WS - ReliableMessaging]] - Via [[Apache Sandesha2]]
* [[WS-Coordination|WS - Coordination]] - Via [[Apache Kandula2]]
* [[WS-AtomicTransaction|WS - AtomicTransaction]] - Via Apache Kandula2
* [[WS-SecurityPolicy|WS - SecurityPolicy]] - Via [[Apache Rampart]]
* [[WS-Security|WS - Security]] - Via Apache Rampart
* [[WS-Trust|WS - Trust]] - Via Apache Rampart
* [[WS-SecureConversation|WS - SecureConversation]] - Via Apache Rampart
* [[SAML 1.1]] - Via Apache Rampart
* [[SAML 2.0]] - Via Apache Rampart
* [[WS-Addressing|WS - Addressing]] - Module included as part of Axis2 core

Below a list of features and selling points cited from the Apache axis site:

* '''Speed''' - Axis2 uses its own object model and [[StAX|StAX (Streaming API for XML)]] parsing to achieve significantly greater speed than earlier versions of Apache Axis.
* '''Low memory foot print''' - Axis2 was designed ground-up keeping low memory foot print in mind.
* '''AXIOM''' - Axis2 comes with its own light-weight object model, AXIOM, for message processing which is extensible, optimized for performance, and simplified for developers.
* '''Hot Deployment''' - Axis2 is equipped with the capability of deploying Web services and handlers while the system is up and running. In other words, new services can be added to the system without having to shut down the server. Simply drop the required Web service archive into the services directory in the repository, and the deployment model will automatically deploy the service and make it available for use.
* '''Asynchronous Web services''' - Axis2 now supports asynchronous Web services and asynchronous Web services invocation using non-blocking clients and transports.
* '''MEP Support''' - Axis2 now comes handy with the flexibility to support Message Exchange Patterns (MEPs) with in-built support for basic MEPs defined in [[Web Services Description Language|WSDL]] 2.0.
* '''Flexibility''' - The Axis2 architecture gives the developer complete freedom to insert extensions into the engine for custom header processing, system management, and anything else you can imagine.
* '''Stability''' - Axis2 defines a set of published interfaces which change relatively slowly compared to the rest of Axis.
* '''Component-oriented Deployment''' - You can easily define reusable networks of Handlers to implement common patterns of processing for your applications, or to distribute to partners.
* '''Transport Framework''' - We have a clean and simple abstraction for integrating and using Transports (i.e., senders and listeners for SOAP over various protocols such as SMTP, FTP, message-oriented middleware, etc.), and the core of the engine is completely transport-independent.
* '''WSDL support''' - Axis2 supports the [[Web Services Description Language]], version 1.1 and 2.0, which allows you to easily build stubs to access remote services, and also to automatically export machine-readable descriptions of your deployed services from Axis2.
* '''Add-ons''' - Several Web services specifications have been incorporated including [http://ws.apache.org/wss4j/ WSS4J] for security (Apache Rampart), Sandesha for reliable messaging, Kandula which is an encapsulation of [[WS-Coordination]], [[WS-AtomicTransaction]] and WS-BusinessActivity.
* '''Composition and Extensibility''' - Modules and phases improve support for composability and extensibility. Modules support composability and can also support new WS-* specifications in a simple and clean manner. They are however not hot deployable as they change the overall behavior of the system.

==Axis2 Modules==
Axis2 modules provides [[Quality of service|QoS]] features like security, reliable messaging, etc.

*[[Apache Rampart module]] - Apache Rampart modules adds [[WS-Security|WS - Security]] features to Axis2 engine
*Apache Sandesha module - An implementation of [[WS-ReliableMessaging|WS - Reliable Messaging]] specification

==Related technologies==
* [[Apache Axis]]
* [[Apache CXF]] - other Apache web services framework (old [[Codehaus XFire|XFire]] &amp; [[Celtix]])
* [[Java Web Services Development Pack]] - web services framework
* [[XML Interface for Network Services]] - RPC/web services framework
* [[Web Services Invocation Framework]] - Java API for invoking Web services
* AlchemySOAP - [[C++]] open source SOAP-based web services framework

==Axis2 Books==
* Quickstart Apache Axis2.

==External links==
{{Portal|Java}} 
* [http://axis.apache.org/axis/ Apache Axis Homepage] at the Apache Software Foundation
* [http://axis.apache.org/axis2/java/core/ Apache Axis2/Java] at the Apache Software Foundation
* [http://axis.apache.org/axis2/c/core/ Apache Axis2/C] at the Apache Software Foundation
* [http://ws.apache.org/axis2/modules/index.html Apache Axis2 Module Page]
* [http://www.ibm.com/developerworks/webservices/library/ws-apacheaxis/index.html Web services using Apache Axis2]
* [http://robaustin.wikidot.com/axis How to run an Axis2 client running against a Windows Web Server] - Rob Austin
* {{cite conference | title = Axis2, Middleware for Next Generation Web Services | id = {{citeseerx|10.1.1.62.1740}} | booktitle = Proceedings of the IEEE International Conference on Web Services | conference = ICWS '06 }}
* [http://www.journaldev.com/255/axis2-web-services-tutorial Axis2 Tutorial]
{{Apache}}

[[Category:Apache Software Foundation|Axis]]
[[Category:Web services]]
[[Category:Web service specifications]]
[[Category:Java enterprise platform]]</text>
      <sha1>koukx38txuagi0ul0agcs4mtsadvfu8</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Camel</title>
    <ns>0</ns>
    <id>11335363</id>
    <revision>
      <id>595039060</id>
      <parentid>590096780</parentid>
      <timestamp>2014-02-11T21:33:51Z</timestamp>
      <contributor>
        <ip>66.187.233.206</ip>
      </contributor>
      <text xml:space="preserve" bytes="4153">{{Blacklisted-links|1=
*http://architects.dzone.com/articles/apache-camel-integration
*:''Triggered by &lt;code&gt;\bdzone\.com\b&lt;/code&gt; on the global blacklist''
*http://refcardz.dzone.com/refcardz/enterprise-integration
*:''Triggered by &lt;code&gt;\bdzone\.com\b&lt;/code&gt; on the global blacklist''|bot=Cyberbot II}}
{{Infobox Software
| name                   = Apache Camel
| logo                   = [[File:Apache-camel-logo.png|150px|Apache Camel Logo]]
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version =  2.10.6
| latest release date    = {{release date|2013|07|07}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Enterprise Integration Patterns]] [[Enterprise Service Bus]] [[Service-oriented architecture|SOA]] [[Message Oriented Middleware]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://camel.apache.org}}
}}
'''Apache Camel''' is a rule-based routing and mediation engine which provides a [[Plain Old Java Object|Java object]]-based implementation of the [[Enterprise Integration Patterns]] using an [[Application programming interface|API]] (or declarative Java [[Domain-specific language|Domain Specific Language]]) to configure routing and mediation rules. The domain-specific language means that Apache Camel can support type-safe smart completion of routing rules in an [[integrated development environment]] using regular Java code without large amounts of [[XML]] configuration files, though XML configuration inside [[Spring Framework|Spring]] is also supported.

Camel is often used with [[Apache ServiceMix]], [[Apache ActiveMQ]] and [[Apache CXF]] in [[service-oriented architecture]] infrastructure projects.

==Tooling==
Graphical, Eclipse-based tooling is available from FuseSource and from Talend.

==Books==

{{refbegin}}
*{{citation
| first1     = Claus
| last1      = Ibsen
| first2     = Jonathan
| last2      = Anstey
| date       = 2010
| title      = Camel in Action
| edition    = 1st
| publisher  = [[Manning Publications]]
| pages      = 552 
| isbn       = 978-1-935182-36-8
| url        = 
}}
*{{citation
| first1     = Scott 
| last1      = Cranton
| first2     = Jakub
| last2      = Korab
| date       = 2013
| title      = Apache Camel Developer's Cookbook
| edition    = 1st
| publisher  = [[Packt Publishing]]
| pages      = 424 
| isbn       = 978-1-782170-30-3
| url        = http://www.packtpub.com/apache-camel-developers-cookbook/book
}}
*{{citation
| first     = Bilgin
| last      = Ibryam
| date       = 2013
| title      = Instant Apache Camel Message Routing
| edition    = 1st
| publisher  = [[Packt Publishing]]
| pages      = 62 
| isbn       = 978-1-783283-47-7
| url        = http://www.packtpub.com/apache-camel-message-routing/book
}}
{{refend}}

==External links==
*[http://camel.apache.org/ Apache Camel Home]
*[http://camelone.org CamelOne Conference]
*[https://www.jboss.org/products/fuse.html JBoss Fuse based on Apache Camel]
* Methods &amp; Tools article - [http://www.methodsandtools.com/tools/tools.php?camel Camel, an Open Source Routing Framework]
* JDJ article - [http://opensource.sys-con.com/read/504392.htm SOA Made Easy with Open Source Apache Camel]
* DZone article - [http://architects.dzone.com/articles/apache-camel-integration Apache Camel: Integration Nirvana]
* [http://refcardz.dzone.com/refcardz/enterprise-integration Enterprise Integration Patterns with Apache Camel Refcard]
*[http://wiki.open-esb.java.net/Wiki.jsp?page=CamelSE Apache Camel JBI Service Engine]
*[http://cwiki.apache.org/qpid/ Apache [[Apache_Qpid]]]
*[http://talend.com/ Talend ESB based on Apache Camel]

{{apache}}

[[Category:Apache Software Foundation|Camel]]
[[Category:Enterprise application integration]]
[[Category:Message-oriented middleware]]
[[Category:Service-oriented architecture-related products]]
[[Category:Java platform]]
[[Category:Java enterprise platform]]
[[Category:Web scraping]]
{{business-software-stub}}</text>
      <sha1>4qhdntyxwsx7plotw267ivhhsoybe01</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Excalibur</title>
    <ns>0</ns>
    <id>12082169</id>
    <revision>
      <id>541644804</id>
      <parentid>479872899</parentid>
      <timestamp>2013-03-02T04:01:10Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 3 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q773156]]</comment>
      <text xml:space="preserve" bytes="1614">{{ Infobox Software
| name                   = Apache Excalibur
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version = 2.2.3
| latest release date    = {{release date|2007|07|05}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Inversion of control]] framework
| license                = [[Apache License]] 2.0
| website                = http://excalibur.apache.org
}}

'''Apache Excalibur''' project produces a set of libraries for component based programming in the Java language.  Its main products include the [[Inversion of Control]] framework Avalon, an Avalon based container named Fortress, and a set of Avalon compatible software components.

Excalibur spun out of the original Apache Avalon project following Avalon's closure in 2004.  Since that time Apache Excalibur has hosted the Avalon framework and related source code.

'''''Excalibur project has been retired by Apache Software foundation and it has been moved to [http://attic.apache.org/ Apache Attic]'''''
==See also==
* [[Apache Software Foundation]]
* [[Apache Cocoon]]

==External links==
* [http://excalibur.apache.org/ Apache Excalibur website]
* [http://wiki.apache.org/excalibur/ExcaliburHistory History of Apache Avalon and Excalibur]
{{apache}}

[[Category:Apache Software Foundation|Excalibur]]
[[Category:Free software programmed in Java]]

{{programming-software-stub}}</text>
      <sha1>orod29vi2exdqwbh4jny6uckgr9mkd3</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Abdera</title>
    <ns>0</ns>
    <id>12647688</id>
    <revision>
      <id>591826596</id>
      <parentid>544900761</parentid>
      <timestamp>2014-01-22T05:28:32Z</timestamp>
      <contributor>
        <username>S4saurabh</username>
        <id>6640499</id>
      </contributor>
      <minor/>
      <comment>Added external link to Abdera Mailing List Archives</comment>
      <text xml:space="preserve" bytes="2246">{{ Infobox Software
| name                   = Apache Abdera
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 1.1.2
| latest release date    = {{Release date|2011|01|15}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Web syndication|Syndication]]
| license                = [[Apache License]] 2.0
| website                = http://abdera.apache.org/
}}
'''Apache Abdera''' is an implementation of the [[Atom (standard)|Atom Syndication Format and Atom Publishing Protocol]], which are standards for creating, editing, and publishing [[web feed]]s and other web resources. The current focus is on a [[Java (programming language)|Java]] implementation, although [[C (programming language)|C]]/[[C++]] and [[.NET Framework|.NET]] implementations are being considered.&lt;ref&gt;[http://incubator.apache.org/abdera/faq.html#whatisabdera Apache Abdera &amp;mdash; Frequently Asked Questions], retrieved December 4, 2007&lt;/ref&gt;

The Abdera code was initially developed by [[IBM]]&lt;ref&gt;[http://wiki.apache.org/incubator/AbderaProposal Abdera Proposal], Apache Incubator Wiki, retrieved December 4, 2007&lt;/ref&gt; and donated to the [[Apache Software Foundation]] in June 2006.&lt;ref&gt;[http://wiki.apache.org/incubator/June2006#head-fcf82aa6d9dafe44705a11ec3f9e0174bd26dcaa June 2006 Report &amp;mdash; Abdera], Apache Incubator Wiki, retrieved December 4, 2007&lt;/ref&gt;

==Features==
* Consumption and production of Atom 1.0 feed and entry documents
* Atom Publishing Protocol client implementation
* A framework for the creation of an Atom Publishing Protocol server
* [[XML Signature|XML Digital Signature]] and encryption of Atom documents
* Support for format and protocol extensions

==References==
{{Reflist}}

==External links==
*[http://incubator.apache.org/abdera/ Abdera project home page]
*[http://qnalist.com/g/abdera Abdera mailing list archives]

{{Apache}}

[[Category:Apache Software Foundation|Log4net]]
[[Category:Java libraries]]


{{network-software-stub}}</text>
      <sha1>1q5jphrwn5d1hnm3n3a555qly6xp68n</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache HiveMind</title>
    <ns>0</ns>
    <id>1885573</id>
    <revision>
      <id>580101674</id>
      <parentid>574443657</parentid>
      <timestamp>2013-11-04T03:14:06Z</timestamp>
      <contributor>
        <username>ChrisGualtieri</username>
        <id>16333418</id>
      </contributor>
      <minor/>
      <comment>General Fixes using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="2398">{{For|the Will Wright game|HiveMind}}
{{multiple issues|
{{Notability|Products|date=September 2011}}
{{primary sources|date=September 2011}}
}}

{{ Infobox Software
| name                   = HiveMind
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Retired
| latest release version = 1.1.1
| latest release date    = {{release date|2007|12|19}}
| latest preview version =  2.0-alpha1
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Inversion of Control]] (IoC) Container
| license                = [[Apache License]] 2.0
| website                = http://hivemind.apache.org/
}}
'''Apache HiveMind''' is an [[Inversion of Control]] (IOC) software project of the [[Apache Software Foundation]] written in [[Java (programming language)|Java]]. It takes the form of a services and configuration [[microkernel]].

In HiveMind, a service is an implementation of a Java [[interface (computer science)|interface]]. Unlike other [[service-oriented architecture]]s (SOAs), HiveMind is explicit about combining Java code within a single [[JVM]].

The HiveMind project, formerly a top-level Apache project, was retired 22 April 2009,&lt;ref&gt;http://www.mail-archive.com/announce@apache.org/msg00674.html&lt;/ref&gt;&lt;ref&gt;http://attic.apache.org/projects/hivemind.html&lt;/ref&gt; ending its life in the Apache Attic repository.  Its successor is considered to be Tapestry IOC [[Apache Tapestry|Tapestry IOC]], a [[Guice]]-like [[Inversion of control|IoC]] container for [[Java EE]] [[Model-view-controller]] web applications.
(Tapestry's requirement for IoC was the reason why the HiveMind was conceived in the first place.)

==See also==
*[[Spring Framework]]

==References==
{{reflist|30em}}

== External links ==
*[http://hivemind.apache.org/ HiveMind home page]
*[http://hivetranse.sourceforge.net/ HiveMind Utilities]
*[http://wiki.apache.org/jakarta-hivemind/ HiveMind Wiki]
*[http://howardlewisship.com/blog/  HiveMind Blog]
*[http://paulreed.com/2006/07/27/hivemind-helloworld/ HiveMind Hello World]
*[https://springmodules.dev.java.net/  Spring HiveMind integration]

{{Apache}}

[[Category:Java platform]]
[[Category:Apache Software Foundation|HiveMind]]


{{Compu-soft-stub}}</text>
      <sha1>95i9vrr7rtett1501777j3jm5w4nea7</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>SuEXEC</title>
    <ns>0</ns>
    <id>12974560</id>
    <revision>
      <id>587531335</id>
      <parentid>544937485</parentid>
      <timestamp>2013-12-24T16:50:17Z</timestamp>
      <contributor>
        <ip>123.202.98.244</ip>
      </contributor>
      <text xml:space="preserve" bytes="2330">{{lowercase}}
Apache '''suEXEC''' is a feature of the [[Apache HTTP Server|Apache]] [[Web server]]. It allows users to run [[Common Gateway Interface|CGI]] and [[Server Side Includes|SSI]] applications as a different user - normally, all web server processes run as the default web server user (often wwwrun, Apache or [[nobody (username)|nobody]]). The '''suEXEC''' feature consists of a module for the web server and a [[Executable|binary]] executable which acts as a wrapper. suEXEC was introduced in Apache 1.2 and is often included in the default Apache package provided by most [[Linux distribution]]s.

If a client requests a CGI and suEXEC is activated, it will call the suEXEC binary which then wraps the CGI scripts and executes it under the user account of the server process (virtual host) defined in the virtual host directive.

Additionally, suEXEC perform a multi-step check on the executed CGI to ensure security for the server (including path-checks, a limit of permitted commands, etc.)&lt;ref&gt;[http://httpd.apache.org/docs/2.2/suexec.html apache.org - suEXEC Support]&lt;/ref&gt;

==Example==
User &quot;Alice&quot; has a website including some CGI scripts in her own public_html folder, which can be accessed by &lt;nowiki&gt;http://server/~alice&lt;/nowiki&gt;.

Bob now views Alice's webpage, which requires Apache to run one of these CGI scripts.

Instead of running all scripts as &quot;wwwrun&quot; (which results in the need that all scripts have to be readable and executable for the &quot;wwwrun&quot; group if the file is owned by that group or for all users otherwise), the scripts in /home/alice/public_html will be wrapped using suEXEC and run with Alice's user ID resulting in higher security and eliminating the need to make the scripts readable and executable for all users or everyone in the &quot;wwwrun&quot; group (instead only alice herself needs to be able to run the script).

== References==
&lt;references/&gt;

==External links==
* [http://httpd.apache.org/docs/2.2/suexec.html apache.org - suEXEC Support (Apache 2.2)]
* [http://httpd.apache.org/docs/1.3/suexec.html apache.org - suEXEC Support (Apache 1.3)]

[[Category:Unix network-related software]]
[[Category:Apache Software Foundation|HTTP Server]]
[[Category:Apache httpd modules]]
[[Category:Computer security software]]
[[Category:Unix security-related software]]


{{security-software-stub}}</text>
      <sha1>6tcf1q5kur4umg15ag7y41y6rawbqs4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>WebWork</title>
    <ns>0</ns>
    <id>2316975</id>
    <revision>
      <id>597265992</id>
      <parentid>593856881</parentid>
      <timestamp>2014-02-26T19:35:49Z</timestamp>
      <contributor>
        <username>Benjamin Mestrallet</username>
        <id>20658134</id>
      </contributor>
      <comment>/* See also */</comment>
      <text xml:space="preserve" bytes="6555">{{About|a web application framework||Webwork (disambiguation){{!}}Webwork}}
{{more footnotes|date=February 2008}}
{{ Infobox Software
| name                   = Webwork
| developer              = [[OpenSymphony]]
| latest release version = 2.2.6
| latest release date    = {{release date|2007|07|21}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)]]
| status                 = Unmaintained
| genre                  = [[Web application framework|Web Framework]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://www.opensymphony.com/webwork/}}
}}
'''WebWork''' was a Java-based [[web application framework]] developed by [[OpenSymphony]] that merged into the current [[Struts2]] framework. It was developed with the specific intention of improving developer productivity and code simplicity. WebWork is built on top of [[XWork]], which provides a generic [[command pattern]] framework as well as an [[Inversion of Control]] container.

WebWork provides support for building reusable UI templates, such as form controls, UI themes, [[internationalization]], dynamic form parameter mapping to [[JavaBeans]], client and server side validation, etc.

==Design goals and overview==
{{Advert|date=May 2008}}
In a standard [[Java Platform, Enterprise Edition|Java EE]] web application, the client will typically submit information to the server via a [[form (web)|web form]]. The information is then either handed over to a [[Java Servlet]] which processes it, interacts with a database and produces an [[HyperText Markup Language|HTML]]-formatted response, or it is given to a [[JavaServer Pages]] (JSP)  document which intermingles HTML and Java code to achieve the same result. Both approaches are often considered inadequate for large projects because they mix application logic with presentation and make maintenance difficult.

WebWork attempts to understand existing frameworks' limitations and works to eliminate them. It supports type conversion, continuations, and interceptors. WebWork also supports multiple-view technologies like JSP, velocity, and FreeMarker.

Ultimately, WebWork has been designed and implemented with a specific set of goals, that are very important for its users. They are as follows:

#Web Designer never has to touch Java code
#Create multiple &quot;Web Skins&quot; for an application
#Change Look and Feel
#Change Layout on a given Web Page
#Change Flow among Web Pages
#Move *existing* data elements from one page to another
#Integrate with various backend infrastructures
#Reuse components
#Perform [[Internationalization and localization|internationalization (i18n)]] of a web application
#Keep the API small and to the point
#Ability to learn WebWork fast, by making all the fancier features optional
#Allow the developer to choose how to implement as much as possible, while providing default implementations that work well in most cases [http://www.opensymphony.com/webwork_old/src/docs/manual/faq.html]

== WebWork lifecycle ==

The architecture of WebWork is based on the [[Model-view-controller|MVC]] Framework, Command, and Dispatcher patterns and the principle of Inversion of Control. The life cycle of a WebWork request begins when the servlet container receives a new request. The new request is passed through a set of filters called the filter chain and sent to the FilterDispatcher. The FilterDispatcher forwards the request to the ActionMapper to determine what needs to be done with the request. If the request requires an action, it sends an ActionMapping object back to the FilterDispatcher. If not, ActionMapper returns a null object, indicating that no action needs to be taken. The FilterDispatcher forwards the request and the ActionMapper object to the ActionProxy for further action. The ActionProxy invokes the Configuration File manager to get the attributes of the action, which is stored in the xwork.xml file and creates an ActionInvocation object. The ActionInvocation object contains attributes like the action, invocation context, result, result code, etc. The configuration file manager has access to these configuration files and is used by the ActionProxy as a gateway to the configuration files. The ActionInvocation object also has information about Interceptors that need to be invoked after or before an action is executed. 

ActionInvocation invokes all the interceptors listed in the ActionInvocation object and then invokes the actual action. When the action is completed, ActionInvocation gets the action result code from the execution. It uses the action result code to look up the appropriate result, which is usually a JSP page, a velocity template or a freemarker template associated with the result code. ActionInvocation also executes the interceptors again in the reverse order and returns the response as a HttpServletResponse.&lt;ref&gt;{{cite web|url=http://javaboutique.internet.com/reviews/webworks/ |title=Java(TM) Boutique - Review - WebWork: The New Framework on the Block |publisher=Javaboutique.internet.com |date=2006-03-24 |accessdate=2012-03-06}}&lt;/ref&gt;

== WebWork / Struts merger ==

On November 27, 2005, WebWork developer Patrick Lightbody announced that WebWork would be merging in to [[Apache Struts|Struts]]2. [http://blogs.opensymphony.com/webwork/2005/11/webwork_joining_struts.html] While the next major release (WebWork 2.2.x) was released under the WebWork name, all future major revisions (namely, 2.3.x and beyond) would be folded into Struts2.

Ted Husted, developer of Struts admitted in an email that WebWork is very similar to Struts 1.x and in fact does certain things better than Struts. Both Husted and Lightbody's rationale was that combining WebWork's superior technology and Struts' superior community could only have a good effect on the Java community as a whole.

== License ==
WebWork uses the [[OpenSymphony Software License]] which is a modified (and fully compatible with) [[Apache Software License]].

==See also==
* [[Apache Struts]]
* [[JavaServer Faces]]
* [[Juzu Web Framework]]
* [[Tapestry (programming)|Tapestry]]
* [[Struts2]]

== References ==
{{reflist}}

==External links==
* [http://www.opensymphony.com/webwork/ WebWork homepage]
* [http://wiki.opensymphony.com/display/WW/WebWork WebWork Wiki]
* [http://javaboutique.internet.com/reviews/webworks/ WebWork: The New Framework on the Block]

{{Application frameworks}}

[[Category:Apache Software Foundation]]
[[Category:Web application frameworks]]
[[Category:Java enterprise platform]]</text>
      <sha1>29hmrlxfg0jdmtn6v3qmb5xuko8yz2s</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Harmony</title>
    <ns>0</ns>
    <id>1894247</id>
    <revision>
      <id>599461195</id>
      <parentid>599108580</parentid>
      <timestamp>2014-03-13T17:31:39Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor/>
      <comment>sp</comment>
      <text xml:space="preserve" bytes="27649">{{Update|inaccurate=yes|article|date=February 2013}}
{{Infobox software
| name                   = Apache Harmony
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Discontinued&lt;ref&gt;{{cite news |url=http://www.h-online.com/open/news/item/The-end-of-Apache-Harmony-1371819.html |title=The end of (Apache) Harmony |publisher=The H |date=4 November 2011}}&lt;/ref&gt;
| latest release version = 5.0M15&lt;br/&gt;6.0M3
| latest release date    = {{Start date and age|2010|09|15}}
| latest preview version =
| latest preview date    =
| operating system       = [[Microsoft Windows|Windows]], [[Linux]]
| programming language   = [[C++]], [[Java (programming language)|Java]]
| genre                  = [[Java Virtual Machine]], Java [[Library (computer science)|Library]]
| license                = [[Apache License]] 2.0
| website                = {{URL|harmony.apache.org}}
}}
'''Apache Harmony''' was an [[open source]], [[free Java implementations|free Java implementation]], developed by the [[Apache Software Foundation]].&lt;ref&gt;{{cite web
| url=http://harmony.apache.org/faq.html
| title=Original FAQ Questions from Project Launch
| publisher=harmony.apache.org
|quote=''We are starting with Java SE 5, as that is the first version of Java SE for which the licensing allows an open source implementation, but we'll continue with Java SE 6 and any subsequent versions that follow.''
| accessdate=2011-02-27}}&lt;/ref&gt; It was announced in early May 2005 and on October 25, 2006, the Board of Directors voted to make Apache Harmony a top-level project. The Harmony project achieved (as of February 2011)  99% completeness for [[J2SE 5.0]], and 97% for [[Java SE 6]].&lt;ref name=&quot;completeness&quot;&gt;{{cite web
| url=http://harmony.apache.org/subcomponents/classlibrary/status.html
| title=Class Library Component Status
| publisher=harmony.apache.org
| accessdate=2011-02-27}}&lt;/ref&gt;

On October 29, 2011 a vote was started by the project lead Tim Ellison whether to retire the project. The outcome was 20 to 2 in favor,&lt;ref name=&quot;atticproposal&quot;&gt;{{cite web |url=http://mail-archives.apache.org/mod_mbox/harmony-dev/201111.mbox/browser |archiveurl=http://old.nabble.com/-Result--Move-Apache-Harmony-to-the-Attic-%28updated%29-p32772228.html |title=Move Apache Harmony to the Attic (updated)|first=Tim |last=Ellison |publisher=org.apache.harmony.dev |archivedate=2011-11-03 |date=2011-11-03}}&lt;/ref&gt; and the project was retired on November 16, 2011.&lt;ref name=&quot;retired&quot;&gt;{{cite web |url=http://mail-archives.apache.org/mod_mbox/harmony-dev/201111.mbox/browser |archiveurl=http://old.nabble.com/Board-accepted-attic-resolution-td32862837.html |title=Board accepted attic resolution |first=Tim |last=Ellison |publisher=org.apache.harmony.dev |date=2011-11-16 |archivedate=2011-11-16 |accessdate=2011-11-27}}&lt;/ref&gt;

== History ==

=== Initiation ===
The Harmony project was initially conceived as an effort to unite all developers of the [[free Java implementations]]. Many [[software developer|developers]] expected that it would be the project above the [[GNU]], [[Apache Software Foundation|Apache]] and other communities. GNU developers were invited into and participated during the initial, preparatory planning.&lt;ref&gt;{{cite web| url=http://article.gmane.org/gmane.comp.java.classpath.devel/5521| title=Harmony!| author=Mark Wielaard| date=2005-05-09| quote=''Apache has set up a proposal for discussion around a full free j2se implementation. Which they call &quot;Harmony&quot;. This is (at the moment) not about code, but about finding out a direction for getting to such a beast. Dalibor, Tom, Jeroen and I were asked to help them in that discussion and possibly show them how to setup a good architecture for it.''}}&lt;/ref&gt;

=== Incompatibility with GNU Classpath ===
Despite the impression given by the preparatory planning, it was decided not to use the code from [[GNU Classpath]], and that Harmony would use an incompatible license; therefore blocking the collaboration between Harmony and existing free Java projects.&lt;ref&gt;{{cite web| url=http://lwn.net/Articles/135111/| title=A proposal for a free Java implementation| author=Geir Magnusson Jr.| date=2006-05-24| publisher=[[Apache Software Foundation|Apache]]}}&lt;/ref&gt; Apache developers would then [[rewrite (programming)|write the needed classes from scratch]] and expect necessary large code donations from [[software company|software companies]]. Various misunderstandings at the start of the project, and the fact that major companies like [[IBM]] proposed to give large amount of existing code, created some confusion in the free Java community about the real objectives of the project.&lt;ref name=&quot;towardfree&quot;&gt;{{cite web| url=http://lwn.net/Articles/184967/| title=Toward a free Java| author=Mark Wielaard| date=2006-05-24| publisher=[[LWN.net]]|quote=''All this means that, despite the fact that there is now some code available donated by Intel, there is no practical cooperation between the original free software projects backing Harmony and the project now known as Apache Harmony. All this made some people think of Harmony as a company consortium in the guise of an ASF project and not a full community project.''}}&lt;/ref&gt;

One major point of incompatibility between the GNU Classpath and Apache Harmony projects was their incompatible licenses: Classpath's [[GNU General Public License]] with the [[GPL linking exception|linking exception]] versus Harmony's [[Apache License]].&lt;ref name=towardfree/&gt;

===Difficulties to obtain a TCK license from Sun===
{{See also|Technology Compatibility Kit}}

On April 10, 2007, the [[Apache Software Foundation]] sent an [[open letter]] to [[Sun Microsystems]] [[Chief executive officer|CEO]], [[Jonathan I. Schwartz|Jonathan Schwartz]] regarding their inability to acquire an acceptable license for the Java SE 5 [[Technology Compatibility Kit]] (TCK), a test kit needed by the project to demonstrate compatibility with the Java SE 5 specification, as needed by the [[Sun Microsystems|Sun]] specification license for Java SE 5.&lt;ref&gt;[http://www.apache.org/jcp/sunopenletter.html Open Letter to Sun Microsystems]&lt;/ref&gt; What makes the license unacceptable for [[Apache Software Foundation|ASF]] is the fact that it imposes rights restrictions through limits on the &quot;field of use&quot; available to users of Harmony, not compliant with the [[Java Community Process]] rules.&lt;ref&gt;According to ASF, 1) a specification lead cannot ''impose any contractual condition or covenant that would limit or restrict the right of any licensee to create or distribute such Independent Implementations'' (section 5.C.III), and 2) ''a specification lead must license all necessary IP royalty-free to any compatible implementation of a specification'' (section 5.B).&lt;/ref&gt;

Sun answered on a company blog&lt;ref&gt;http://blogs.sun.com/ontherecord/&lt;/ref&gt;&lt;ref&gt;http://java.sys-con.com/read/360602.htm&lt;/ref&gt; that it intended to create an open source implementation of the Java platform under [[GPL]], including the TCK, but that their current priority was to make the [[Java (software platform)|Java Platform]] accessible to the [[Linux|GNU/Linux]] community under [[GPL]] as quickly as possible.

This answer triggered some reactions, either criticizing [[Sun Microsystems|Sun]] for not responding &quot;in a sufficiently open manner&quot; to an open letter,&lt;ref&gt;http://ianskerrett.wordpress.com/2007/04/16/the-silence-from-an-open-sun/&lt;/ref&gt; or rather [[Apache Software Foundation]]; some think that ASF acted unwisely to aggressively demand something they could have obtained with more diplomacy from Sun, especially considering the timescale of the opening class library.&lt;ref&gt;http://gnu.wildebeest.org/diary/2007/04/21/openjck/&lt;/ref&gt;&lt;ref&gt;{{cite web
| url=http://jroller.com/page/dgilbert?entry=five_reasons_why_apache_will
| title=Five Reasons Why Apache Will Regret That Open Letter
| last=Gilbert|first=Dave
| date=2007-04-16
| accessdate=2008-03-08}}&lt;/ref&gt;

Since Sun's release of [[OpenJDK]], [[Sun Microsystems|Sun]] has released a specific license to allow to run the TCK in the OpenJDK context for any [[GNU General Public License|GPL]] implementation deriving substantially from OpenJDK.&lt;ref&gt;{{cite web
| url=http://openjdk.java.net/legal/openjdk-tck-license.pdf
| title=OpenJDK Community TCK License Agreement V 1.1
| publisher=[[Sun Microsystems]]
|quote=''Subject to and conditioned upon its Licensee Implementation being substantially derived from OpenJDK Code and, if such Implementation has or is to be distributed to a third party, its being distributed under the GPL License, Sun hereby grants to Licensee, to the extent of Sun's Intellectual Property Rights in the TCK, a worldwide, personal, non-exclusive, non-transferable, limited license to use the TCK internally and solely for the purpose of developing and testing Licensee Implementation.''
| accessdate=2008-03-08|format=PDF}}&lt;/ref&gt;

On December 9, 2010, the Apache Software Foundation resigned from the Java Community Process Executive Committee,&lt;ref&gt;{{cite web | url=https://blogs.apache.org/foundation/entry/the_asf_resigns_from_the | title=The ASF Resigns From the JCP Executive Committee | publisher=Apache Software Foundation}}&lt;/ref&gt; in protest over the difficulty in obtaining a license acceptable to Apache for use with the Harmony project.&lt;ref&gt;{{Cite web | url=http://arstechnica.com/open-source/news/2010/12/apache-resigns-from-jcp-in-protest-of-oracle-governance-failures.ars | title=Apache quits Java governance group in protest of Oracle abuses | publisher=Ars Technica }}&lt;/ref&gt;

===Use in Android SDK===
[[Dalvik virtual machine|Dalvik]], the virtual machine used in [[Google]]'s [[Android (operating system)|Android]] platform, uses a subset of Harmony for the core of its [[Java Class Library|Class Library]].&lt;ref&gt;{{cite web
| url=http://www.infoq.com/news/2007/11/android-java
| title=Google's Android SDK Bypasses Java ME in Favor of Java Lite and Apache Harmon
| publisher=infoq.com
| date=2007-11-12
| quote=''Instead of providing a full version of the Java SE or Java ME Google has diverged on two fronts. First, a limited subset of the core Java packages is provided. (...) By going this route Android is following in the footsteps of another Google project GWT which uses Java as its development language but does not support the full JDK.''
| accessdate=2009-05-31}}&lt;/ref&gt;&lt;ref&gt;{{cite web
| url=http://developer.android.com/reference/packages.html
| title=Package Index
| publisher=[[Open Handset Alliance]]
| accessdate=2009-05-31| archiveurl= http://web.archive.org/web/20090627025423/http://developer.android.com/reference/packages.html| archivedate= 27 June 2009 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; However, Dalvik does not align to [[Java Platform, Standard Edition|Java SE]] nor [[Java Platform, Micro Edition|Java ME]] [[Java Class Library|Class Library]] profiles (for example [[Java Platform, Micro Edition|J2ME]] classes, [[Abstract Window Toolkit|AWT]] and [[Swing (Java)|Swing]] are not supported).

===Disengagement from IBM===
On 11 October 2010, [[IBM]], by far the biggest participant in the project, decided to join [[Oracle Corporation|Oracle]] on the [[OpenJDK]] project, effectively shifting its efforts from Harmony to the [[Oracle Corporation|Oracle]] reference implementation.&lt;ref&gt;{{cite web
| url=http://www.marketwire.com/press-release/Oracle-and-IBM-Collaborate-to-Accelerate-Java-Innovation-Through-OpenJDK-NASDAQ-ORCL-1332855.htm
| title=Oracle and IBM Collaborate to Accelerate Java Innovation Through OpenJDK
| publisher=[[Oracle Corporation]]
| accessdate=2010-10-22| archiveurl= http://web.archive.org/web/20101014150141/http://www.marketwire.com/press-release/Oracle-and-IBM-Collaborate-to-Accelerate-Java-Innovation-Through-OpenJDK-NASDAQ-ORCL-1332855.htm| archivedate= 14 October 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;&lt;ref&gt;{{cite web
| url=http://arstechnica.com/open-source/news/2010/10/ibm-joins-openjdk-as-oracle-shuns-apache-harmony.ars
| title=Java wars: IBM joins OpenJDK as Oracle shuns Apache Harmony
|author=Ryan Paul
| publisher=[[Ars Technica]]
| accessdate=2010-10-22| archiveurl= http://web.archive.org/web/20101019094951/http://arstechnica.com/open-source/news/2010/10/ibm-joins-openjdk-as-oracle-shuns-apache-harmony.ars| archivedate= 19 October 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; Bob Sutor, IBM's head of Linux and open source, blogged that &quot;IBM will be shifting its development effort from the Apache Project Harmony to OpenJDK&quot;.&lt;ref&gt;{{cite web
| url=http://www.sutor.com/c/2010/10/ibm-joins-the-openjdk-community/
| title=IBM joins the OpenJDK community, will help unify open source Java efforts
|author=Bob Sutor
|quote=''IBM will be shifting its development effort from the Apache Project Harmony to OpenJDK. For others who wish to do the same, we’ll work together to make the transition as easy as possible. IBM will still be vigorously involved in other Apache projects.''
| accessdate=2010-10-22| archiveurl= http://web.archive.org/web/20101018160132/http://www.sutor.com/c/2010/10/ibm-joins-the-openjdk-community/| archivedate= 18 October 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

On March 2011, [[IBM]]'s Tim Ellison announced that he resigned as Project Management Chair for Harmony, a move which brought questions about the future of the project.&lt;ref&gt;{{cite web
| url=http://harmony.markmail.org/message/ah5f42h4p2bhub6o
| title=Rebooting the Harmony project
| author=Tim Ellison
| date=2011-03-14
| accessdate=2011-03-20}}&lt;/ref&gt;&lt;ref&gt;{{cite web
| url=http://www.infoq.com/news/2011/03/apache-harmony
| title=What is the Future of Apache Harmony?
| publisher=infoq.com
| date=2011-03-14
| accessdate=2011-03-20}}&lt;/ref&gt; Since the beginning of 2011, there has been almost no more development on the project, nor discussions on the mailing list.&lt;ref&gt;{{cite web
| url=http://mail-archives.apache.org/mod_mbox/harmony-dev/
| title=Mailing list archives: dev@harmony.apache.org
| publisher=apache.org
| accessdate=2011-09-11}}&lt;/ref&gt;

===End of the project===
On October 29, 2011 a poll was started on the harmony-dev mailing list by the project lead Tim Ellison whether to retire the project. The outcome on November 3 was 20 to 2 in favor of retirement.&lt;ref name=&quot;atticproposal&quot; /&gt;  On November 16, 2011, the Apache Software Foundation board of directors passed a motion to terminate the project.&lt;ref name=ASFBODminutes&gt;{{cite web | url=http://www.apache.org/foundation/records/minutes/2011/board_minutes_2011_11_16.txt | title=Board of Directors Meeting Minutes | publisher=Apache Software Foundation | date=November 16, 2011 | accessdate=August 8, 2012}}&lt;/ref&gt;   One director,  [[Lawrence Rosen (attorney)|Larry Rosen]], cast a &quot;no&quot; vote, based on the timing rather than the merits of the proposal; it was otherwise unanimous.&lt;ref name=ASFBODminutes/&gt;  The project was retired on November 16, 2011.&lt;ref name=&quot;retired&quot; /&gt;

== Development team ==
At the start, Apache Harmony received some large code contributions from several companies. Development discussions have taken place on open mailing lists.  Later, the Apache Software foundation mentors put a lot of effort
into bringing the development process more in line with &quot;the Apache way,&quot;&lt;ref&gt;{{cite web| url=http://article.gmane.org/gmane.comp.java.harmony.devel/10742| title=We would like all the development to happen right here|author=Leo Simons| date=2006-07-24}}&lt;/ref&gt;&lt;ref&gt;{{cite web| url=http://incubator.apache.org/learn/theapacheway.html| title=The Apache Way}}&lt;/ref&gt; and it seemed that their efforts were successful.

== Last development status ==
Apache Harmony was accepted among the official Apache projects on 29 October 2006.

=== Architecture ===
&lt;!--  Commented out because image was deleted: [[Image:DRL structure.gif|300px|thumb|Major Dynamic Runtime Layer Components and their interfaces]] --&gt;
The Dynamic Runtime Layer virtual machine consists of the following components:

#'''The VM core:''' with its subcomponents concentrates most of the JVM control functions.
#'''The porting layer''': hides platform-specific details from other VM components behind a single interface and is based on the [[Apache Portable Runtime]] layer.
#'''The [[garbage collection (computer science)|garbage collector]]''': allocates Java objects in the heap memory and reclaims unreachable objects using various algorithms
#'''Execution Manager''': selects the execution engine for compiling a method, handles profiles and the dynamic recompilation logic.
#'''Class Library''':  is a Java standard library.
#'''The thread manager''' that handle operating system threading
#'''The execution engine:''' consists of the following:
##The [[just-in-time compiler]] for compilation and execution of method code.
##The [[interpreter (computing)|interpreter]] for easier debugging.

===Support platform and operating system===
The project provided a portable implementation that ease development on many platforms and operating systems. The main focus was on [[Microsoft Windows|Windows]] and [[Linux]] operating systems on x86 and x86-64 architectures.&lt;ref&gt;[http://harmony.apache.org/supported_platforms.html Apache Harmony supported platforms and operating systems]&lt;/ref&gt;
{| class=&quot;wikitable&quot;
|-
!
! [[Windows 2000]]
! [[Microsoft Windows|Windows]] [[Windows XP|XP]], [[Windows Server 2003|Server 2003]], [[Windows Vista|Vista]]
! [[Linux]] [[RHEL]], [[SUSE Linux Enterprise Server|SLES]], [[Debian]], [[Gentoo Linux|Gentoo]], [[Fedora (operating system)|Fedora]]
! [[FreeBSD]]
! [[AIX]]
! [[Mac OS X]]
|-
| '''[[IA-32]] (Pentium III or better)'''
| {{No}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{n/a}}
| {{n/a}}
|-
| '''[[x86-64]] (Intel 64, AMD64)'''
| {{n/a}}
| {{Yes}}
| {{Yes}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|-
| '''[[Itanium]] (IA64, IPF)'''
| {{n/a}}
| {{No}}
| {{Yes}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|-
| '''[[PowerPC|PowerPC 32-bit]]'''
| {{n/a}}
| {{n/a}}
| {{No}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|-
| '''[[ppc64|PowerPC 64-bit]]'''
| {{n/a}}
| {{n/a}}
| {{No}}
| {{n/a}}
| {{No}}
| {{n/a}}
|-
| '''[[IBM System z|zSeries]] 31-bit'''
| {{n/a}}
| {{n/a}}
| {{No}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|}

=== Class library coverage ===

The expected donations from software companies were actually received. The Apache Harmony now contains the working code, including the [[Swing (Java)|Swing]], [[Abstract Window Toolkit|AWT]] and [[Java 2D]] code which were contributed by [[Intel]].

The Harmony project currently achieve (as of February 2011)  99% completeness for [[J2SE 5.0|JDK 5.0]], and 97% for [[Java SE 6]].&lt;ref name=completeness/&gt;

The progress of the Apache Harmony project can be tracked against J2SE 1.4 and Java SE 5.0.&lt;ref&gt;[http://people.apache.org/~chunrong/latest-harmony-japi.html Apache Harmony Library Coverage against Java SE 5.0]&lt;/ref&gt; Also, there is a branch for Harmony v6.0 in development for Java SE 6.0.

Apache Harmony developers integrate several existing, field-tested open-source projects to meet their goal (not [[reinventing the wheel]]). Many of these projects are mature and well known and other parts of the library needed to be written from scratch.

This is a list of existing open source components that are used in the Apache Harmony project; some of them were in use before the project started.

{| class=&quot;wikitable&quot;
! Component
! Description
|-
| [[International Components for Unicode|ICU]]
| Mature C/C++ and Java libraries for [[Unicode]] support and [[Internationalization and localization|software internationalization and globalization]]
|-
| [[Apache Xalan]]
| [[XSLT]] stylesheet processor for [[Java (programming language)|Java]], [[C++]] which implements [[XPath]] language
|-
| [[Apache Xerces]]
| [[XML parser]] library for Java, C++, [[Perl]]
|-
| [[Apache Portable Runtime]]
| [[Cross-platform]] abstraction library, provides platform independence
|-
| [[Apache CXF]]
| Robust, high performance [[Web service]]s framework work over protocols such as [[SOAP]], XML/HTTP, [[Representational State Transfer|RESTful]] HTTP, [[CORBA]]
|-
| [[Byte Code Engineering Library|BCEL]]
| Libraries to decompose, modify, and recompose binary Java classes, i.e., [[bytecode]]
|-
| [[MX4J]]
| [[Java Management Extensions]] (JMX) tools to manage and monitor applications, system objects, devices and service oriented networks
|-
| [[Jikes RVM#VM Magic|VM Magic]]
| Set of extensions to Java language to facilitate systems programming in Java by adding direct memory operations, etc.
|-
| [[Bouncy Castle (cryptography)|Bouncy Castle]]
| Libraries collection of lightweight cryptography for Java and [[C Sharp (programming language)|C#]]
|-
| [[ANTLR]]
| Language tool, provides a framework to construct recognizers, interpreters, compilers, and translators from grammatical descriptions containing actions in many target languages
|}

=== Documentation ===
Harmony is currently less documented than the alternative free Java implementations. For instance, in GNU Classpath every method of the central [[CORBA]] class (ORB) has the explaining comment both in the standard abstract API class&lt;ref&gt;http://cvs.savannah.gnu.org/viewcvs/*checkout*/classpath/org/omg/CORBA/ORB.java?rev=1.2.2.12&amp;root=classpath&lt;/ref&gt; and implementation.&lt;ref&gt;http://cvs.savannah.gnu.org/viewcvs/*checkout*/classpath/gnu/CORBA/OrbFunctional.java?rev=1.6&amp;root=classpath&lt;/ref&gt; In the [http://incubator.apache.org/yoko/ Yoko] project, used by Harmony,&lt;ref&gt;http://www.mail-archive.com/yoko-dev@incubator.apache.org/msg01428.html&lt;/ref&gt; most methods both in the standard declaration&lt;ref&gt;http://svn.apache.org/repos/asf/incubator/yoko/trunk/yoko-spec-corba/src/main/java/org/omg/CORBA/ORB.java&lt;/ref&gt; and implementing class&lt;ref&gt;http://svn.apache.org/repos/asf/incubator/yoko/trunk/core/src/main/java/org/apache/yoko/orb/OBCORBA/ORB_impl.java&lt;/ref&gt; were undocumented at the end of October, 2006. Also, GNU Classpath supported both older and current CORBA features (same as Sun's implementation). Harmony, differently, left the central method of the older standard (&lt;code&gt;ORB.connect(Object)&lt;/code&gt;) fully unimplemented.

=== Tools ===
A complete implementation of the Java platform also needs a [[compiler]] that translates Java source code into [[bytecode]]s, a program that manages [[JAR (file format)|JAR files]], a [[debugger]], and an [[applet]] viewer and [[web browser]] [[Plug-in (computing)|plugin]], to name a few. Harmony currently has the [[Java compiler|compiler]], [[appletviewer]], jarsigner, javah, javap, [[keytool]], policytool, and [[unpack200]] [http://incubator.apache.org/harmony/roadmap.html#General].

=== Virtual machine support ===
Harmony currently has seven [[virtual machine]] implementations that run Harmony Class Library, all of which were donations by external groups:
* JC Harmony Edition VM, &quot;JCHEVM,&quot; based on the [[JC virtual machine|JCVM's]] [[interpreter (computing)|interpreter]], contributed by the author, Archie Cobbs.
* BootJVM, a simple [[bootstrapping (computing)|bootstrapping]] virtual machine, contributed by Daniel Lydick.
* [[SableVM]], an advanced, portable interpreter, contributed by authors from the [[Sable Research Group]]; and the Dynamic Runtime Layer Virtual Machine.
* [[DRLVM]], a [[just-in-time compiler]] contributed by [[Intel]].
* BEA announced the availability of an evaluation version of JRockit VM running Apache Harmony Class Library.&lt;ref&gt;[http://mail-archives.apache.org/mod_mbox/harmony-dev/200701.mbox/%3C1D7F0297-B0C5-4B05-AD27-B457B309C425@pobox.com%3E BEA JRockit VM under a binary, evaluation-only license]&lt;/ref&gt;
* [[JikesRVM]], an open-source [[meta-circular evaluator|meta-circular]] JVM that use the Apache Harmony Class Library.&lt;ref&gt;[http://mail-archives.apache.org/mod_mbox/harmony-dev/200808.mbox/%3cb1e4cffb0808070820u2554f4faw4d98aa5059b5b425@mail.gmail.com%3e Announcing Jikes RVM 3.0 + Apache Harmony]&lt;/ref&gt;
* [[Ja.NET SE]], an open source project providing a Java 5 JDK (class libraries, tools, etc.) that run on the [[.NET Framework]] CLR. Ja.NET SE is based on the Apache Harmony Class Libraries.&lt;ref&gt;[http://www.janetdev.org/ Ja.NET SE an open source  project is providing a Java 5 JDK running on the .NET CLR]&lt;/ref&gt;
In the end of November, 2006, the language support provided by these virtual machine was still incomplete, and the build instructions recommended to use [[IBM]]'s [[proprietary software|proprietary]] J9 instead to run the class library test suite. However, this is not necessary anymore (as of July 2007).

As for the rest of the project, DRLVM [[virtual machine]] development has now stalled (as of May 2011).&lt;ref name=&quot;jiracommits&quot;&gt;{{cite web
| url=https://issues.apache.org/jira/browse/HARMONY#selectedTab=com.atlassian.jira.plugin.ext.subversion%3Asubversion-project-tab
| title=Subversion Commits
| publisher=harmony.apache.org
| accessdate=2011-05-28}}&lt;/ref&gt;

==Application status==
Since its conception, Harmony grew in its ability to execute non-trivial Java applications.&lt;ref&gt;http://wiki.apache.org/harmony/Application_Status&lt;/ref&gt; {{as of|2007|7}}, supported applications include:
* [[Eclipse (software)|Eclipse]]: 99.3% of the 36000 [[reference implementation (computing)|reference implementation]] (RI) test pass on Harmony's DRLVM + class library.&lt;ref&gt;http://wiki.apache.org/harmony/Eclipse_Unit_Tests_Pass_on_DRLVM#PassRate_2007&lt;/ref&gt;
* [[Apache Tomcat]]: 100% of the RI tests pass.&lt;ref&gt;http://wiki.apache.org/harmony/Apache_Tomcat&lt;/ref&gt;
* [[JUnit]]: 100% of the RI tests pass.&lt;ref&gt;http://wiki.apache.org/harmony/JUnit&lt;/ref&gt;
* [[Apache Ant]]: 97% of the RI tests pass.&lt;ref&gt;http://wiki.apache.org/harmony/Apache_Ant&lt;/ref&gt;
* Other applications pass with a high success rate, such as [[Apache Derby]], [[Apache Axis]], [[Log4j]], [[Apache Velocity]], [[Apache Cocoon]], [[jEdit]], and [[Apache Commons]].

However, Harmony's incomplete library prevented it from launching some other applications:
* [[ArgoUML]]: because it needs a [[Java applet]] implementation, which was still unavailable in Harmony.
* [[Apache Geronimo]] runs on Apache Harmony with some issues and workarounds.&lt;ref&gt;[http://cwiki.apache.org/confluence/display/GMOxDOC20/Apache+Harmony Running Geronimo on Harmony]&lt;/ref&gt;
* [[Vuze]], formerly Azureus, because of unimplemented security classes.

==See also==
{{Portal|Free software|Java}}

*[[GNU Classpath]]
*[[List of Java virtual machines]]
*[[Free Java implementations]]
*[[Java Class Library]]
*[[OpenJDK]]
*[[IcedTea]]

==References==
{{Reflist|2}}

==External links==
* {{official website|harmony.apache.org}}
*[http://mail-archives.apache.org/mod_mbox/incubator-general/200505.mbox/%3CE3603144-2C26-4C31-896D-6CC7445A63EB@apache.org%3E Apache Harmony FAQ]
*[http://svn.apache.org/viewvc/harmony/?root=Apache-SVN Apache Harmony source code repository]
*[http://developers.sun.com/learning/javaoneonline/2006/coreplatform/TS-3752.html JavaOne 2006 Online Harmony Session]
*[http://developers.sun.com/learning/javaoneonline/j1sessn.jsp?sessn=TS-7820&amp;yr=2007&amp;track=6 JavaOne 2007 Online Harmony Session]
*[http://parleys.com/display/PARLEYS/Apache+Harmony?showComments=true Apache Harmony] by Geir Magnusson Jr at JavaPolis 2006
*[http://www.osnews.com/story.php?news_id=10806 The Java open source debate - a good summary of the debate]
*[http://www.infoq.com/news/2011/03/apache-harmony What is the Future of Apache Harmony?]
*[http://www.h-online.com/open/news/item/Apache-Harmony-loses-project-manager-1210343.html Apache Harmony loses project manager]

{{Java Virtual Machine}}
{{Java (Sun)}}
{{apache}}

[[Category:Apache Software Foundation|Harmony]]
[[Category:Java virtual machine]]
[[Category:Java libraries]]
[[Category:Java programming language]]</text>
      <sha1>t9h84xw1yptovkvuim9vaf6sxatl7ct</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Chainsaw (log file viewer)</title>
    <ns>0</ns>
    <id>14109962</id>
    <revision>
      <id>573283100</id>
      <parentid>553616814</parentid>
      <timestamp>2013-09-17T08:53:53Z</timestamp>
      <contributor>
        <ip>176.252.114.149</ip>
      </contributor>
      <comment>/* Alternatives */</comment>
      <text xml:space="preserve" bytes="2483">{{ Infobox Software
| name                   =  Apache Chainsaw
| logo                   = &lt;!-- Missing image removed: [[Image:Apache Chainsaw Logo.png|150px|Apache Chainsaw]]  --&gt;
| screenshot             = &lt;!-- Missing image removed: [[Image:Apache Chainsaw.png|250px]] --&gt;
| caption                = Demonstration of the Apache Chainsaw GUI
| collapsible            = yes
| developer              = [[Apache Software Foundation]] 
| status                 = Inactive
| latest release version = 2.0
| latest release date    = {{release date|2006|03|02}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| size                   = 
| programming language   = [[Java (programming language)|Java]]
| genre                  = GUI tool to view and analyze log files
| license                = [[Apache License|Apache 2.0 Licence]] 
| website                = http://logging.apache.org/chainsaw/
}}
'''Chainsaw''' is a java-based GUI [[software]] tool to view and analyze [[computer]] log files - specifically logs generated by the [[Log4j]] logging system. Both Log4j and Chainsaw are [[Open source]] projects under [[Apache Software Foundation]]. The latest release is Chainsaw v2.&lt;ref&gt;{{cite web|url=http://logging.apache.org/chainsaw/|title=Apache Chainsaw v2 Download' |accessdate=2008-01-30}}&lt;/ref&gt; Chainsaw can read log files formatted in Log4j's &lt;code&gt;[http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/xml/XMLLayout.html XMLLayout]&lt;/code&gt;, receive events from remote locations, read events from a DB, or work with JDK 1.4 logging events.

==License==
The project is licensed under Apache License, V 2.0. Dependencies may not follow the same licensing.&lt;ref&gt;{{cite web|url=http://logging.apache.org/chainsaw/license.html|title=Apache Chainsaw Project License' |accessdate=2008-01-30}}&lt;/ref&gt;

==Alternatives==
* [http://code.google.com/p/otroslogviewer/ OtrosLogViewer]
* [http://sourceforge.net/projects/lilith/ Lilith]
* [http://glogg.bonnefon.org/ glogg]
* [http://www.legitlog.com/ Legit Log Viewer]
* [http://www.logmx.com/ LogMX]
* [http://insightextensions.codeplex.com/ ReflectInsight Log Viewer]

== References ==
{{reflist}} 

== External links ==
* [http://logging.apache.org/chainsaw/ Apache Logging Services Chainsaw v2 Home]
* [http://logging.apache.org/chainsaw/apidocs/ JavaDoc API reference for Chainsaw v2]

{{apache}}

[[Category:Apache Software Foundation]] 
[[Category:Java programming language]]</text>
      <sha1>dr0tbmuyusgia9kvm94tjau1ut52lhi</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Formatting Objects Processor</title>
    <ns>0</ns>
    <id>632271</id>
    <revision>
      <id>561869382</id>
      <parentid>540850838</parentid>
      <timestamp>2013-06-27T20:08:30Z</timestamp>
      <contributor>
        <ip>131.239.15.66</ip>
      </contributor>
      <text xml:space="preserve" bytes="3955">{{Other uses|FOP (disambiguation)}}
{{Infobox software
| name                   = Apache FOP
| logo                   =
| screenshot             =
| caption                =
| author                 = [[James Tauber]]
| developer              = [[Apache Software Foundation]]
| latest release version = 1.1
| latest release date    = {{Start date and age|2012|10|20}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[XSL-FO]]
| license                = [[Apache License]] 2.0
| website                = http://xmlgraphics.apache.org/fop
}}

'''Formatting Objects Processor''' ('''FOP''', also known as '''Apache FOP''') is a [[Java (programming language)|Java]] application that converts [[XSL Formatting Objects]] (XSL-FO) files to [[Portable Document Format|PDF]] or other printable formats.
FOP was originally developed by [[James Tauber]] who donated it to the [[Apache Software Foundation]] in 1999. It is part of the [[Apache XML Graphics]] project.

FOP is [[open source software]], and is distributed under the [[Apache License]] 2.0.

==Current Status==

The latest version of Apache FOP is 1.1. This is the third stable release after a large redesign effort and implements a large subset of the XSL-FO Version 1.1 W3C Recommendation. In version 0.95, the code had undergone a substantial rewrite compared to 0.20.5 which was the previous stable version. As of release 0.91alpha, FOP became much more compliant to the XSL-FO Recommendation.

==Major Limitations==

Most important elements added in XSL-FO 1.1 (flow maps, table markers, indexes. etc.) are not available &lt;ref&gt;{{cite web|url=http://xmlgraphics.apache.org/fop/compliance-static.html|title=Apache FOP XSL-FO Compliance}}&lt;/ref&gt;

In addition, older XSL-FO 1.0 features are still not supported including automatic table layout, floats and more.

==Input Support==

Apache FOP supports embedding a number of image formats in the XSL-FO (through the &lt;code&gt;&lt;fo:external-graphic&gt;&lt;/code&gt; element). These include:

* [[Scalable Vector Graphics|SVG]]
* [[Portable Network Graphics|PNG]]
* Bitmap [[BMP file format|BMP]]
* [[PostScript]] (as EPS)
* [[JPEG]]
* Some [[TIFF]] formats.

Apache FOP does not implement the &lt;code&gt;&lt;fo:float&gt;&lt;/code&gt; element. External graphics objects are thus limited to being drawn inline or in a block with no wrapped text.

==Output Formats==

Apache FOP supports the following output formats:

* [[PDF]] (best output support), including [[PDF/X]] and [[PDF/A]] with some limitations&lt;ref&gt;{{cite web |url=http://xmlgraphics.apache.org/fop/0.95/pdfx.html |title=FOP 0.95 - PDF/X (ISO 15930) |accessdate=2011-05-22}}&lt;/ref&gt;
* [[ASCII]] text file facsimile
* [[PostScript]]
* Direct printer output ([[Printer Control Language|PCL]])
* [[Advanced Function Presentation|AFP]]
* [[Rich Text Format|RTF]]
* [[Java2D]]/[[Abstract Window Toolkit|AWT]] for display, printing, and page rendering to [[Portable Network Graphics|PNG]] and [[Tagged Image File Format|TIFF]]

In progress:
*[[Maker Interchange Format|MIF]]
*[[Scalable Vector Graphics|SVG]]

==See also==
{{Portal|Free software}}
* [[XSL Formatting Objects]] (XSL-FO)
* [[Extensible Stylesheet Language|XSL]]

==External links==
* [http://xmlgraphics.apache.org/fop/ Apache FOP Project]
* [http://www.ibstaff.net/fmartinez/?p=15 VelFop: Dynamic FOP with Velocity]
* [http://www.data2type.de/software/antillesxml AntillesXML (GUI for FOP and other Processors)] (in German)
* [http://www.ecrion.com/Products/XFDesigner/Overview.aspx Visual XSL-FO Editor/Designer and FOP Cleanup Plug-In]
* [http://xmlgraphics.apache.org/fop/dev/tools.html FOP Development Tools]

==References==
{{Reflist}}

{{Apache}}

[[Category:Apache Software Foundation]]
[[Category:Free system software]]
[[Category:Free software programmed in Java]]
[[Category:Java libraries]]

{{compu-library-stub}}</text>
      <sha1>oolu6kvdsv4ag82yhszy7mzois34rme</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Synapse</title>
    <ns>0</ns>
    <id>17911473</id>
    <revision>
      <id>598956347</id>
      <parentid>543746958</parentid>
      <timestamp>2014-03-10T08:32:17Z</timestamp>
      <contributor>
        <username>Kbrose</username>
        <id>3938795</id>
      </contributor>
      <comment>flag</comment>
      <text xml:space="preserve" bytes="3963">{{expert|reason=reads like an advertisement and not like an informative, educational article}}
{{ Infobox Software
| name                   = Apache Synapse
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest_release_version = 2.1
| latest_release_date    = {{release_date|2012|01|06}}
| latest_preview_version = 
| latest_preview_date    = 
| operating_system       = [[Cross-platform]]
| programming_language   = [[Java (programming language)|Java]]
| genre                  = [[enterprise service bus|Enterprise Service Bus]] 
| license                = [[Apache License]] 2.0
| website                = http://synapse.apache.org
}}
[[Apache Software Foundation|Apache]] '''Synapse''' is a simple, lightweight and extremely high performance open source [[enterprise service bus]] (ESB) and [[mediation engine]]. It began incubation at the [[Apache Software Foundation]] on August 22, 2005,&lt;ref&gt;[http://incubator.apache.org/projects/index.html Apache Incubator]&lt;/ref&gt; and graduated as a sub project of the Apache Web Services project on January 2, 2007. After implementing extensive support for legacy systems integration, it moved out from the Apache Web Services project as a Top Level Project (TLP) of the [[Apache Software Foundation]] on the February 5, 2008.&lt;ref&gt;[http://apache.sys-con.com/read/496070.htm TLP Graduation]&lt;/ref&gt; Apache Synapse is released under the [[Apache License]].

Synapse supports the creation of Proxy Services, which allows users to easily create virtual services on the ESB layer to front existing services. Existing services may be [[SOAP]], POX/REST services over [[HTTP]]/[[HTTPS|S]], as well as SOAP or legacy services over [[JMS]], Apache VFS file systems (e.g. s/ftp, file, zip/tar/gz, webdav, cifs etc.), Mail systems (e.g. pop3, imap, smtp), [[Financial Information eXchange]] (FIX), [[Hessian (web service protocol)|Hessian]], [[Advanced Message Queuing Protocol|AMQP]] etc. The proxy services allows easy switching of transport, interface (WSDL/Schema/Policy), message format (SOAP 1.1, 1.2/POX/REST, Text, Binary/Hessian etc.), QoS (WS-Addressing, WS-Security, WS-Reliable Messaging) and message optimization (MTOM/SwA) etc.

Synapse has implemented a non-blocking [[HTTP]]/[[HTTPS|S]] transport implementation over the Apache HttpComponents/NIO module to handle thousands of concurrent requests using very little resources and threads. This implementation is capable of connection throttling to control the rate at which large messages are read and processed, and thus can handle heavy concurrent loads of large messages using constant memory. 

Synapse also supports clustered deployments, with support for load balancing, throttling and caching over clustered deployments. The integration with an external Registry/Repository allows Synapse to use externally defined resources for mediation, as well as store its configuration into an externally managed Registry/Repository for [[SOA Governance]]. Synapse can be easily extended with custom Java extensions or POJO classes, or via Apache BFS scripting languages such as Javascript, Ruby, Groovy etc. Synapse ships with over 50 samples that can be executed out of the box.

==See also==
*[[Enterprise Service Bus]]
*[[Service-oriented architecture]]
*[[Apache Axis2]]
*[[Web service]]
*[[Apache License]]

==References==
&lt;references /&gt;

== External links ==
*[http://synapse.apache.org Synapse web site]
*[http://synapse.apache.org/Synapse_QuickStart.html Quick Start Guide]
*[http://synapse.apache.org/Synapse_Configuration_Language.html Configuration Language Syntax]
*[http://synapse.apache.org/Synapse_Samples.html Samples Guide]

{{apache}}

[[Category:Apache Software Foundation|Synapse]]
[[Category:Java platform]]
[[Category:Java enterprise platform]]
[[Category:Service-oriented architecture-related products]]
[[Category:Enterprise application integration]]</text>
      <sha1>rppc2pve5ociqt0rnip1g40ls9ttzk4</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Commons</title>
    <ns>0</ns>
    <id>4642733</id>
    <revision>
      <id>571181106</id>
      <parentid>564102711</parentid>
      <timestamp>2013-09-02T05:46:06Z</timestamp>
      <contributor>
        <username>Wickorama</username>
        <id>7705463</id>
      </contributor>
      <comment>[[Category:Software using the Apache license]]</comment>
      <text xml:space="preserve" bytes="3325">{{Citation style|date=February 2011|details=Violates Wikipedia:External links: &quot;Wikipedia articles may include links to web pages outside Wikipedia (external links), but they should not normally be used in the body of an article.&quot;}}

The '''Apache Commons''' is a project of the [[Apache Software Foundation]], formerly under the [[Jakarta Project]]. The purpose of the Commons is to provide reusable, [[open source]] Java software. The Commons is composed of three parts: proper, sandbox, and dormant.

== Commons Proper ==
The Commons Proper is dedicated to creating and maintaining reusable [[Java platform|Java]] components. The Commons Proper is a place for collaboration and sharing, where developers from throughout the Apache community can work together on projects to be shared by Apache projects and Apache users.
Commons developers will make an effort to ensure that their components have minimal dependencies on other [[Library (computer science)|software libraries]], so that these components can be [[Software deployment|deployed]] easily. In addition, Commons components will keep their [[Interface (computer science)|interfaces]] as stable as possible, so that Apache users, as well as other Apache projects, can implement these components without having to worry about changes in the future.&lt;ref&gt;[http://commons.apache.org/ The Apache Commons root page]&lt;/ref&gt; 

In August 2006, there were over thirty projects in the Commons Proper, falling into five general categories.
{|class=&quot;wikitable&quot;
|-
!Component Category	
!Example
|-
!Packages 	
|Codec and Modeler
|-
!Trivial 	
|CLI, Discovery, Lang, and Collections
|-
!Utilities 	
|BeanUtils, Configuration, Logging, DBCP, Pool, and Validator
|-
!Web-related 	
| FileUpload and Net
|-
!XML-related 	
| Betwixt, Digester, Jelly, and  JXPath
|}''Table from'' {{Harv|Goyal|2003}}

== Commons Sandbox ==

The Commons Sandbox is a workspace where Commons contributors collaborate and experiment on projects that have not been included in the Commons Proper. Projects in the Sandbox are championed by Commons members for promotion to the Commons Proper, and groups of developers work to enhance Sandbox projects until they meet the standards for promotion.

The current list of Commons Sandbox projects is available on the Commons Sandbox page.

== Commons Dormant ==
The Commons Dormant is a collection of components that have been declared inactive due to little recent development activity. These components may be used, but must be built yourself. It is best to assume that these components will not be released in the near future.

The current list of Commons Dormant projects is available on the Commons Dormant page.

== See also ==
* [[Google Guava]]

== References ==
{{Reflist}}

{{Citation|last=Goyal|first=Vikram|url=http://www.onjava.com/pub/a/onjava/2003/06/25/commons.html|title=Using the Jakarta Commons, Part I|accessdate=August 13, 2006|year=2003}}

== External links ==
* [http://commons.apache.org/ Apache Commons]
* [http://commons.apache.org/components.html Components page]
* [http://commons.apache.org/sandbox.html Sandbox page]
* [http://commons.apache.org/dormant/index.html Dormant page]

{{Apache}}

&lt;!--Categories--&gt;
[[Category:Java libraries]]
[[Category:Apache Software Foundation|Commons]]
[[Category:Software using the Apache license]]</text>
      <sha1>n7tlv6zmbc3vc8om77k3342udto4qjo</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache HTTP Server</title>
    <ns>0</ns>
    <id>2581</id>
    <revision>
      <id>602337738</id>
      <parentid>602337638</parentid>
      <timestamp>2014-04-01T23:21:12Z</timestamp>
      <contributor>
        <username>Codename Lisa</username>
        <id>16847332</id>
      </contributor>
      <comment>/* top */ Why subst is not working?</comment>
      <text xml:space="preserve" bytes="13543">{{infobox software
| name                   = Apache HTTP Server
| logo                   = [[File:ASF-logo.svg|frameless]]
| screenshot             =
| caption                =
| author                 = Robert McCool
| developer              = [[Apache Software Foundation]]
| released               = {{Start date|1995}}&lt;ref&gt;{{cite web | url=http://httpd.apache.org/ABOUT_APACHE.html | title=About the Apache HTTP Server Project | publisher=[[Apache Software Foundation]] | accessdate=2008-06-25 | archiveurl= http://web.archive.org/web/20080607122013/http://httpd.apache.org/ABOUT_APACHE.html| archivedate= 7 June 2008 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;
| status                 = Active
| programming language   = [[C (programming language)|C]], [[Forth (programming language)|Forth]], [[XML]]&lt;ref&gt;{{cite web|title=Languages |work=
Apache HTTP Server |agency=Ohloh |publisher= Black Duck Software |url=https://www.ohloh.net/p/apache/analyses/latest/languages_summary |accessdate=2 April 2014}}&lt;/ref&gt;
| genre                  = [[Web server]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://httpd.apache.org/}}
}}

The '''Apache HTTP Server''', commonly referred to as '''Apache''' ({{IPAc-en|ə|ˈ|p|æ|tʃ|iː}} {{Respell|ə|PA|chee}}), is a [[web server]] application notable for playing a key role in the initial growth of the [[World Wide Web]].&lt;ref&gt;[http://news.netcraft.com/archives/web_server_survey.html Netcraft Market Share] for Top Servers Across All Domains August 1995 - today (monthly updated)&lt;/ref&gt; Originally based on the [[NCSA HTTPd]] server, development of Apache began in early 1995 after work on the NCSA code stalled. Apache quickly overtook NCSA HTTPd as the dominant HTTP server, and has remained the most popular HTTP server in use since April 1996. In 2009, it became the first web server software to serve more than 100 million websites.&lt;ref name=&quot;100millionsites&quot;&gt;{{cite web |url=http://news.netcraft.com/archives/2009/02/18/february_2009_web_server_survey.html |title=February 2009 Web Server Survey |publisher=[[Netcraft]] |accessdate=2009-03-29| archiveurl= http://web.archive.org/web/20090226092501/http://news.netcraft.com//archives//2009//02//18//february_2009_web_server_survey.html| archivedate= 26 February 2009 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

Apache is developed and maintained by an open community of developers under the auspices of the [[Apache Software Foundation]]. Most commonly used on a [[Unix-like]] system,&lt;ref&gt;[https://secure1.securityspace.com/s_survey/data/man.200907/apacheos.html OS/Linux Distributions using Apache&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; the software is available for a wide variety of [[operating system]]s, including [[Unix]], [[FreeBSD]], [[Linux]], [[Solaris (operating system)|Solaris]], [[Novell NetWare]], [[OS X]], [[Microsoft Windows]], [[OS/2]], [[Transaction Processing Facility|TPF]], [[OpenVMS]] and [[eComStation]]. Released under the [[Apache License]], Apache is [[open-source software]].

{{As of|2013|06}}, Apache was estimated to serve 54.2% of all active [[website]]s and 53.3% of the top servers across all domains.&lt;ref name=&quot;netcraft&quot;&gt;{{cite web|title=June 2013 Web Server Survey|url=http://news.netcraft.com/archives/2013/06/06/june-2013-web-server-survey-3.html|publisher=[[Netcraft]]|accessdate=2013-06-12}}&lt;/ref&gt;&lt;ref&gt;[http://w3techs.com/technologies/overview/web_server/all sage of web servers for websites]&lt;/ref&gt;&lt;ref&gt;[http://www.ntchosting.com/apache-web-server.html Information about the Apache Web Server. (n.d.). Web Hosting Services, VPS Servers and Domain Names by NTC Hosting. Retrieved November 12, 2012]&lt;/ref&gt;&lt;ref&gt;[http://compnetworking.about.com/cs/webserver Mitchell, B. (n.d.). What Is The Apache Web Server?. Networking - Computer and Wireless Networking Basics - Home Networks Tutorials. Retrieved November 12, 2012]{{dead link|date=January 2014}}&lt;/ref&gt;&lt;ref&gt;[http://www.modulehosting.com/apache.html Apache Server Definition. (n.d.). Module for hosting (mod_hosting) for apache 2 servers. Retrieved November 12, 2012]&lt;/ref&gt;

== Name ==

According to the FAQ in the Apache project website, the name Apache was chosen out of respect to the Native American tribe [[Apache]] and its superior skills in warfare and strategy. The name was widely believed to be a pun on A Patchy Server (since it was a set of [[Patch (computing)|software patches]]), but this is erroneous.&lt;ref&gt;{{cite web | url = http://wiki.apache.org/httpd/FAQ#Why_the_name_.22Apache.22.3F | title = Why the name 'Apache'? | work = HTTPd Frequently Asked Questions }}&lt;/ref&gt; Official documentation used to give this very explanation of the name,&lt;ref&gt;{{cite web | url = http://web.archive.org/web/19970415054031/www.apache.org/info.html | title = Information on the Apache HTTP Server Project | date = 1997-04-15 }}&lt;/ref&gt; but in a 2000 interview, [[Brian Behlendorf]], one of the creators of Apache, set the record straight:&lt;ref&gt;[http://www.linux-mag.com/id/472/ Apache Power | Linux Magazine&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

{{ quote
| The name literally came out of the blue. I wish I could say that it was something fantastic, but it was out of the blue. I put it on a page and then a few months later when this project started, I pointed people to this page and said: &quot;Hey, what do you think of that idea?&quot; ... Someone said they liked the name and that it was a really good pun. And I was like, &quot;A pun? What do you mean?&quot; He said, &quot;Well, we're building a server out of a bunch of software patches, right? So it's a patchy Web server.&quot; I went, &quot;Oh, all right.&quot; ... When I thought of the name, no. It just sort of connoted: &quot;Take no prisoners. Be kind of aggressive and kick some ass.&quot;
}}

== Features ==

Apache supports a variety of features, many implemented as [[Compiler|compiled]] [[Modular programming|modules]] which extend the core functionality. These can range from server-side programming language support to authentication schemes. Some common language interfaces support &lt;!-- ATTENTION AUTO-EDITORS, THESE UNDERSCORES ARE INTENTIONAL --&gt;[[mod perl|Perl]], [[mod python|Python]], [[Tcl]], and [[PHP]]. Popular authentication modules include mod_access, mod_auth, mod_digest, and mod_auth_digest, the successor to mod_digest. A sample of other features include [[Secure Sockets Layer]] and [[Transport Layer Security]] support ([[mod_ssl]]), a [[proxy server|proxy]] module ([[mod_proxy]]), a [[URL rewriting|URL rewriter]] (mod_rewrite), custom log files (mod_log_config), and filtering support (mod_include and mod_ext_filter).

Popular compression methods on Apache include the external extension module, mod_gzip&lt;!-- redirects to here --&gt;, implemented to help with reduction of the size (weight) of web pages served over [[HTTP]]. [[ModSecurity]] is an open source intrusion detection and prevention engine for web applications. Apache logs can be analyzed through a web browser using free scripts such as [[AWStats]]/[[W3Perl]] or [[Visitors (program)|Visitors]].

[[Virtual hosting]] allows one Apache installation to serve many different websites. For example, one machine with one Apache installation could simultaneously serve www.example.com, www.example.org, test47.test-server.example.edu, etc.

Apache features configurable error messages, [[Database management system|DBMS]]-based authentication databases, and [[content negotiation]]. It is also supported by several [[graphical user interface]]s (GUIs).

It supports password authentication and [[digital certificate]] authentication. Because the source code is freely available, anyone can adapt the server for specific needs, and there is a large public library of Apache add-ons.&lt;ref&gt;[http://www.webopedia.com/TERM/A/Apache_Web_server.html What is Apache Web server? - A Word Definition From the Webopedia Computer Dictionary. (n.d.). Webopedia: Online Computer Dictionary for Computer and Internet Terms and Definitions. Retrieved November 12, 2012]&lt;/ref&gt;

== Performance ==
[[File:LAMP software bundle.svg|thumb|400px|The [[LAMP (software bundle)|LAMP software bundle]] (here additionally with [[Squid (software)|Squid]]). A high performance and high-availability solution for a hostile environment]]

Although the main design goal of Apache is not to be the &quot;fastest&quot; web server, Apache does have performance similar{{Citation needed|date=March 2014}} to other &quot;high-performance&quot; web servers. Instead of implementing a single architecture, Apache provides a variety of MultiProcessing Modules (MPMs) which allow Apache to run in a process-based, hybrid (process and thread) or event-hybrid mode, to better match the demands of each particular infrastructure. This implies that the choice of correct MPM and the correct configuration is important. Where compromises in performance need to be made, the design of Apache is to reduce latency and increase [[throughput]], relative to simply handling more requests, thus ensuring consistent and reliable processing of requests within reasonable time-frames.

The Apache 2.2 series was considered significantly slower than [[nginx]] for delivering static pages, although remaining significantly faster{{Citation needed|date=March 2014}} for dynamic pages. To address this issue, the Apache version considered by the Apache Foundation as providing high-performance is the multi-threaded version which mixes the use of several processes and several threads per process.&lt;ref&gt;[http://httpd.apache.org/docs/2.2/mod/worker.html Apache MPM worker]&lt;/ref&gt; This architecture, and the way it was implemented in the Apache 2.4 series, provides for performance equivalent or slightly better than event-based webservers, as it claimed by President of the Apache Foundation, [[Jim Jagielski]].&lt;ref&gt;[http://people.apache.org/~jim/presos/ACNA11/Apache_httpd_cloud.pdf Apache httpd 2.4]&lt;/ref&gt; While some independent benchmarks show that it still twice slower than nginx.&lt;ref&gt;[http://blog.zhuzhaoyuan.com/2012/02/apache-24-faster-than-nginx/ Apache 2.4 Faster Than Nginx?]&lt;/ref&gt;&lt;ref&gt;[http://www.eschrade.com/page/performance-of-apache-2-4-with-the-event-mpm-compared-to-nginx/ Performance of Apache 2.4 with the event MPM compared to Nginx]&lt;/ref&gt;

== Licensing ==

The Apache HTTP Server [[codebase]] was relicensed to the [[Apache License|Apache 2.0 License]] (from the previous 1.1 license) in January 2004,&lt;ref&gt;{{cite web|url=https://www.apache.org/licenses/LICENSE-2.0.html|title=Apache License, Version 2.0|publisher=The Apache Software Foundation|date=January 2004|accessdate=2013-05-21}}&lt;/ref&gt; and Apache HTTP Server 1.3.31 and 2.0.49 were the first [[Software release life cycle|releases]] using the new license.&lt;ref&gt;{{cite newsgroup|url=|title=FYI: Apache HTTP Server 2.0.49 Released|last=Burton|first=Richard Antony|newsgroup=alt.apache.configuration|accessdate=2013-05-23}}&lt;/ref&gt;

The [[OpenBSD]] project did not like the change and continued the use of pre-2.0 Apache versions, effectively forking Apache 1.3.x for its purposes.&lt;ref&gt;{{cite mailing list|url=http://marc.info/?l=openbsd-misc&amp;m=107714762916291|title=The new apache license|last=de Raadt|first=Theo|authorlink=Theo de Raadt|mailinglist=openbsd-misc|date=18 February 2004|accessdate=2013-05-21}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.openbsd.org/policy.html|title=Copyright Policy|publisher=OpenBSD|accessdate=2013-05-12}}&lt;/ref&gt; Later it switched to nginx.&lt;ref&gt;[http://www.openbsd.org/faq/upgrade52.html#nginx OpenBSD Upgrade Guide: 5.1 to 5.2]&lt;/ref&gt;&lt;ref&gt;[http://www.openbsd.org/faq/current.html#20140313b OpenBSD Following -current]&lt;/ref&gt;

== Development ==

The Apache HTTP Server Project is a collaborative software development effort aimed at creating a robust, commercial-grade, feature-rich and freely-available source code implementation of an HTTP (Web) server. The project is jointly managed by a group of volunteers located around the world, using the Internet and the Web to communicate, plan, and develop the server and its related documentation. This project is part of the Apache Software Foundation. In addition, hundreds of users have contributed ideas, code, and documentation to the project.&lt;ref&gt;[http://httpd.apache.org/ABOUT_APACHE.html Netcraft. (n.d.). About the Apache HTTP Server Project - The Apache HTTP Server Project. Welcome! - The Apache HTTP Server Project. Retrieved November 12, 2012]&lt;/ref&gt;&lt;ref&gt;[http://www.ohloh.net/p/apache The Apache HTTP Server Open Source Project on Ohloh. (n.d.). Ohloh, the open source network. Retrieved November 12, 2012]&lt;/ref&gt;&lt;ref&gt;[http://docs.fedoraproject.org/en-US/Fedora/13/html/Managing_Confined_Services/chap-Managing_Confined_Services-The_Apache_HTTP_Server.html Chapter 4. The Apache HTTP Server. (n.d.). Fedora Documentation. Retrieved November 12, 2012]&lt;/ref&gt;

== See also ==
{{Portal|Free software}}

* [[ApacheBench]]
* [[Comparison of web server software]]
* [[.htaccess]]
* [[.htpasswd]]
* [[IBM HTTP Server]]
* [[Internet Cache Protocol]]
* [[LAMP (software bundle)]]
* [[List of Apache modules]]
* [[POSSE project]]
* [[Proxy server]]
* [[Reverse proxy]]
* [[suEXEC]]
* [[WAMP (software bundle)]]
* [[Web accelerator]]
* [[XAMPP]]

== References ==

{{reflist|30em}}

== External links ==

* {{official website|https://httpd.apache.org/}}

{{Apache}}
{{Web server software}}
{{Web interfaces}}

[[Category:1995 software]]
[[Category:Apache Software Foundation|HTTP Server]]
[[Category:Cross-platform software]]
[[Category:Free software programmed in C]]
[[Category:Free web server software]]
[[Category:Reverse proxy]]
[[Category:Unix network-related software]]</text>
      <sha1>ioeh02xih9um5qw1fk9qcytwti9t6jk</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Category:Apache Software Foundation projects</title>
    <ns>14</ns>
    <id>1972685</id>
    <revision>
      <id>476300199</id>
      <parentid>367205573</parentid>
      <timestamp>2012-02-11T16:53:33Z</timestamp>
      <contributor>
        <username>MGA73bot</username>
        <id>9332754</id>
      </contributor>
      <minor/>
      <comment>Robot: Adding {{Commons category|Apache Software Foundation}}</comment>
      <text xml:space="preserve" bytes="472">Please note that this listing is incomplete: another one is being maintained at [http://projects.apache.org projects.apache.org]

{{Portal|Free software}}
{{Commons category|Apache Software Foundation}}

[[Category:Free software projects]]
[[Category:Apache Software Foundation]]

[[de:Kategorie:Apache-Projekt]]
[[ko:분류:아파치 소프트웨어 재단 프로젝트]]
[[nl:Categorie:Apache Software Foundation project]]
[[ru:Категория:Проекты Apache]]</text>
      <sha1>69qk0y4x76jcwqq58lt5nnhjqm27i3x</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Log4j</title>
    <ns>0</ns>
    <id>3291868</id>
    <revision>
      <id>601352537</id>
      <parentid>601352401</parentid>
      <timestamp>2014-03-26T13:55:36Z</timestamp>
      <contributor>
        <ip>171.16.208.2</ip>
      </contributor>
      <comment>/* Ports */</comment>
      <text xml:space="preserve" bytes="14112">{{Multiple issues|
{{Refimprove|date=May 2009}}
{{Howto|date=July 2012}}
{{Citation style|date=July 2012}}
}}
{{Infobox software
| name                   = Apache log4j
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 1.2.17
| latest release date    = {{release date|2012|05|06}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Logging Tool]]
| license                = [[Apache License]] 2.0
| website                = http://logging.apache.org/log4j
}}
{{lowercase|title=log4j}}
Apache '''log4j''' is a [[Java platform|Java]]-based [[data logging|logging]] utility. It was originally written by Ceki Gülcü and is now a project of the [[Apache Software Foundation]].  log4j is one of several [[Java logging frameworks]].

Gülcü has since started the [[SLF4J]] and Logback&lt;ref&gt;[http://logback.qos.ch/ Logback website]&lt;/ref&gt; projects, with the intention of offering a successor to log4j.

The log4j team has created a successor to log4j with version number [http://logging.apache.org/log4j/2.x/ 2.0], which is currently in beta release. log4j 2.0 was developed with a focus on the problems of log4j 1.2, 1.3, java.util.logging and logback, and addresses issues which appeared in those frameworks. In addition, log4j 2.0 offers a plugin architecture which makes it more extensible than its predecessor.  log4j 2.0 is not backwards compatible with 1.x versions,&lt;ref&gt;http://logging.apache.org/log4j/2.x/index.html See &quot;News&quot; section&lt;/ref&gt; although an &quot;adapter&quot; is available.

== Log level ==
The following table defines the log levels and messages in log4j, in decreasing order of severity. The left column lists the log level designation in log4j and the right column provides a brief description of each log level.

{| class=&quot;wikitable&quot;
!'''Level'''
!'''Description'''
|-
| '''OFF'''
| The highest possible rank and is intended to turn off logging.
|-
| '''FATAL'''
| Severe errors that cause premature termination. Expect these to be immediately visible on a status console.
|-
| '''ERROR'''
| Other runtime errors or unexpected conditions. Expect these to be immediately visible on a status console.
|-
| '''WARN'''
| Use of deprecated APIs, poor use of API, 'almost' errors, other runtime situations that are undesirable or unexpected, but not necessarily &quot;wrong&quot;. Expect these to be immediately visible on a status console.
|-
| '''INFO'''
| Interesting runtime events (startup/shutdown). Expect these to be immediately visible on a console, so be conservative and keep to a minimum.
|-
| '''DEBUG'''
| Detailed information on the flow through the system. Expect these to be written to logs only.
|-
| '''TRACE'''
| Most detailed information. Expect these to be written to logs only. Since version [http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/Level.html#TRACE 1.2.12].
|}

==Configuration of log4j 1.2==
There are three ways to configure log4j: with a [[.properties|properties file]], with an [[XML]] file and through Java code. Within either you can define three main components: Loggers, Appenders and Layouts. Configuring logging via a file has the advantage of turning logging on or off without modifying the application that uses log4j. The application can be allowed to run with logging off until there's a problem, for example, and then logging can be turned back on simply by modifying the configuration file.

'''Loggers''' are logical log file names. They are the names that are known to the Java application. Each logger is independently configurable as to what level of logging (FATAL, ERROR, etc.) it currently logs. In early versions of log4j, these were called category and priority, but now they're called logger and level, respectively.

The actual outputs are done by '''Appenders'''. There are numerous Appenders available, with descriptive names, such as FileAppender, ConsoleAppender, SocketAppender, SyslogAppender, NTEventLogAppender and even SMTPAppender. Multiple Appenders can be attached to any Logger, so it's possible to log the same information to multiple outputs; for example to a file locally and to a [[Internet socket|socket]] listener on another computer.

Appenders use '''Layouts''' to format log entries. A popular way to format one-line-at-a-time log files is PatternLayout, which uses a pattern string, much like the [[C (programming language)|C]] / [[C++]] function [[printf]]. There are also HTMLLayout and XMLLayout formatters for use when [[HTML]] or XML formats are more convenient, respectively.

To debug a misbehaving configuration use the Java VM property &lt;code&gt;-Dlog4j.debug&lt;/code&gt; which will output to [[Standard streams|standard out]]. To find out where a log4j.properties was loaded from inspect &lt;code&gt;getClass().getResource(&quot;/log4j.properties&quot;)&lt;/code&gt; or &lt;code&gt;getClass().getResource(&quot;/log4j.xml&quot;)&lt;/code&gt;.

There is also an implicit &quot;unconfigured&quot; configuration of log4j, that of a log4j-instrumented Java application which lacks any log4j configuration. This prints to stdout a warning that the program is unconfigured, and the URL to the log4j web site where details on the warning and configuration may be found. As well as printing this warning, an unconfigured log4j application does not print messages at INFO, DEBUG or TRACE levels -and possibly not higher level messages.

===Example for log4j 1.2===
&lt;source lang=&quot;xml&quot; enclose=&quot;div&quot;&gt;
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;!DOCTYPE log4j:configuration PUBLIC &quot;-//LOGGER&quot; &quot;http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/xml/doc-files/log4j.dtd&quot;&gt;
&lt;log4j:configuration&gt;
    &lt;!-- 
         an appender is an output destination, such as the console or a file;
         names of appenders are arbitrarily chosen
    --&gt;
    &lt;appender name=&quot;stdout&quot; class=&quot;org.apache.log4j.ConsoleAppender&quot;&gt;
        &lt;layout class=&quot;org.apache.log4j.PatternLayout&quot;&gt;
            &lt;param name=&quot;ConversionPattern&quot;
                value=&quot;%d{ABSOLUTE} %5p %c{1}:%L - %m%n&quot; /&gt;
        &lt;/layout&gt;
    &lt;/appender&gt;
 
    &lt;!-- 
         loggers of category 'org.springframework' will only log messages of level &quot;info&quot; or higher;
         if you retrieve Loggers by using the class name (e.g. Logger.getLogger(AClass.class))
         and if AClass is part of the org.springframework package, it will belong to this category
    --&gt;
    &lt;logger name=&quot;org.springframework&quot;&gt;
        &lt;level value=&quot;info&quot;/&gt;
    &lt;/logger&gt;

    &lt;!-- 
         everything of spring was set to &quot;info&quot; but for class 
         PropertyEditorRegistrySupport we want &quot;debug&quot; logging 
    --&gt;
    &lt;logger name=&quot;org.springframework.beans.PropertyEditorRegistrySupport&quot;&gt;
        &lt;level value=&quot;debug&quot;/&gt;
    &lt;/logger&gt;
 
    &lt;logger name=&quot;org.acegisecurity&quot;&gt;
        &lt;level value=&quot;info&quot;/&gt;
    &lt;/logger&gt;
    
    
    &lt;root&gt;
        &lt;!-- 
            all log messages of level &quot;debug&quot; or higher will be logged, unless defined otherwise 
            all log messages will be logged to the appender &quot;stdout&quot;, unless defined otherwise 
        --&gt;
        &lt;level value=&quot;debug&quot; /&gt;
        &lt;appender-ref ref=&quot;stdout&quot; /&gt;
    &lt;/root&gt;
&lt;/log4j:configuration&gt;
&lt;/source&gt;

== TTCC ==

TTCC is a message format used by log4j.&lt;ref&gt;[http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/TTCCLayout.html TTCC documentation]&lt;/ref&gt;  TTCC is an acronym for ''Time Thread Category Component''.  It uses the following pattern:

  %r [%t] %-5p %c %x - %m%n

Where
{| class=&quot;wikitable&quot;
|-
! Mnemonic
! Description
|-
| %r
| Used to output the number of milliseconds elapsed from the construction of the layout until the creation of the logging event.
|-
| %t
| Used to output the name of the thread that generated the logging event.
|-
| %p
| Used to output the priority of the logging event.
|-
| %c
| Used to output the category of the logging event.
|-
| %x
| Used to output the [http://logging.apache.org/log4j/docs/api/org/apache/log4j/NDC.html NDC] (nested diagnostic context) associated with the thread that generated the logging event.
|-
| %X{key}
| Used to output the [http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/MDC.html MDC] (mapped diagnostic context) associated with the thread that generated the logging event for specified key
|-
| %m
| Used to output the application supplied message associated with the logging event.
|-
| %n
| Used to output the platform-specific [[newline]] character or characters.
|}

'''Example output'''&lt;br /&gt;
467 [main] INFO  org.apache.log4j.examples.Sort - Exiting main method.

==Ports==
* ''log4c'' - A port for C. ''Log4C'' is a C-based [[computer data logging|logging]] library, released on [[SourceForge]] under the [[LGPL]] license.  For various [[Unix]] operating systems the [[autoconf]] and [[automake]] files are provided. On [[Microsoft Windows|Windows]] a [[Makefile]] is provided for use with [[MSVC]]. [[Software developer|Developers]] may also choose to use their own make system to compile the source, depending on their build engineering requirements. An instance of the ''log4c'' library may be configured via three methods: using [[environment variable]]s, programmatically, or via [[XML]] configuration file. Last version is 1.2.1, released in 2007, and the project is no longer actively developed.&lt;ref&gt;[http://sourceforge.net/projects/log4c/ SourceForge Log4C homepage]&lt;/ref&gt;
* ''log4js'' - A port for [[JavaScript]]. Log4js is available under the licence of [[Apache Software Foundation]]. One special feature of Log4js is the ability to log the events of the browser remotely on the server. Using [[Ajax (programming)|Ajax]] it is possible to send the logging events in several formats ([[XML]], [[JSON]], plain [[ASCII]], etc.) to the server to be evaluated there. The following appenders are implemented for ''log4js'': AjaxAppender, ConsoleAppender, FileAppender, JSConsoleAppender, MetatagAppender, and WindowsEventsAppender. The following Layout classes are provided: BasicLayout, HtmlLayout, JSONLayout, and XMLLayout. Last version is 1.1, released in 2008.&lt;ref&gt;[http://log4js.berlios.de Log4js homepage]&lt;/ref&gt;
* [http://log4javascript.org/ log4javascript] - Another port for JavaScript. log4javascript is a JavaScript logging framework based on the ''log4j''. The latest version is 1.4.6, released in March 2013.&lt;ref&gt;[http://log4javascript.org/ Log4javascript official page]&lt;/ref&gt;
* [http://logging.apache.org/log4net/ Apache Log4net] - A port to the Microsoft [[.NET Framework]]. The initial work was done by [[Neoworks]] and was donated to the [[Apache Software Foundation]] in February 2004. The framework is similar to the original log4j while taking advantage of new features in the .NET runtime. Provides Nested Diagnostic Context (NDC) and Mapped Diagnostic Context (MDC). Last version is 1.2.13, released in 2013.&lt;ref&gt;[http://logging.apache.org/log4net/ Log4net project home page]&lt;/ref&gt;
* [http://mschilli.github.com/log4perl log4perl] -  A [[Perl]] port of the widely popular log4j logging package.&lt;ref&gt;[http://mschilli.github.com/log4perl Github Log4perl homepage]&lt;/ref&gt;
* [http://log4r.rubyforge.org log4r] - A &quot;port&quot; for [[Ruby (programming language)|Ruby]]. Log4r was inspired by and provides much of the features of the Apache Log4j project, but is not a direct implementation or clone. Aside from superficial similarities, the projects are not related in any way and the code base is completely distinct. Log4r was developed without even looking at the Apache Log4j code.&lt;ref&gt;[http://log4r.rubyforge.org Rubyforge Log4jr homepage]&lt;/ref&gt;
* [https://github.com/tmuth/Logger---A-PL-SQL-Logging-Utility PL-SQL-Logging-Utility] is an adaptation of log4j in PL/SQL.
* [http://angoca.github.io/log4db2/ Log4db2] is a logging utility for DB2 for LUW that uses SQL instructions with SQL PL code.

==Apache Log4j 2==
Apache Log4j 2 is the successor of Log4j 1. Currently the library is available as Release Candidate, but a stable release can be expected for 2014. The framework was rewritten from scratch and has been inspired by existing logging solutions, including Log4j 1 and JUL. The main differences&lt;ref&gt;The new log4j 2.0: http://www.grobmeier.de/the-new-log4j-2-0-05122012.html&lt;/ref&gt; to Log4j 1 are:
*Improved configuration syntax 
*Support for xml and json configuration
*Improved Filters
*Property support
*Markers
*Improved speed
*Modularity: Log4j 2 supports a plugin system
*Improved reliability. Messages are not lost while reconfiguring the framework like in Log4j 1 or Logback
*Automatic reloading of configuration
One of the most recognized features of Log4j 2 is the performance of the &quot;Asynchronous Loggers&quot;.&lt;ref&gt;Asynchronous Logger Performance: http://logging.apache.org/log4j/2.x/manual/async.html#Performance&lt;/ref&gt; Log4j 2 makes use of the [http://lmax-exchange.github.io/disruptor/ LMAX Disruptor]. The library reduces the need for kernel logging and increases the logging performance by the factor 12. For example, in the same environment Log4j 2 can write more than 18,000,000 messages per second, whereas other frameworks like Logback and Log4j 1 just write &lt; 2,000,000 messages per second.

Log4j 2 provides support for SLF4J, Commons Logging, Apache Flume and Log4j 1.

== See also ==
{{Portal|Free software|Java}}
*[[Chainsaw (log file viewer)]]

==References==
{{Reflist}}

==Further reading==
{{Refbegin}}
*{{Citation
| first1     = Ceki
| last1      = Gülcü
| date       = February 2010
| title      = The Complete Log4j Manual
| edition    = 2nd
| publisher  = QOS.ch
| pages      = 204
| isbn       = 978-2-9700369-0-6
}}
*{{Citation
| first1     = Samudra
| last1      = Gupta
| date       = June 22, 2005
| title      = Pro Apache Log4j
| edition    = 2nd
| publisher  = [[Apress]]
| pages      = 224
| isbn       = 978-1-59059-499-5
}}
{{Refend}}

==External links==
*[http://logging.apache.org/log4j/ Official log4j Homepage]

{{apache}}

[[Category:Apache Software Foundation]]
[[Category:Free software programmed in Java]]
[[Category:Log file formats]]</text>
      <sha1>cm2nhns4r98rwha5mb6aywq9yas22f5</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>MyFaces Trinidad</title>
    <ns>0</ns>
    <id>23229103</id>
    <revision>
      <id>580297831</id>
      <parentid>566367160</parentid>
      <timestamp>2013-11-05T12:57:49Z</timestamp>
      <contributor>
        <username>Lmarinho</username>
        <id>2081806</id>
      </contributor>
      <text xml:space="preserve" bytes="1990">{{Infobox Software
| name                   = Apache MyFaces Trinidad
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version = 1.2.11
| latest release date    = 
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[web framework|Web Framework]]|
| license                = 
| website                = http://myfaces.apache.org/trinidad/
}}
Apache '''MyFaces Trinidad''' is a [[JavaServer Faces|JSF]] framework including a large, enterprise quality component library, supporting critical features such as accessibility (e.g. Section 508), right-to-left languages, etc. It also includes a set of framework features, including :

* Partial-page rendering support for the entire component set
* Integrated client-side validation
* A dialog framework
* pageFlowScope, for communicating between pages&lt;ref name=&quot;Apache MyFaces Trinidad Project Page&quot;&gt;http://myfaces.apache.org/trinidad/&lt;/ref&gt;

Trinidad is a subproject of [[Apache MyFaces]] project and was donated by [[Oracle Corporation|Oracle]], where it was known as [[Oracle Application Development Framework|ADF]] Faces. It was renamed Trinidad after a long voting process. Trinidad is more than just a component library because it also contains a lot of goodies which solve common development challenges.&lt;ref&gt;{{cite book | last = Wadia, Marinschek, Saleh, Byrne | first = | authorlink = | coauthors = | title = The Definitive Guide to Apache MyFaces and Facelets | publisher = Apress | date = 2008 | location = New York | pages = 111 | url = | doi = | id = | isbn = 9781590597378}}&lt;/ref&gt;

==References==
&lt;references/&gt;

{{apache}}

[[Category:JavaServer Faces|MyFaces]]
[[Category:Apache Software Foundation|MyFaces]]
[[Category:Java enterprise platform|MyFaces]]


{{web-software-stub}}</text>
      <sha1>0ozduf0w2vo81kv98eifdz1qps0evga</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>XMLBeans</title>
    <ns>0</ns>
    <id>3765547</id>
    <revision>
      <id>591486201</id>
      <parentid>577245218</parentid>
      <timestamp>2014-01-20T00:02:15Z</timestamp>
      <contributor>
        <ip>203.0.215.4</ip>
      </contributor>
      <comment>Spelling correction</comment>
      <text xml:space="preserve" bytes="9460">{{Infobox software
| name                   = Apache XMLBeans
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 2.6.0
| latest release date    = {{release date|2012|08|14}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[XML data binding|XML binding]]
| license                = [[Apache License]] 2.0
| website                = http://xmlbeans.apache.org
}}
'''XMLBeans''' is a [[Java (programming language)|Java]]-to-[[XML data binding|XML binding]] [[Software framework|framework]] which is part of the [[Apache Software Foundation]] [[XML]] project.

==Description==

XMLBeans is a tool that allows access to the full power of XML in a Java friendly way. The idea is to take advantage of the richness and features of XML and [[XML Schema (W3C)|XML Schema]] and have these features mapped as naturally as possible to the equivalent Java language and typing constructs. XMLBeans uses XML Schema to compile Java interfaces and classes that  can then be used to access and modify XML instance data. Using XMLBeans is similar to using any other Java interface/class: with methods like getFoo or setFoo, just as when working with Java. While a major use of XMLBeans is to access XML instance data with strongly typed Java classes there are also APIs that allow access to the full [[XML Information Set|XML infoset]] (XMLBeans keeps XML Infoset fidelity) as well as to allow reflection into the XML schema itself through an XML Schema Object model.

===Characteristics of XMLBeans===

# Large [[XML Schema (W3C)|XML Schema]] support.
# Large [[XML Information Set|XML Infoset]] support.

Large XML Schema support: XMLBeans fully supports XML Schema and the corresponding java classes provide constructs for all of the major functionality of XML Schema. This is critical since often one has no control over the features of XML Schema needed to work with in Java. Also, XML Schema oriented applications can take full advantage of the power of XML Schema and not have to restrict themselves to a subset.

Large XML Infoset support: When unmarshalling an XML instance the full XML infoset is kept and is available to the developer. This is critical because that subset of XML is not easily represented in Java. For example, order of the elements or comments might be needed in a particular application.

===Objective===

A major objective of XMLBeans has been its applicability in all non-streaming (in memory) XML programming situations. The developer should be able to compile their XML Schema into a set of Java classes and know that they will be able to:

# use XMLBeans for all of the schemas they encounter.
# access the XML at whatever level is necessary without other tools.

=== APIs ===

To accomplish the above objectives, XMLBeans provides three major APIs:

* XmlObject
* XmlCursor
* SchemaType

XmlObject: The java classes that are generated from an XML Schema are all derived from XmlObject.  These provide strongly typed getters and setters for each of the elements within the defined XML.  Complex types are in turn XmlObjects.  For example getCustomer might return a CustomerType (which is an XmlObject).  Simple types turn into simple getters and setters with the correct java type. For example getName might return a String.

XmlCursor: From any XmlObject the developer can get an XmlCursor. This provides efficient, low level access to the XML Infoset. A cursor represents a position in the XML instance. The cursor can be moved around the XML instance at any level of granularity needed from individual characters to Tokens.

SchemaType: XMLBeans provides a full XML Schema object model that can be used to reflect on the underlying schema meta information. For example, the developer might generate a sample XML instance for an XML schema or perhaps find the enumerations for an element so that they may be displayed.

== Example ==
An example of a simple XML Schema Definition to describe a country is given below.
&lt;source lang=&quot;xml&quot;&gt;
 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
 &lt;xs:schema targetNamespace=&quot;http://www.openuri.org/domain/country/v1&quot;
            xmlns:tns=&quot;http://www.openuri.org/domain/country/v1&quot;
            xmlns:xs=&quot;http://www.w3.org/2001/XMLSchema&quot;
            elementFormDefault=&quot;qualified&quot;
            attributeFormDefault=&quot;unqualified&quot;
            version=&quot;1.0&quot;&gt;
   &lt;xs:element name=&quot;Country&quot; type=&quot;tns:Country&quot;/&gt;
   &lt;xs:complexType name=&quot;Country&quot;&gt;
     &lt;xs:sequence&gt;
       &lt;xs:element name=&quot;Name&quot; type=&quot;xs:string&quot;/&gt;
       &lt;xs:element name=&quot;Population&quot; type=&quot;xs:int&quot;/&gt;
       &lt;xs:element name=&quot;Iso&quot; type=&quot;tns:Iso&quot;/&gt;
     &lt;/xs:sequence&gt;
   &lt;/xs:complexType&gt;
   &lt;xs:complexType name=&quot;Iso&quot;&gt;
     &lt;xs:annotation&gt;&lt;xs:documentation&gt;ISO 3166&lt;/xs:documentation&gt;&lt;/xs:annotation&gt;
     &lt;xs:sequence&gt;
       &lt;xs:element name=&quot;Alpha2&quot; type=&quot;tns:IsoAlpha2&quot;/&gt;
       &lt;xs:element name=&quot;Alpha3&quot; type=&quot;tns:IsoAlpha3&quot;/&gt;
       &lt;xs:element name=&quot;CountryCode&quot; type=&quot;tns:IsoCountryCode&quot;/&gt;
     &lt;/xs:sequence&gt;
   &lt;/xs:complexType&gt;
   &lt;xs:simpleType name=&quot;IsoCountryCode&quot;&gt;
     &lt;xs:restriction base=&quot;xs:int&quot;&gt;
       &lt;xs:totalDigits value=&quot;3&quot;/&gt;
     &lt;/xs:restriction&gt;
   &lt;/xs:simpleType&gt;
   &lt;xs:simpleType name=&quot;IsoAlpha2&quot;&gt;
     &lt;xs:restriction base=&quot;xs:string&quot;&gt;
       &lt;xs:pattern value=&quot;[A-Z]{2}&quot;/&gt;
       &lt;xs:whiteSpace value=&quot;collapse&quot;/&gt;
     &lt;/xs:restriction&gt;
   &lt;/xs:simpleType&gt;
   &lt;xs:simpleType name=&quot;IsoAlpha3&quot;&gt;
     &lt;xs:restriction base=&quot;xs:string&quot;&gt;
       &lt;xs:pattern value=&quot;[A-Z]{3}&quot;/&gt;
       &lt;xs:whiteSpace value=&quot;collapse&quot;/&gt;
     &lt;/xs:restriction&gt;
   &lt;/xs:simpleType&gt;
 &lt;/xs:schema&gt;
&lt;/source&gt;
When the schema is compiled into XMLBean classes (e.g., using [[Apache Ant|Ant]]), it is very easy to create and manipulate XML data that conforms to the schema definition.  The following Java code is a simple example that illustrates how an XML document can be created and validated.
&lt;source lang=&quot;java&quot;&gt;
 import org.openuri.domain.country.v1.Country;
 import org.openuri.domain.country.v1.Iso;
 public class CountrySample {
   public static void main(String[] args) {
     Country country = Country.Factory.newInstance();
     country.setName(&quot;Denmark&quot;);
     country.setPopulation(5450661);  // from Wikipedia :-)
     // print out country XMLBean as XML
     System.out.println(country.xmlText());
     // check if document is valid - will print &quot;Document is invalid&quot;
     // because required Iso child element in not in the object
     System.out.println (&quot;Document is &quot; + (country.validate() ? &quot;valid&quot; : &quot;invalid&quot;));
     // add child with complex type Iso to make the document valid
     Iso iso = country.addNewIso();
     iso.setAlpha2(&quot;DK&quot;);
     iso.setAlpha3(&quot;DNK&quot;);
     iso.setCountryCode(208);
     // print out country XMLBean as XML
     System.out.println(country.xmlText());
     // check if document is valid - will print &quot;Document is valid&quot;
     System.out.println (&quot;Document is &quot; + (country.validate() ? &quot;valid&quot; : &quot;invalid&quot;));
   }
 }
&lt;/source&gt;

== History ==

[[David Bau]] was the chief designer for the XMLBeans 1.0 project while he was working for [[BEA Systems|BEA]]. XMLBeans started on the grounds of [[XMLMaps]], an XML binding tool included in previous BEA [[WebLogic]] products. XMLBeans was originally developed as part of the proprietary BEA [[WebLogic]] Workshop Framework, but it was obvious from interviews conducted when it was first announced on January 27, 2003, that BEA wanted it to become an open standard.  At that time it was not decided which organization BEA wanted to involve in the standardization effort. Later that year it was donated to the Apache Software Foundation. The original team included Cezar Cristian Andrei and Eric Vasilik, later the team added Cliff Schmidt and Radu Preotiuc-Pietro, Jacob Danner, Kevin Krouse and Wing Yew Poon.

* January 27, 2003: BEA announces XMLBeans as a technology preview.
* September 24, 2003: BEA donates XMLBeans to the Apache Software Foundation where it joins the [[Apache Incubator|Apache Incubator Project]].
* April 23, 2004: XMLBeans Version 1.0.2 is released. This is the first release from the incubator project.
* June 25, 2004: XMLBeans graduated out of the Apache Incubator Project to become top level project.
* June 30, 2005: XMLBeans Version 2.0 is released.
* November 16, 2005: XMLBeans Version 2.1 is released.
* June 23, 2006: XMLBeans Version 2.2 is released.
* June 1, 2007: XMLBeans Version 2.3 is released.
* July 8, 2008: XMLBeans Version 2.4 is released.
* December 14, 2009: XMLBeans Version 2.5 is released.
* August 14, 2012: XMLBeans Version 2.6 is released.

== See also ==

* [[Java Architecture for XML Binding]] (JAXB)
* [[xmlbeansxx]] &amp;mdash; [[XML Data Binding]] code generator for [[C++]]

== External links ==
* {{Official website|http://xmlbeans.apache.org/}}
* [http://xmlbeans.apache.org/resources/index.html XMLBeans resources]
* [http://xml.apache.org/ The Apache XML project]
* [http://xmlbeans.apache.org/docs/2.0.0/guide/antXmlbean.html XMLBean Ant task]
* [http://xmlbeans.googlepages.com/ XML Binding Resources - xbeans of popular schemas]
{{apache}}

{{DEFAULTSORT:Xmlbeans}}
[[Category:Java platform]]
[[Category:Java development tools]]
[[Category:XML parsers]]
[[Category:Apache Software Foundation]]</text>
      <sha1>legr6zv5hdri8tb0kjja4k6r03sb23o</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Wink</title>
    <ns>0</ns>
    <id>23973422</id>
    <revision>
      <id>577238533</id>
      <parentid>552966373</parentid>
      <timestamp>2013-10-15T06:10:56Z</timestamp>
      <contributor>
        <ip>2.91.109.180</ip>
      </contributor>
      <text xml:space="preserve" bytes="3607">{{ Infobox Software
| name                   = Apache Wink
| logo                   = [[Image:Apache Wink Logo.png|Apache Wink Logo]]
| screenshot             = 
| caption                = 
| collapsible            = 
| developer              = [[Apache Software Foundation]]
| latest release version = 1.4.0
| latest release date    = {{Release date|2013|09|15}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[JAX-RS]]&lt;br /&gt;[[Representational State Transfer|RESTful Web services]]
| size                   = 11.4 MB (archived)
| license                = [[Apache License]] 2.0
| website                = {{url|http://wink.apache.org/}}
}}

'''Apache Wink''' is an [[open source]] framework that enables development and consumption of [[Representational State Transfer|REST]] style web services.

== History ==
The project was initiated in 2007 as an [[HP]] internal REST SDK called Symphony. At the beginning of 2009, [[HP]] and [[IBM]] joined forces to deliver the Apache Wink project,&lt;ref&gt;[http://wiki.apache.org/incubator/WinkProposal Apache Wink Proposal], retrieved August 14, 2009&lt;/ref&gt; [[HP]] contributed the SDK and [[IBM]] contributed a lot of integration tests.

The Apache Wink is currently part of the [[Apache Incubator]] project.

==Features==
Apache Wink is composed of a Server module and a Client module. The Server module is an implementation of the [[JAX-RS]] (JSR-311) specification along with additional features to facilitate the development of RESTful Web services. The Client module is a Java-based framework that provides functionality for communicating with [[RESTful]] Web services.

* JAX-RS 1.0 compliant &lt;ref&gt;[http://incubator.apache.org/wink/0.1/Apache_Wink_User_Guide.htm Apache Wink User Guide], retrieved August 14, 2009&lt;/ref&gt;
* Has support for [[Atom (standard)|Atom Syndication Format]], [http://tools.ietf.org/html/rfc4287 Atom Publishing Protocol] and [[RSS]].
* Has support for [[JSON]], [[Comma-separated values|CSV]] and [[OpenSearch]].

== Releases ==
{| class=&quot;wikitable sortable&quot;
! Release
! Date
! Notes
|-
|pre 0.1
|2009-05-27
|Wink enters incubation.
|-
|pre 0.1
|2009-07-01
|Wink is JAX-RS 1.0 compliant
|-
|0.1
|2009-08-26 
|Wink 0.1 released.
|-
|1.1
|2010-05-14
|Wink 1.1 released.
|-
|1.1.1
|2010-07-07
|Wink 1.1.1 released.
|-
|1.1.2
|2010-11-15
|Wink 1.1.2 released.
|-
|1.1.3
|2011-05-09
|Wink-1.1.3 released.
|-
|1.2.0
|2012-05-21
|Wink-1.2.0 released.
|-
|1.3.0
|2013-04-12
|Wink-1.3.0 released.
|-
|1.4.0
|2013-09-15
|Wink-1.4.0 released.
|}

==See also==
* [[Apache CXF]], a web services framework with JAX-RS support.

==References==
{{Reflist}}

==External links==
* {{official website|http://incubator.apache.org/wink/}}
*[http://cwiki.apache.org/confluence/display/WINK/Index Apache Wink Wiki]
*[http://www.ibm.com/developerworks/web/library/wa-apachewink1/index.html IBM DeveloperWorks Apache Wink Article]
*[http://www.ibm.com/developerworks/web/library/wa-apachewink1/ RESTful Web services with Apache Wink, Part 1: Build an Apache Wink REST service]
*[http://www.ibm.com/developerworks/web/library/wa-apachewink2/ RESTful Web services with Apache Wink, Part 2: Advanced topics in Apache Wink REST development]
*[http://www.ibm.com/developerworks/web/library/wa-apachewink3/ RESTful Web services with Apache Wink, Part 3: Apache Wink and the REST]
{{Java API for RESTful Web Services}}
{{Apache}}

[[Category:Apache Software Foundation|Wink]]
[[Category:Java libraries]]


{{web-software-stub}}</text>
      <sha1>88hp4uwlo6bw21f9dqrtiimxzsf7t3r</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache License</title>
    <ns>0</ns>
    <id>145908</id>
    <revision>
      <id>600118126</id>
      <parentid>598182165</parentid>
      <timestamp>2014-03-18T05:53:30Z</timestamp>
      <contributor>
        <ip>39.113.36.19</ip>
      </contributor>
      <comment>/* Version history */</comment>
      <text xml:space="preserve" bytes="8419">{{Infobox software license
| name            = Apache License
| image           = [[Image:ASF-logo.svg|160px]]
| caption         = The Apache logo
| author          = [[Apache Software Foundation]]
| version         = 2.0
| copyright       = Apache Software Foundation
| date            = January 2004
| OSI approved    = Yes&lt;ref name=&quot;osi&quot;&gt;{{cite web
 | title = OSI-approved licenses by name
 | url = http://www.opensource.org/licenses/alphabetical
 | publisher = [[Open Source Initiative]]
 | accessdate =31 March 2011
| archiveurl= https://web.archive.org/web/20110428141712/http://opensource.org/licenses/alphabetical| archivedate= 28 April 2011 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;
| Debian approved = Yes&lt;ref name=&quot;dfsglist&quot;&gt;{{cite web
 |url=http://wiki.debian.org/DFSGLicenses#The_Apache_Software_License_.28ASL.29
 |title=The Apache Software License (ASL)
 |work=The Big DFSG-compatible Licenses
 |publisher=[[Debian Project]]
 |accessdate=6 July 2009
}}&lt;/ref&gt;
| Free Software   = Yes&lt;ref name=&quot;fsflist&quot;&gt;{{cite web
 |url=http://www.gnu.org/licenses/license-list.html#apache2
 |title=Apache License, Version 2.0
 |work=Various Licenses and Comments about Them
 |publisher=[[Free Software Foundation]]
 |accessdate=6 July 2009
| archiveurl= https://web.archive.org/web/20090716201618/http://www.gnu.org/licenses/license-list.html| archivedate= 16 July 2009 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;
| copyfree = No&lt;ref&gt;{{cite web | url=http://copyfree.org/rejected/ | title=Copyfree Licenses | accessdate=16 November 2011}}&lt;/ref&gt;
| GPL compatible  = Yes – version 2.0 is compatible with [[GPL v3]],&lt;ref name=&quot;fsflist&quot; /&gt; but 1.0 &amp; 1.1 are incompatible.&lt;ref&gt;{{cite web | url=https://www.gnu.org/licenses/license-list.html | title=GNU License List | accessdate=1 October 2013}}&lt;/ref&gt;
| copyleft        = No
| linking         = Yes
}}

The '''Apache License''' ({{IPAc-en|ə|ˈ|p|æ|tʃ|i}}) is a [[free software license]] written by the [[Apache Software Foundation]] (ASF). The Apache License requires preservation of the [[copyright]] notice and [[disclaimer]]. Like other [[free software license]]s, the license allows the user of the software the freedom to use the software for any purpose, to distribute it, to modify it, and to distribute modified versions of the software, under the terms of the license, without concern for [[royalties]].

The ASF and its projects release the software they produce under the Apache License. Some non-ASF software is also licensed using the license. In October 2012, 8708 projects located at [[SourceForge.net]] were available under the terms of the Apache License.&lt;ref&gt;{{cite web|url=http://sourceforge.net/search/?&amp;fq%5B%5D=trove%3A401|title=Projects at SourceForge under Apache License|accessdate=28 October 2012}}&lt;/ref&gt;  In a blog post from May 2008, [[Google]] mentioned that over 25% of the nearly 100,000 projects then hosted on [[Google Code]] were using the Apache License,&lt;ref&gt;{{cite web|url=http://google-opensource.blogspot.com/2008/05/standing-against-license-proliferation.html|title=Standing Against License Proliferation|accessdate=24 October 2009}}&lt;/ref&gt; including the [[Android operating system]].&lt;ref&gt;[http://source.android.com/source/licenses.html Android Open Source licenses]&lt;/ref&gt;

==Version history==

The '''Apache License 1.0''' was the original Apache License which applies only to older versions of Apache packages (such as version 1.2 of the Web server).

The '''Apache License 1.1''' was approved by the ASF in 2000: ''The primary change from the 1.0 license is in the 'advertising clause' (section 3 of the 1.0 license); derived products are no longer required to include attribution in their advertising materials, but only in their documentation.''&lt;ref name='Apache licenses'&gt;{{cite web|url=http://www.apache.org/licenses/ |title=Licenses – The Apache Software Foundation |accessdate=7 July 2007 | archiveurl= https://web.archive.org/web/20070701222753/http://www.apache.org/licenses/| archivedate= 1 July 2007 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

The ASF adopted the '''Apache License 2.0''' in January 2004. The stated goals of the license included ''making the license easier for non-ASF projects to use, improving [[License compatibility|compatibility]] with [[GNU General Public License|GPL]]-based software, allowing the license to be included by reference instead of listed in every file, clarifying the license on contributions, and requiring a patent license on contributions that necessarily infringe a contributor's own patents''.&lt;ref name='Apache licenses'/&gt;
Apache 2 License

==Licensing conditions==

The Apache License is widely, but not universally{{by whom|date=March 2014}}, considered [[Permissive free software licence|permissive]] in that it does not require a [[derivative work]] of the software, or modifications to the original, to be distributed using the same license (unlike [[copyleft]] licenses – see [[comparison of free software licenses|comparison]]).  It still requires application of the same license to all unmodified parts and, in every licensed file, any original copyright, patent, trademark, and attribution notices in redistributed code must be preserved (excluding notices that do not pertain to any part of the derivative works); and, in every licensed file changed, a notification must be added stating that changes have been made to that file.

If a NOTICE text file is included as part of the distribution of the original work, then derivative works must include a readable copy of these notices within a NOTICE text file distributed as part of the derivative works, within the source form or documentation, or within a display generated by the derivative works (wherever such third-party notices normally appear).

The contents of the NOTICE file do not modify the license, as they are for informational purposes only, and adding more attribution notices as addenda to the NOTICE text is permissible, provided that these notices cannot be understood as modifying the license. Modifications may have appropriate copyright notices, and may provide different license terms for the modifications.

Unless explicitly stated otherwise, any contributions submitted by a licensee to a licensor will be under the terms of the license without any terms and conditions, but this does not preclude any separate agreements with the licensor regarding these contributions.

==GPL compatibility==
The Apache Software Foundation and the [[Free Software Foundation]] both agree that the Apache License 2.0 is a [[free software license]], compatible with version 3 of the [[GNU General Public License]] (GPL),&lt;ref name=gnulist&gt;{{cite web|url=http://www.gnu.org/licenses/license-list.html#apache2|title=Various Licenses and Comments about Them|date=14 January 2008|publisher=Free Software Foundation|accessdate=30 January 2008| archiveurl= https://web.archive.org/web/20080118133219/http://www.gnu.org/licenses/license-list.html#DocumentationLicenses| archivedate= 18 January 2008 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; meaning that code under GPL version 3 and Apache License 2.0 can be combined, as long as the resulting software is licensed under the GPL version 3.&lt;ref&gt;{{cite web|url=http://www.apache.org/licenses/GPL-compatibility.html|title=Apache License v2.0 and GPL Compatibility|author=Apache Software Foundation|accessdate=30 January 2008| archiveurl= https://web.archive.org/web/20080115045635/http://www.apache.org/licenses/GPL-compatibility.html| archivedate= 15 January 2008 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

The Free Software Foundation considers all versions of the Apache License ({{as of|2013|lc=on}}) to be [[License compatibility|incompatible]] with the previous GPL versions 1 and 2&lt;ref&gt;{{cite web|url=http://www.fsf.org/licensing/licenses|title=Licenses| date=2013-02-28| author=Free Software Foundation| archiveurl= https://web.archive.org/web/20130305182742/http://www.gnu.org/licenses/license-list.html| archivedate= 2013-03-05&lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; and, furthermore, considers Apache License versions before v2.0 incompatible with GPLv3.

==See also==
{{portal|Free software}}
* [[Free software license]]

==References==
{{reflist}}

==External links==
* {{official website|http://www.apache.org/licenses|Apache Licenses}}

{{apache}}
{{FOSS}}

{{use dmy dates|date=January 2012}}

[[Category:Apache Software Foundation]]
[[Category:Free and open-source software licenses]]</text>
      <sha1>0jzd1w21g6gow8znt17udh9vkkz9h6n</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>OJB</title>
    <ns>0</ns>
    <id>17718315</id>
    <revision>
      <id>572453875</id>
      <parentid>572453834</parentid>
      <timestamp>2013-09-11T07:00:21Z</timestamp>
      <contributor>
        <username>StAnselm</username>
        <id>4007668</id>
      </contributor>
      <comment>added hatnote</comment>
      <text xml:space="preserve" bytes="2736">{{Infobox Software
| name                   = Apache ObJectRelationalBridge
| logo                   =
| screenshot             =
| caption                =
| collapsible            = yes
| developer              = [[Apache Software Foundation]]
| latest release version = 1.0.55
| latest release date    = {{release date|mf=yes|2005|12|31}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Object-relational mapping]]
| license                = [[Apache License]] 2.0
| website                = http://db.apache.org/ojb/
}}
{{other uses|Organization of the Jews in Bulgaria}}
'''Apache ObJectRelationalBridge''' (OJB) is an Object/Relational mapping tool that allows transparent persistence for [[Java (programming language)|Java]] Objects against [[relational database]]s. It was released on April 6th, 2005.&lt;ref&gt;[http://www.linuxtoday.com/developer/2005040602826NWSVDV - Linux Today - Internetnews.com: Apache releases Object Relational Bridge]&lt;/ref&gt;

As of January 16, 2011 Apache ObJectRelationalBridge has been retired.&lt;ref&gt;[http://attic.apache.org/projects/ojb.html Apache ObJectRelationalBridge (OJB)], Apache Software Foundation.&lt;/ref&gt;

== Features ==
OJB is  an ''[[open source]]'' project.
It is lightweight and easy to use requiring simply configure two files to implement a persistence layer.
It is easy to integrate into an existing application because it does not generate code.
It allows the use of different patterns of persistence: owner (PersistenceBroker [[API]]), [[Java Data Objects|JDO]] and Object Data Management Group  ([[ODMG]]).

==Functionality==
OJB uses an XML based Object/Relational mapping. The mapping resides in a dynamic [[Metadata|MetaData]] layer, which can be manipulated at runtime through a simple [[Metaobject|Meta-Object-Protocol]] (MOP) to change the behaviour of the [[Persistence (computer science)|persistence]] kernel.

== Configuration ==
At least two files are required to configure OJB:  OJB.properties and repository.xml

== Allocation ==
For mapping a 1-1 relationship, for example, you have two tables: person and account. In this case, a person has an account and vice versa.

==See also==
{{Portal|Free software}}
*[[Apache OpenJPA]]

== References ==
&lt;references /&gt;

==External links==
*[http://db.apache.org/ojb/ Apache ObJectRelationalBridge]
{{Apache}}

{{Software-stub}}
[[Category:Apache Software Foundation|Tomcat]]
[[Category:Apache Software Foundation projects]]
[[Category:Java enterprise platform]]
[[Category:Free software programmed in Java]]
[[Category:Free web server software]]
[[Category:Cross-platform free software]]</text>
      <sha1>het77jjr5zbkzf8iwglgvw0p1dv1xhr</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>List of Apache Software Foundation projects</title>
    <ns>0</ns>
    <id>24299663</id>
    <revision>
      <id>598566125</id>
      <parentid>598565891</parentid>
      <timestamp>2014-03-07T16:22:08Z</timestamp>
      <contributor>
        <username>Gaul</username>
        <id>61711</id>
      </contributor>
      <minor/>
      <comment>add hyperlink</comment>
      <text xml:space="preserve" bytes="9771">This '''list of Apache Software Foundation projects''' contains the [[software development]] projects of the [[Apache Software Foundation]] (ASF).

*[[Apache Abdera|Abdera]]: implementation of the [[Atom (standard)|Atom Syndication Format and Atom Publishing Protocol]]
*[[Apache Accumulo|Accumulo]]: secure implementation of [[BigTable]]
*[[Apache ActiveMQ|ActiveMQ]]: a [[message broker]] supporting different communication protocols and clients, including a full [[Java Message Service]] (JMS) 1.1 client.
*[[Apache Ant|Ant]]: [[Java (programming language)|Java]]-based build tool
*[[Apache Portable Runtime|APR]]: Apache Portable Runtime, a portability library written in [[C (programming language)|C]]
*[http://archiva.apache.org/ Archiva]: The Build Artifact Repository Manager
*[[Apache beehive|Beehive]]: a Java visual object model
*[[Apache Bloodhound|Bloodhound]]: a defect tracker based on [[Trac]]&lt;ref&gt;{{cite web|title=Bloodhound Project Incubation Status|url=http://incubator.apache.org/projects/bloodhound.html|publisher=Apache Software Foundation|accessdate=21 March 2013}}&lt;/ref&gt; 
*[[Apache Camel|Camel]]: a declarative routing and mediation rules engine which implements the [[Enterprise Integration Patterns]] using a Java based domain specific language
*[[Apache CloudStack|CloudStack]]: software to deploy and manage cloud infrastructure 
*[[Apache Cocoon|Cocoon]]: [[XML]] publishing framework
*[[Apache Commons|Commons]]: Reusable Java libraries and utilities too small to merit their own project
*[[Apache Continuum|Continuum]]: a continuous integration server
*[[Apache CXF|CXF]]: web services framework
*DB: [[database]] systems
**[[Cassandra (database)|Cassandra]]: a highly scalable second-generation distributed database
**[[Apache Cayenne|Cayenne]]: a Java [[Object-relational mapping|ORM]] framework
**[[CouchDB]]: [[Document-oriented database]]
**[[Apache Derby|Derby]]: a pure [[Java (programming language)|Java]] [[relational database management system]]
**[[Java Data Objects|JDO]]: Java Data Objects, persistence for [[Java (programming language)|Java]] objects
**[[Apache Torque|Torque]]: an [[Object-relational mapping|ORM]] for [[Java (programming language)|Java]]
*[[Apache Directory Server|Directory]]: a directory server supporting [[Lightweight Directory Access Protocol|LDAP]] and other protocols
*[[Apache Excalibur|Excalibur]]: [[Inversion of Control]] container named [[Fortress]] and related components
*[[Apache Felix|Felix]]: Implementation of the OSGi Release 4 core framework specification
*[[Apache Flex|Flex]]: a cross-platform SDK for developing and deploying rich Internet applications.
*[[Apache Forrest|Forrest]]: documentation framework based upon Cocoon
*[http://flume.apache.org/ Flume]: a large scale log aggregation framework
*[[Apache Geronimo|Geronimo]]: a [[Java Platform, Enterprise Edition|Java EE]] server
*[[Apache Gump|Gump]]: [[digital integration|integration]], [[coupling (computer science)|dependencies]], and [[versioning]] management
*[[Apache Harmony|Harmony]]: a [[Java (programming language)|Java]] implementation
*[[Hadoop]]: [[Java (programming language)|Java]] [[software framework]] that supports data intensive distributed applications
*[[Apache HiveMind|HiveMind]]: Services and configuration microkernel
*[[Apache HTTP Server|HTTP Server]]: a [[Web server]]
*[[iBATIS]]: [[Persistence framework]] which enables mapping [[sql]] queries to [[POJO]]s
*[[Apache Incubator|Incubator]]: for aspiring ASF projects
*[[Apache Jackrabbit|Jackrabbit]]: implementation of the [[content repository API for Java|Java Content Repository API]]
*[[Jakarta Project|Jakarta]]: server side [[Java (programming language)|Java]], including its own set of subprojects
*[[Apache James|James]]: [[Java (programming language)|Java]] [[email]] and [[usenet|news]] server
*[http://jclouds.apache.org jclouds]: an open source multi-cloud toolkit for the Java platform
*[[Apache Kafka|Kafka]]: message broker software
*[http://labs.apache.org/ Labs]: a place for innovation where committees of the foundation can experiment with new ideas
*[[Apache Lenya|Lenya]]: a [[content management system]] (CMS) based on [[Apache Cocoon]]
*[http://logging.apache.org/ Logging]: a cross-language logging services for purposes of application debugging and auditing
**[[log4j]]
*[[Gradle]]: is a project automation tool that builds upon the concepts of Apache Ant and Apache Maven
*[[Lucene]]: text search engine library written entirely in [[Java (programming language)|Java]]
**[[Solr]]: enterprise search server based on the Lucene Java search library
*Lucy: A loose port of the Lucene search engine library, written in C and targeted at dynamic language users.
*[[Apache Mahout|Mahout]]: A machine learning and data mining solution. [http://mahout.apache.org/ Mahout]
*[[Apache Marmotta|Marmotta]]: an open platform for [[Linked Data]].
*[[Apache Maven|Maven]]: [[Java (programming language)|Java]] [[project management]] and comprehension tool
*[[Apache MINA|MINA]]: Multipurpose Infrastructure for Network Application, a framework to develop high performance and high scalability network applications. [http://mina.apache.org/ MINA]
**FtpServer: [[FTP server]] written entirely in Java
*[[mod_perl]]:  module that integrates the [[Perl]] interpreter into Apache server
*[[mod_python]]: module that integrates the [[Python (programming language)|Python]] interpreter into Apache server. Deprecated in favour of [[mod_wsgi]].
*[[MyFaces]]: [[JavaServer Faces]] implementation
*[[OFBiz]]: Open for Business: enterprise automation software
*[[Object Oriented Data Technology (OODT)|OODT]]: Object Oriented Data Technology, a data management framework for capturing and sharing data
*[[Oozie]]: Oozie is a workflow scheduler system to manage Apache Hadoop jobs.
*[[Apache OpenJPA|OpenJPA]]: Java Object Persistence
*[[Apache OpenOffice|OpenOffice]]: Office suite, as of June 2011.&lt;ref name=&quot;OpenH01Jun11&quot;&gt;{{Cite news|url = http://www.h-online.com/open/news/item/OpenOffice-proposed-as-Apache-project-1254201.html|title = OpenOffice proposed as Apache project|accessdate = 1 June 2011|last = Heise Media UK Ltd|authorlink = |date=June 2011| work = The H Open}}&lt;/ref&gt;
*[http://pdfbox.apache.org/ PDFBox]: a Java based PDF library (reading, text extraction, manipulation, viewer)
*[[Apache Pivot|Pivot]]: a platform for building rich internet applications in Java
*[[Apache POI|POI]]: Poor Obfuscation Implementation, a library for reading and writing [[Microsoft Office]] formats
*[http://portals.apache.org/ Portals]: [[web portal]] related software
*[http://santuario.apache.org/ Santuario]: XML Security in Java and C++
*[[ServiceMix]]: [[enterprise service bus]] that supports [[JBI]] and [[OSGi]]
*[[Shale Framework (software)|Shale]]: Web application framework based on JavaServer Faces
*[[SpamAssassin]]: email filter used to identify [[e-mail spam|spam]]
*[[Apache Stanbol|Stanbol]]: extend traditional content management systems with semantic services
*[http://incubator.apache.org/stonehenge/ Stonehenge]: [[Service-oriented architecture|SOA]] interoperability 
*[[Apache Struts|Struts]]: [[Java (programming language)|Java]] web applications framework 
*[[Apache Subversion|Subversion]]: an open source [[Revision control|version control]] (client/server) system
*[[Apache Tapestry|Tapestry]]: Component-based Java web framework
*[http://tika.apache.org/ Tika]: a content analysis toolkit for extracting metadata and text from digital documents of various types, e.g., audio, video, image, office suite, web, mail, and binary 
*[[Tcl]]: Originally &quot;Tool Command Language&quot;, a dynamic GUI scripting language
*[[Apache Tomcat|Tomcat]]: a [[web container]] for serving servlets and JSP
*[[Traffic Server]]: an HTTP/1.1 compliant caching proxy server. [http://trafficserver.apache.org/ Traffic Server]
*[[Apache UIMA|UIMA]]: an unstructured content analytics framework
*[[Apache Velocity|Velocity]]: a Java template creation engine
*[[Apache Wave|Wave]]:  online real-time collaborative editing
*[[Web service]]s: Web service related systems
**[[Apache Axis|Axis]]: an open source, XML based Web service framework
**[[Apache Muse|Muse]]: an implementation of the WS-ResourceFramework ([[Web Services Resource Framework|WSRF]]), WS-BaseNotification ([[WSN]]), and WS-DistributedManagement ([[Web Services Distributed Management|WSDM]]) specifications
**[[Apache Rampart module|Rampart]]: an implementation of the WS-Security standard for the Axis2 Web services engine
**[[Apache Tuscany|Tuscany]]: a [[service component architecture|SCA]] implementation, also providing other [[service-oriented architecture|SOA]] implementations
*[[Wicket framework|Wicket]]: Component-based Java web framework
*[[Xalan]]: XSLT processors in Java and C++
*[[Xerces]]: a validating XML parser
*[[XMLBeans]]: [[XML]]-[[Java (programming language)|Java]] binding tool
*XML Graphics: conversion of [[XML]] formats to graphical output
**[[Batik software|Batik]]: a pure Java library for [[Scalable Vector Graphics|SVG]] content manipulation
**[[Formatting Objects Processor|FOP]]: a Java print formatter driven by XSL formatting objects (XSL-FO); supported output formats include PDF, PS, PCL, AFP, XML (area tree representation), Print, AWT and PNG, and to a lesser extent, RTF and TXT
*[[Apache ZooKeeper|Zookeeper]]: Coordination service for distributed applications

Note: some projects are recognized as subprojects of other ''top level projects'' (TLPs)

==References==
{{reflist}}
{{cite web
|url=http://projects.apache.org/indexes/alpha.html
|title=Alphabetical Index
|publisher=The Apache Software Foundation
|year=2013
|accessdate=2013-03-08
}}

[[Category:Apache Software Foundation| ]]
[[Category:Apache Software Foundation projects]]</text>
      <sha1>c3fixv1l56qe25pbnv5vvku5bnhd2w7</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Software Foundation</title>
    <ns>0</ns>
    <id>1336</id>
    <revision>
      <id>591920698</id>
      <parentid>582872445</parentid>
      <timestamp>2014-01-22T20:25:09Z</timestamp>
      <contributor>
        <username>Hult041956</username>
        <id>5114599</id>
      </contributor>
      <minor/>
      <comment>/* Projects */ short-cut the redirection</comment>
      <text xml:space="preserve" bytes="10299">{{Infobox Non-profit
| Non-profit_name   = Apache Software Foundation
| Non-profit_logo   = [[File:ASF-logo.svg|225px]]
| Non-profit_type   = [[501(c)#501(c)(3)|501(c)(3)]]
| founded_date      = June 1999
| founder           = [[Brian Behlendorf]], [[Ken Coar]], Mark Cox, [[Lars Eilebrecht]], [[Ralf S. Engelschall]], [[Roy T. Fielding]], [[Dean Gaudet]], [[Ben Hyde]], [[Jim Jagielski]], [[Alexei Kosut]], [[Martin Kraemer]], [[Ben Laurie]], [[Doug MacEachern]], [[Aram Mirzadeh]], [[Sameer Parekh]], [[Cliff Skolnick]], [[Marc Slemko]], [[William (Bill) Stoddard]], [[Paul Sutton 2|Paul Sutton]], [[Randy Terbush]], [[Dirk-Willem van Gulik]]
| location          = [[Forest Hill, Maryland]], The USA 
| origins           = 
| key_people        = 
| area_served       = 
| product           =
| focus             = [[Open source software]]
| method            = [[Apache License]]
| revenue           = 
| endowment         = 
| num_volunteers    = 
| num_employees     = 
| num_members       = 
| subsib            = 
| owner             = 
| Non-profit_slogan = 
| homepage          = [http://www.apache.org/ apache.org]
| dissolved         = 
| footnotes         = 
}}
The '''Apache Software Foundation''' {{IPAc-en|ə|ˈ|p|æ|tʃ|i:}} ('''ASF''') is an American non-profit corporation (classified as [[501(c)#501(c)(3)|501(c)(3)]] in the United States) to support Apache software projects, including the [[Apache HTTP Server]]. The ASF was formed from the Apache Group and incorporated in [[Delaware corporation|Delaware]], U.S., in June 1999.&lt;ref name=incorporation&gt;
{{cite web
| first       = Roy T.
| last        = Fielding
| title       = Certificate of Incorporation of the Apache Software Foundation
| url         = http://www.apache.org/foundation/records/certificate.html
| accessdate  = 2009-05-26
| archiveurl= http://web.archive.org/web/20090531160220/http://apache.org/foundation/records/certificate.html| archivedate= 31 May 2009 &lt;!--DASHBot--&gt;| deadurl= no}}
&lt;/ref&gt;&lt;ref name=effDate&gt;{{cite web
| first       = Jim
| last        = Jagielski
| title       = The Apache Software Foundation Board of Directors Meeting Minutes 01 June 1999
| url         = http://www.apache.org/foundation/records/minutes/1999/board_minutes_1999_06_01.txt
| accessdate  = 2009-05-26
}}&lt;/ref&gt;

The Apache Software Foundation is a decentralized community of developers. The software they produce is distributed under the terms of the [[Apache License]] and is therefore [[free and open source software]] (FOSS). The Apache projects are characterized by a collaborative, consensus-based development process and an open and pragmatic software license. Each project is managed by a self-selected team of technical experts who are active contributors to the project. The ASF is a [[meritocracy]], implying that membership of the foundation is granted only to volunteers who have actively contributed to Apache projects. The ASF is considered a second generation&lt;ref&gt;[[François Letellier]], see 'Third Generation Open Source'&lt;/ref&gt; open-source organization, in that commercial support is provided without the risk of [[platform lock-in]].

Among the ASF's objectives are: to provide legal protection&lt;ref&gt;''See'' the [[Volunteer Protection Act]] article.&lt;/ref&gt; to volunteers working on Apache projects; to prevent the ''Apache'' brand name from being used by other organizations without permission.

The ASF also holds several [http://www.apachecon.com/ ApacheCon] conferences each year, highlighting Apache projects, related technology, and encouraging Apache developers to gather together.

==History==
The history of the Apache Software Foundation is linked to the Apache HTTP Server, development beginning in February, 1995. A group of eight developers started working on enhancing the [[NCSA HTTPd]] daemon. They came to be known as the Apache Group. On March 25, 1999, the Apache Software Foundation was formed.&lt;ref name=incorporation/&gt; The first official meeting of the Apache Software Foundation was held on April 13, 1999, and by general consent that the initial membership list of the Apache Software Foundation, would be: [[Brian Behlendorf]], [[Ken Coar]], Miguel Gonzales, Mark Cox, Lars Eilebrecht, Ralf S. Engelschall, [[Roy Fielding|Roy T. Fielding]], Dean Gaudet, Ben Hyde, [[Jim Jagielski]], Alexei Kosut, Martin Kraemer, [[Ben Laurie]], Doug MacEachern, Aram Mirzadeh, [[Sameer Parekh]], Cliff Skolnick, Marc Slemko, William (Bill) Stoddard, Paul Sutton, Randy Terbush and Dirk-Willem van Gulik.&lt;ref name=firstMeeting&gt;{{cite web
| first       = Ben
| last        = Hyde
| title       = The Apache Software Foundation Board of Directors Meeting Minutes 13 April 1999
| url         = http://www.apache.org/foundation/records/minutes/1999/board_minutes_1999_04_13.txt
| accessdate  = 2009-05-26
}}&lt;/ref&gt; After a series of additional meetings to elect board members and resolve other legal matters regarding incorporation, the effective incorporation date of the Apache Software Foundation was set to June 1, 1999.&lt;ref name=effDate/&gt;

The name 'Apache' was chosen from respect for the Native American [[Apache]] Nation, well known for their superior skills in warfare strategy and their inexhaustible endurance. It also makes a pun on &quot;a patchy web server&quot;—a server made from a series of patches—but this was not its origin. The group of developers who released this new software soon started to call themselves the &quot;Apache Group&quot;. {{citation needed|date=February 2013}}

==Projects==
{{See also|List of Apache Software Foundation projects}}
Apache divides its [[software development]] activities into separate semi-autonomous areas called &quot;top-level projects&quot; (formally known as a &quot;Project Management Committee&quot; in the bylaws&lt;ref name=bylaws&gt;
{{cite web
| title       = Bylaws of The Apache Software Foundation
| url         = http://www.apache.org/foundation/bylaws.html
| publisher = Apache Software Foundation
| accessdate  = 2011-08-10
| archiveurl= http://web.archive.org/web/20110725012133/http://apache.org/foundation/bylaws.html| archivedate= 25 July 2011 &lt;!--DASHBot--&gt;| deadurl= no}}
&lt;/ref&gt;), some of which have a number of sub-projects. Unlike some other organizations that host [[Free and Open Source Software|FOSS]] projects, before a project is hosted at Apache it has to be licensed to the ASF with a grant or contributor agreement.&lt;ref name=licenses&gt;
{{cite web
| title       = Licenses
| url         = http://www.apache.org/licenses/#clas
| publisher = Apache Software Foundation
| accessdate  = 2011-08-10
| archiveurl= http://web.archive.org/web/20110725011702/http://www.apache.org/licenses/| archivedate= 25 July 2011 &lt;!--DASHBot--&gt;| deadurl= no}}
&lt;/ref&gt; In this way, the ASF gains the necessary intellectual property rights for the development and distribution of all its projects.&lt;ref&gt;{{cite book|last= St. Amant|first=Kirk|coauthor=Brian Still|title=Handbook of research on open source software: technological, economic, and social perspectives|publisher=Idea Group Inc (IGI)|year=2007|pages=217–219|isbn=978-1-59140-999-1|url=http://books.google.co.uk/books?id=75KT6GdcWbYC&amp;pg=PA218&amp;dq=%22apache+foundation%22#v=onepage&amp;q=%22apache%20foundation%22&amp;f=false}}&lt;/ref&gt;

==Board of directors==
The ASF board of directors has responsibility for overseeing the ASF's activities and acting as a central point of contact and communication for its projects. The board assigns corporate issues, assigning resources to projects, and manages corporate services, including funds and legal issues. It does not make technical decisions about individual projects; these are made by the individual Project Management Committees. The board is elected annually by members of the foundation and, after the May 2013 Annual Members Meeting, it consists of:&lt;ref&gt;{{cite book
  | last = Weber
  | first = Steve
  | title = The success of open source
  | publisher = Harvard University Press
  | year = 2004
  | location = 
  | page = 187
  | url = http://books.google.co.uk/books?id=ELieXMxR1h4C&amp;pg=PA187&amp;dq=%22apache+software+foundation%22+board+of+directors#v=onepage&amp;q=%22apache%20software%20foundation%22%20board%20of%20directors&amp;f=false
  | isbn = 978-0-674-01292-9}}&lt;/ref&gt;&lt;ref&gt;{{cite web
  | title = Board of Directors
  | publisher = Apache Software Foundation
  | year = 2013
  | url = http://www.apache.org/foundation/board/
  | accessdate = 2013-05-23 }}&lt;/ref&gt;&lt;ref&gt;{{cite web
  | title = How the ASF works
  | publisher = Apache Software Foundation
  | year = 2010
  | url = http://www.apache.org/foundation/how-it-works.html#structure
  | accessdate = 08/04/10| archiveurl= http://web.archive.org/web/20100722185522/http://www.apache.org/foundation/how-it-works.html| archivedate= 22 July 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

* Shane Curcuru
* [[Doug Cutting]]
* Bertrand Delacretaz
* [[Roy T. Fielding]]
* [[Jim Jagielski]]
* Chris Mattmann
* Brett Porter (chairman)
* [[Sam Ruby]]
* [[Greg Stein]]

==Financials==
In the 2010–11 fiscal year, the Foundation took in $539,410, almost entirely from grants and contributions with $12,349 from two ApacheCons.  With no employees and 2,663 volunteers, it spent $270,846 on infrastructure, $92,364 on public relations, and $17,891 on two ApacheCons.&lt;ref&gt;{{cite web|url=http://www.apache.org/foundation/records/990-2010.pdf|date=2011-09-14|title=Apache Software Foundation IRS Form 990 for 2010|accessdate=2013-02-20}}&lt;/ref&gt;

==See also==
* [[Apache Incubator]]
* [[Apache Attic]]
* [[Free Software Foundation]]
* [[Open Software Foundation]]

==Notes==
{{reflist|2}}

==Further reading==
*''[[Wikinomics|Wikinomics: How Mass Collaboration Changes Everything]]'' (2006); [[Don Tapscott]], [[Anthony D. Williams (author)|Anthony D. Williams]].

==External links==
{{Commons category|Apache Software Foundation}}
*[http://www.apache.org Apache Foundation Official website]
*http://wiki.apache.org/general
*http://wiki.apache.org/apachecon/FrontPage
*[http://streaming.linux-magazin.de/en/archive_apachecon09.htm Free recordings of presentations from Apachecon09]

{{Apache}}
{{FOSS}}

[[Category:Apache Software Foundation| ]]
[[Category:Free and open-source software organizations]]
[[Category:Companies established in 1999]]
[[Category:Non-profit organizations based in Maryland]]
[[Category:Free software project foundations]]</text>
      <sha1>mu08xbavok057frk79wr3lvmk9hjj3w</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Tomcat</title>
    <ns>0</ns>
    <id>200698</id>
    <revision>
      <id>601623242</id>
      <parentid>600620997</parentid>
      <timestamp>2014-03-28T07:47:59Z</timestamp>
      <contributor>
        <ip>195.238.92.121</ip>
      </contributor>
      <comment>/* See also */</comment>
      <text xml:space="preserve" bytes="13313">{{lead too short|date=October 2013}}
{{primary sources|date=February 2013}}
{{Infobox software
| name                   = Apache Tomcat
| logo                   = [[File:Tomcat-logo.svg|200px|Apache Tomcat Logo]]
| screenshot             = [[File:Apache-tomcat-frontpage-epiphany-browser.jpg|250px]]
| caption                = Apache Tomcat Default Page
| collapsible            = yes
| developer              = [[Apache Software Foundation]]
| status                 = Active
| frequently updated = yes
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Java Servlet#Servlet containers|Servlet container]]&lt;br /&gt;[[Hypertext Transfer Protocol|HTTP]] [[web server]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://tomcat.apache.org/}}
| file_ext               = .war
}}
'''Apache Tomcat''' (or simply '''Tomcat''', formerly also ''Jakarta Tomcat'') is an [[open source]] [[web server]] and [[Java Servlet|servlet]] [[web container|container]] developed by the [[Apache Software Foundation]] (ASF). Tomcat implements the [[Java Servlet]] and the [[JavaServer Pages]] (JSP) specifications from [[Sun Microsystems]], and provides a &quot;pure [[Java (programming language)|Java]]&quot; [[Hypertext Transfer Protocol|HTTP]] [[web server]] environment for [[Java (programming language)|Java]] code to run in.

Apache Tomcat includes tools for configuration and management, but can also be configured by editing [[XML]] configuration files.

==Components==
Tomcat 4.x was released with Catalina (servlet container), Coyote (a HTTP connector) and Jasper (a [[JSP engine]]).

===Catalina===
Catalina is Tomcat's  [[Web container|servlet container]]. Catalina implements [[Sun Microsystems]]' specifications for [[Java servlet|servlet]] and [[JavaServer Pages]] (JSP). In Tomcat, a Realm element represents a &quot;database&quot; of usernames, passwords, and roles (similar to Unix groups) assigned to those users. Different implementations of Realm allow Catalina to be integrated into environments where such authentication information is already being created and maintained, and then use that information to implement Container Managed Security as described in the Servlet Specification.&lt;ref&gt;{{cite web|url=http://tomcat.apache.org/tomcat-5.5-doc/config/realm.html |title=Apache Tomcat Configuration Reference - The Realm Component |publisher=Tomcat.apache.org |date= |accessdate=2013-11-01}}&lt;/ref&gt;

===Coyote===
Coyote is Tomcat's HTTP Connector component that supports the HTTP 1.1 protocol for the web server or application container. Coyote listens for incoming connections on a specific [[Transmission Control Protocol|TCP]] port on the server and forwards the request to the Tomcat Engine to process the request and send back a response to the requesting client. It can execute JSP's and Servlets.

===Jasper===
Jasper is Tomcat's JSP Engine.  Jasper parses [[JSP files]] to compile them into Java code as servlets (that can be handled by Catalina). At runtime, Jasper detects changes to JSP files and recompiles them.

As of version 5, Tomcat uses Jasper 2, which is an implementation of the [[Sun Microsystems]]'s [[JavaServer Pages|JSP]] 2.0 specification. From Jasper to Jasper 2, important features were added:
* JSP Tag library pooling - Each tag markup in JSP file is handled by a tag handler class. Tag handler class objects can be pooled and reused in the whole JSP servlet.
* Background JSP compilation - While recompiling modified JSP Java code, the older version is still available for server requests. The older JSP servlet is deleted once the new JSP servlet has finished being recompiled.
* Recompile JSP when included page changes - Pages can be inserted and included into a JSP at runtime. The JSP will not only be recompiled with JSP file changes but also with included page changes.
* JDT Java compiler - Jasper 2 can use the Eclipse JDT (Java Development Tools) Java compiler instead of [[Apache Ant|Ant]] and &lt;code&gt;[[javac]]&lt;/code&gt;.

Three new components were added with the release of Tomcat 7:

===Cluster===
This component has been added to manage large applications. It is used for [[Load balancing (computing)|load balancing]] that can be achieved through many techniques. Clustering support currently requires the JDK version 1.5 or later.

===High availability===
A high-availability feature has been added to facilitate the scheduling of system upgrades (e.g. new releases, change requests) without affecting the live environment. This is done by dispatching live traffic requests to a temporary server on a different port while the main server is upgraded on the main port. It is very useful in handling user requests on high-traffic web applications.&lt;ref&gt;{{cite web |url=http://www.javaworld.com/javaworld/jw-12-2004/jw-1220-tomcat.html |title=High availability Tomcat - Connect Tomcat servers to Apache and to each other to keep your site running |last1=King |first1=Graham |date=2004-12-20 |publisher=[[JavaWorld]] |accessdate=2013-02-13}}&lt;/ref&gt;

===Web Application===
It has also added user as well as system based web applications enhancement to add support for deployment across the variety of environments. It also tries to manage session as well as applications across the network.

Tomcat is building additional components.  A number of additional components may be used with Apache Tomcat. These components may be built by users should they need them or they can be downloaded from one of the mirrors.&lt;ref&gt;{{cite web|author=Remy Maucherat |url=https://tomcat.apache.org/tomcat-7.0-doc/extras.html |title=Apache Tomcat 7 (7.0.47) - Additional Components |publisher=Tomcat.apache.org |date=2013-10-18 |accessdate=2013-11-01}}&lt;/ref&gt;

==Features==
Tomcat 7.x implements the Servlet 3.0 and JSP 2.2 specifications.&lt;ref&gt;{{cite web|url=http://tomcat.apache.org/whichversion.html|title=Apache Tomcat Versions|accessdate=2011-11-12}}&lt;/ref&gt; It requires Java version 1.6, although previous versions have run on Java 1.1 through 1.5. Versions 5 through 6 saw improvements in [[Garbage collection (computer science)|garbage collection]], JSP parsing, performance and scalability. Native wrappers, known as &quot;Tomcat Native&quot;, are available for [[Microsoft Windows]] and Unix for platform integration.

==History==
Tomcat started off as a servlet [[reference implementation (computing)|reference implementation]] by [[James Duncan Davidson]], a software architect at [[Sun Microsystems]]. He later helped make the project [[open source]] and played a key role in its donation by [[Sun Microsystems]] to the Apache Software Foundation. The [[Apache Ant]] software build automation tool was developed as a side-effect of the creation of Tomcat as an open source project.

Davidson had initially hoped that the project would become open sourced and, since many open source projects had [[O'Reilly Media|O'Reilly]] books associated with them featuring an animal on the cover, he wanted to name the project after an animal. He came up with ''[[Cat|Tomcat]]'' since he reasoned the animal represented something that could fend for itself. Although the tomcat was already in use for another O'Reilly title, his wish to see an animal cover eventually came true when O'Reilly published their Tomcat book with a [[snow leopard]] on the cover.&lt;ref&gt;{{Citation |title=Tomcat: The Definitive Guide |url=http://www.oreilly.com/catalog/tomcat/ |author=Jason Brittain, Ian F. Darwin |publisher=O'Reilly Books |isbn=0-596-00318-8 |page=322 |postscript=&lt;!--none--&gt;}}&lt;/ref&gt;

===Releases===
{{Prose|section|date=March 2013}}
{| class=&quot;wikitable sortable&quot;  style=&quot;margin:auto; margin:0 0 0 2em;&quot;
|+ Apache Tomcat versions
|-
! Version
! Release Date
! Description
|-
| 3.0.x. (initial release)
| 1999
| Merger of donated Sun Java Web Server code and ASF and Implements Servlet 2.2 and JSP 1.1 specifications.
|-
| 3.3.2
| 2004-03-09
| Latest 3.x release.
|-
| 4.1.31
| 2004-10-11
|
|-
| 4.1.40
| 2009-06-25
| Latest 4.x release.
|-
| 5.0.0
| 2002-10-09
|
|-
| 5.0.30
| 2004-08-30
| Latest 5.0.x release
|-
| 5.5.0
| 2004-08-31
|
|-
| 5.5.36
| 2012-10-10
| Latest 5.5.x release, &lt;ref&gt;http://tomcat.apache.org/tomcat-55-eol.html&lt;/ref&gt;
|-
| 6.0.0
| 2006-12-01
|
|-
| 6.0.39
| 2014-01-31
| Latest 6.x release.
|-
| 7.0.0 beta
| 2010-06-29
| First Apache Tomcat release to support the [[Java Servlet|Servlet 3.0]], [[JavaServer Pages|JSP 2.2]], and [[Expression Language|EL 2.2]] specifications.
|-
| 7.0.52
| 2014-02-17
| Current stable version.
|-
| 8.0.
| 2013-08-18
| dev version.
|-
| 8.0.3
| 2014-02-11
| Current Beta
|}

==Communities==
Apache software is built as part of a community process that involves both user and developer [http://tomcat.apache.org/lists.html mailing lists]. The developer list is where discussion on building and testing the next release takes place, while the user list is where users can discuss their problems with the developers and other users.

Some of the free Apache Tomcat resources and communities include [http://www.tomcatexpert.com/ Tomcatexpert.com] (a [[SpringSource]] sponsored community for developers and operators who are running Apache Tomcat in large-scale production environments) and MuleSoft's [http://www.mulesoft.com/understanding-apache-tomcat/ Apache Tomcat Resource Center] (which has instructional guides on installing, updating, configuring, monitoring, troubleshooting and securing various versions of Tomcat).

==Apache TomEE==
[[Apache TomEE]] (pronounced &quot;Tommy&quot;) is the [[Java Enterprise Edition]] of Apache Tomcat (Tomcat + Java EE = TomEE) that combines several Java enterprise projects including [[Apache OpenEJB]], Apache OpenWebBeans, [[Apache OpenJPA]], [[Apache MyFaces]] and others.&lt;ref&gt;{{cite web|url=http://openejb.apache.org/apache-tomee.html|title=Apache TomEE|publisher=[[Apache OpenEJB]]}}&lt;/ref&gt; In October 2011, the project obtained certification by [[Oracle Corporation]] as a compatible implementation of the Java EE 6 Web Profile.&lt;ref&gt;{{cite web|url=http://www.marketwatch.com/story/the-apache-software-foundation-announces-apache-tomee-certified-as-java-ee-6-web-profile-compatible-2011-10-04|title=The Apache Software Foundation Announces Apache TomEE Certified as Java EE 6 Web Profile Compatible|publisher=[[MarketWatch]]|date=4 Oct 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.infoworld.com/d/application-development/apache-tomee-web-stack-gains-approval-175341|title=Apache TomEE Web stack gains approval|publisher=[[InfoWorld]]|date=7 Oct 2011}}&lt;/ref&gt;

==See also==
* Tcat,&lt;ref&gt;[http://www.mulesoft.com/tcat Tcat]&lt;/ref&gt; an enterprise Tomcat server from MuleSoft
* [[Apache Geronimo]], an application server that can use Tomcat as its web container
* [[Resin Server]] Application Server from [[Caucho Technology]]
* [[WildFly]], formerly known as JBoss Application Server
* [[Jetty (web server)]]
* [[JOnAS]], application server that can use Tomcat as its web container
* [[Apache OpenEJB]], can be added to Tomcat to turn it into a JavaEE server
* [[GlassFish]], the [[reference implementation]] of Java EE , supporting [[Enterprise JavaBeans|EJB]], [[Java Persistence API|JPA]], [[JavaServer Faces|JSF]], [[Java Message Service|JMS]], [[Java remote method invocation|Java RMI]], [[JavaServer Pages|JSP]], servlets etc
* [[Comparison of web server software|Comparison of web servers]]

== References ==
{{reflist}}

==Bibliography==
{{refbegin}}
*{{citation
| first1     = Jason
| last1      = Brittain
| first2     = Ian
| last2      = Darwin
| date       = October 23, 2007
| title      = Tomcat: The Definitive Guide
| edition    = 2nd
| publisher  = [[O'Reilly Media]]
| page      = 494
| isbn       = 978-0-596-10106-0
| url        = http://oreilly.com/catalog/9780596101060/
| accessdate        = 2009-10-08
| postscript     = &lt;!--none--&gt;
}}
*{{citation
| first1     = Vivek
| last1      = Chopra
| first2     = Sing
| last2      = Li
| first3     = Jeff
| last3      = Genender
| date       = August 13, 2007
| title      = Professional Apache Tomcat 6
| edition    = 1st
| publisher  = [[Wrox Press|Wrox]]
| page      = 629
| isbn       = 978-0-471-75361-2
| url        = http://www.wrox.com/WileyCDA/WroxTitle/productCd-0471753610.html
| accessdate        = 2009-10-08
| postscript     = &lt;!--none--&gt;
}}
*{{citation
| first1     = Matthew
| last1      = Moodie
| first2     = Kunal
| last2      = Mittal (Ed.)
| date       = March 22, 2007
| title      = Pro Apache Tomcat 6
| edition    = 1st
| publisher  = [[Apress]]
| page      = 325ে
| isbn       = 978-1-59059-785-9
| url        = http://www.apress.com/book/view/9781590597859
| accessdate        = 2009-10-08
| postscript     = &lt;!--none--&gt;
}}
{{refend}}

==External links==
{{Commons category}}
*{{Official website|tomcat.apache.org}}
*[http://wiki.apache.org/tomcat/ Project Wiki]
*[http://www.tomcatexpert.com/ Enterprise Tomcat Community site]
*[http://www.coreservlets.com/Apache-Tomcat-Tutorial/ Tutorial - Configuring &amp; Using Tomcat 6 and Tomcat 7]

{{Apache}}
{{Web server software}}

[[Category:Apache Software Foundation|Tomcat]]
[[Category:Java enterprise platform]]
[[Category:Free software programmed in Java]]
[[Category:Free web server software]]
[[Category:Cross-platform free software]]
[[Category:Software using the Apache license]]</text>
      <sha1>cmjgegtop0dm7oufdh3xp4bysvgcf0z</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Traffic Server</title>
    <ns>0</ns>
    <id>25285165</id>
    <revision>
      <id>588613181</id>
      <parentid>587712636</parentid>
      <timestamp>2014-01-01T03:29:47Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor/>
      <comment>fixed [[:Category:CS1 errors: dates|CS1 errors: dates]] &amp; [[WP:AWB/GF|General fixes]] using [[Project:AWB|AWB]] (9832)</comment>
      <text xml:space="preserve" bytes="5514">{{Infobox software
| name                   = Apache Traffic Server
| logo                   = [[Image:ASF-logo.svg|160px]]
| screenshot             =
| caption                =
| author                 =
| developer              = [[Apache Software Foundation]]
| released               =
| latest release version = 4.1.2
| latest release date    = {{Start date and age|2013|12|17|df=yes/no}}　&lt;ref&gt;{{cite web |url=https://trafficserver.apache.org/downloads |title=Apache Traffic Server Downloads  |date=17 Dec 2013 |website=https://trafficserver.apache.org/ |publisher=Apache Software Foundation |accessdate=December 26, 2013}}&lt;/ref&gt;
| programming language   = [[C++ (programming language)|C++]]
| operating system       =
| language               = English
| genre                  = [[Web server]]
| license                = [[Apache License]] 2.0&lt;ref&gt;{{ cite web | url=https://svn.apache.org/repos/asf/incubator/trafficserver/traffic/trunk/LICENSE | title=Traffic Server license file | publisher=[[Apache Software Foundation]] | accessdate=2009-12-24 }}&lt;/ref&gt;
| website                = {{URL|https://trafficserver.apache.org/}}
}}
{{Portal|Free software}}
The [[Apache Software Foundation|Apache]] '''Traffic Server''' (TS) is a modular, high-performance [[reverse proxy]] and [[forward proxy]] server, generally comparable to [[Nginx]] and [[Squid (software)|Squid]].  It was created by [[Inktomi (company)|Inktomi]], and distributed as a commercial product called the Inktomi Traffic Server, before [[Inktomi]] was acquired by [[Yahoo!]].  Yahoo!'s OStatic guest post ''Yahoo's Cloud Team Open Sources Traffic Server''&lt;ref&gt;{{cite web|url=http://ostatic.com/blog/guest-post-yahoos-cloud-team-open-sources-traffic-server|title=Yahoo's Cloud Team Open Sources Traffic Server|date=2009-11-02}}&lt;/ref&gt; states that Yahoo! uses TS in production to serve more than 30 Billion objects per day on sites like the Yahoo! homepage, and Yahoo! Sports, Mail and Finance. Yahoo! released the TS source to Apache as an Apache Incubator project in July 2009.&lt;ref&gt;{{cite web|url=http://wiki.apache.org/incubator/August2009|title=Apache Incubator Wiki August 2009 Board reports|date=2009-08-12}}&lt;/ref&gt;  On April 21, 2010, the Apache board accepted Traffic Server as a TLP, graduating the project out of incubation.&lt;ref&gt;{{cite web|url=http://developer.yahoo.com/blogs/ydn/traffic-server-graduates-top-level-open-source-project-7941.html|title=Traffic Server graduates to top-level open-source project|date=2010-04-23}}&lt;/ref&gt;

TS is now in version 4.0.1, with quarterly minor versions scheduled. As of version 4.0, all releases are considered stable release, and all releases follow regular [[Semantic Versioning]]. No more developer preview releases will be made, instead, the git master branch is considered preview quality at all time. Long term support is provided for the last minor version within a major release, for one additional year.&lt;ref&gt;{{Cite web|url = https://cwiki.apache.org/confluence/display/TS/Release+Management|title = Official project release management process|date = 2013-09-06}}&lt;/ref&gt;

The OStatic post describes TS as shipping &quot;... with not only an HTTP web proxy and caching solution, but also ... a server framework, with which you can build very fast servers for other protocols&quot;.  Traffic Server has been benchmarked to handle 200,000 requests per second or more (small objects out of cache).&lt;ref&gt;{{cite web|url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces13|title=The Apache Software Foundation Announces Apache Traffic Server v3.0.0 : The Apache Software Foundation Blog|date=2011-06-14}}&lt;/ref&gt;  At a talk at the 2009 Cloud Computing Expo,&lt;ref&gt;{{cite web|url=http://cloudcomputingexpo.com|title=2009 Cloud Computing Expo|date=2009-08-12}}&lt;/ref&gt; members of the Yahoo! TS team stated that TS is used in production at Yahoo! to handle 400TB of traffic per day using only 150 commodity machines.  The OStatic post describes TS as the &quot;product of literally hundreds of developer-years&quot;.

In the context of cloud computing, TS would sit conceptually at the edge of the cloud, routing requests as they come in. In Yahoo!, it is used for the edge services as shown in a graphic&lt;ref&gt;{{cite web|url=http://farm3.static.flickr.com/2452/4078596535_a25824be31.jpg|title=Yahoo's edge services graphic| accessdate=2011-06-14|date=2011-06-14}}&lt;/ref&gt; distributed at the 2009 Cloud Computing Expo depicting Yahoo!'s private cloud architecture.  In practical terms, a typical server configuration might use TS to serve static content, such as images and JavaScript, CSS, and HTML files, and route requests for dynamic content to a web server such as [[Apache HTTP Server]].

==References==
&lt;!--- See [[Wikipedia:Footnotes]] on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

==External links==
* [http://trafficserver.apache.org/ The ASF Traffic Server - official project's website]
* [http://static.usenix.org/events/lisa11/tech/slides/hedstrom.pdf Apache Traffic Server: More Than Just a Proxy] - talk by Leif Hedstrom at [[USENIX]] ({{YouTube|id=RNTw7jZwlKQ|title=Video Archive}})

{{Apache}}

[[Category:Apache Software Foundation]]
[[Category:Free proxy servers]]
[[Category:Free software programmed in C++]]
[[Category:Free web server software]]
[[Category:Cross-platform free software]]
[[Category:Forward proxy]]
[[Category:Proxy servers]]
[[Category:Reverse proxy]]
[[Category:Unix network-related software]]</text>
      <sha1>5vgr8f47nk0gdeomtci4yng2w9hujoo</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>List of Apache modules</title>
    <ns>0</ns>
    <id>27470570</id>
    <revision>
      <id>600246899</id>
      <parentid>598296399</parentid>
      <timestamp>2014-03-19T01:43:14Z</timestamp>
      <contributor>
        <username>Torkel.bjornson</username>
        <id>9846299</id>
      </contributor>
      <minor/>
      <comment>Updated entry for mod_gnutls</comment>
      <text xml:space="preserve" bytes="29789">{{Expand list|date=May 2010}}

[[Apache HTTP Server|Apache]] is an open source [[HTTP server]]. It comprises a small core for [[Hypertext Transfer Protocol|HTTP request/response processing]] and Multi-Processing Modules (MPM) which dispatches data processing to threads and/or processes. Many additional modules are available to extend its core functionality for special purposes.

The following is a list of all the first and third party modules available for the [[Apache]] web server:
&lt;!--
ATTENTION!
ATTENTION!
ATTENTION!
ATTENTION!
ATTENTION!
ATTENTION!
ATTENTION!
ATTENTION!
ATTENTION!
ATTENTION!

Do NOT add modules here if they are not modules for Apache HTTPd web server.

That means NO JOOMLA MODULES, etc.

--&gt;
{|class=&quot;wikitable sortable&quot;
!Name!!Compatibility!!Status!!Developer(s)!!License!!Description
|-
|mod_access||Versions older than 2.1||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Provides access control based on the client and the client's request&lt;ref&gt;{{cite web|url=http://httpd.apache.org/docs/2.0/mod/mod_access.html |title=Apache Module mod_access |work=Apache HTTP Server 2.0 Documentation |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
|-
|mod_actions||Versions 1.1 and later||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Provides [[Common Gateway Interface|CGI]] ability based on request method and media type&lt;ref&gt;{{cite web|url=http://httpd.apache.org/docs/1.3/mod/mod_actions.html |title=Module mod_actions |work=Apache HTTP Server 1.3 Documentation |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
|-
|mod_alias||Versions 1.1 and later||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Allows for client requests to be mapped to different parts of a server's filesystem and for the requests to be redirected entirely&lt;ref&gt;While the documentation doesn't specifically mention compatibility with versions 1.1 and 1.2, it says that certain directives worked in those versions. This does not however mean, that older versions were not compatibile with this module. {{cite web|url=http://httpd.apache.org/docs/1.3/mod/mod_alias.html |title=Module mod_alias |work=Apache HTTP Server 1.3 Documentation |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
|-
|mod_asis||Versions 1.3 and later||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Allows for the use of files that contain their own [[HTTP headers]]&lt;ref&gt;This module is noted in the documentation for Apache versions 1.3, but no mention is made of earlier versions. This doesn't mean that the module doesn't exist for older versions, but only that the documentation provides no indication that it does. {{cite web|url=http://httpd.apache.org/docs/1.3/mod/mod_asis.html |title=Module mod_alias |work=Apache HTTP Server 1.3 Documentation |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
|-
|mod_aspdotnet|| || || ||  || No longer under the Apache Software Foundation umbrella. Development has been resumed by the original authors at its new home, the [http://sourceforge.net/projects/mod-aspdotnet/ mod-aspdotnet SourceForge project.]
|-
|mod_auth||Versions older than 2.1||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Authenticates users via [[Basic access authentication]] by checking against [[plaintext]] password and group files. In Apache 2.1 and later, this plaintext authentication is enabled by mod_authn_file instead&lt;ref&gt;{{cite web|url=http://httpd.apache.org/docs/2.0/mod/mod_auth.html |title=Apache Module mod_auth |work=Apache HTTP Server 2.0 Documentation |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
|-
|mod_auth_anon||Versions 1.1-2.1||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Allows authentication with a special user id of 'anonymous' and an email address as the password. As an authentication mechanism, this was replaced by mod_authn_anon&lt;ref&gt;{{cite web|url=http://httpd.apache.org/docs/2.0/mod/mod_auth_anon.html |title=Apache Module mod_auth_anon |work=Apache HTTP Server 2.0 Documentation |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
|-
|mod_auth_basic||Versions 2.1 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Authenticates users via [[Basic access authentication|HTTP Basic Authentication]], the backend mechanism for verifying user authentication is left to configurable providers, usually other Apache modules. This module replaces the authentication frontend of several older modules.&lt;ref&gt;{{cite web|url=http://httpd.apache.org/docs/2.2/mod/mod_auth_basic.html |title=Apache Module mod_auth_basic |work=Apache HTTP Server 2.2 Documentation |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
|-
|mod_auth_db||Versions 1.1-1.3||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Provides user authentication using Berkeley DB files.
|-
|mod_auth_dbm||Versions older than 2.1||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||This module provides for HTTP Basic Authentication, where the usernames and passwords are stored in DBM type database files. It is an alternative to the plain text password files provided by mod_auth.
|-
|mod_auth_digest||Versions 1.3.8 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Authenticates users via [[Digest access authentication|HTTP Digest Authentication]] utilizing MD5 encryption. This is more secure than [[Basic access authentication|HTTP Basic Authentication]] provided by other modules. As of Apache 2.1, this module acts as a front-end to authentication providers who verify the actual login.&lt;ref&gt;{{cite web|url=http://httpd.apache.org/docs/2.2/mod/mod_authn_file.html |title=Apache Module mod_auth_digest |work=Apache HTTP Server 2.2 Documentation |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
|-
|mod_auth_kerb|| || || || ||
|-
|mod_auth_ldap||Versions 2.0.41-2.1||Experimental Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Allows [[Basic access authentication|HTTP Basic Authentication]] by checking against an [[LDAP]] directory. The authentication mechanism of checking against an [[LDAP]] directory is provided via mod_authnz_ldap in Apache versions 2.1 and later.
|-
|mod_auth_oid||Version 2.2||Third-party module||Pascal Buchbinder||[[GNU General Public License#Version_2|GNU General Public License, Version 2]]||Allows an Apache server to act as an [[OpenID]] &quot;Relying Party&quot;&lt;ref&gt;{{cite web|url=http://auth-openid.sourceforge.net/ |title=mod_auth_oid |author=Pascal Buchbinder |publisher=[[SourceForge|SourceForge.net]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
|-
|mod_authn_alias||Version 2.1 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||This module allows extended authentication providers to be created within the configuration file and assigned an alias name. The alias providers can then be referenced through the directives AuthBasicProvider or AuthDigestProvider in the same way as a base authentication provider.
|-
|mod_authn_anon||Version 2.1 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Acts as an authentication provider to other modules, like mod_auth_basic and mod_auth_digest, users are authenticated by using a special user id of &quot;anonymous&quot; and providing their email as the password.&lt;ref&gt;{{cite web|url=http://httpd.apache.org/docs/2.2/mod/mod_authn_anon.html |title=Apache Module mod_authn_anon |work=Apache HTTP Server 2.2 Documentation |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
|-
|mod_authn_dbd||Version 2.1 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||This module provides authentication front-ends such as mod_auth_digest and mod_auth_basic to authenticate users by looking up users in SQL tables. Similar functionality is provided by, for example, mod_authn_file.
|-
|mod_authn_dbm||Version 2.1 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||This module provides authentication front-ends such as mod_auth_digest and mod_auth_basic to authenticate users by looking up users in dbm password files. Similar functionality is provided by mod_authn_file.
|-
|mod_authn_default||Version 2.1 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||This module is designed to be the fallback module, if you don't have configured an authentication module like mod_auth_basic. It simply rejects any credentials supplied by the user.
|-
|mod_authn_file||Version 2.1 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Acts as an authentication provider to other modules, like mod_auth_basic and mod_auth_digest, by checking users against plaintext password files.&lt;ref&gt;{{cite web|url=http://httpd.apache.org/docs/2.2/mod/mod_authn_file.html |title=Apache Module mod_authn_file |work=Apache HTTP Server 2.2 Documentation |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
|-
|mod_authnz_external|| || || || ||
|-
|mod_authnz_ldap||Version 2.1 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Acts as an authentication provider to other modules and checks authentication against an [[LDAP]] directory
|-
|mod_authnz_mysql||Version 2.2|| || || ||This module provides both authentication and authorization for the Apache 2.2 webserver like mod-authnz-ldap . It uses a MySQL database to retrieve user and group informations.
|-
|mod_authz_host||Version 2.1 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Group authorizations based on host (name or IP address)
|-
|mod_authz_svn|| || || || ||Configuration Directives — Apache configuration directives for configuring path-based authorization for Subversion repositories served through the Apache HTTP Server.
|-
|mod_autoindex||Version 1.3 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Generates automatic directory listing for display by the server&lt;ref&gt;{{cite web|url=http://httpd.apache.org/docs/2.2/mod/mod_autoindex.html |title=Apache Module mod_autoindex |work=Apache HTTP Server 2.2 Documentation |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
|-
|http://www.backhand.org/mod_backhand/|mod_backhand|| || || || ||Seamless redirection of HTTP requests from one web server to another. Can be used to target machines with under-utilized resources. 
|-
|mod_balancer|| || || || ||
|-
|http://www.cohprog.com/mod_bandwidth.html|mod_bandwidth|| || || || ||Server-wide or per connection bandwidth limits, based on the directory, size of files and remote IP/domain. 
|-
|mod_bonjour|| || || || ||
|-
|mod_bw|| || || || ||The httpd web server doesn't really have a way to control how much resources a given virtual host can have/ a user can request. This module should be able to limit access to certain areas of the website and to limit malicious users.
|-
|mod_bwlimited|| || || || ||mod_bwlimited is a CPanel module that allows limiting and monitoring of bandwidth and connection speed etc. It allows CPanel to give very accurate reports of bandwidth usage on HTTP, HTTPS, FTP, SMTP and a few other services as well as limiting bandwidth usage and connection speed. The module is only available on servers running CPanel and isn't available anywhere else
|-
|http://sourceforge.net/projects/mod-c/|mod_c|| ||Third-party module || || ||Cache DLL/SO executables to create very high speed dynamic web pages. mod_c is supported by EHTML (executable HTML)
|-
|mod_cache||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_cern_meta||Version 1.1 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_cgi||Version 1.1 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_cgid||Version 2.0 and newer, &quot;Unix threaded MPMs only&quot;&lt;ref&gt;{{cite web|url=http://httpd.apache.org/docs/2.2/mod/mod_cgid.html |title=Apache Module mod_cgid |work=Apache HTTP Server 2.2 Documentation |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_charset_lite||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|[http://httpd.apache.org/docs/2.2/mod/mod_dav_fs.html mod_dav]||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Provides WebDAV (Web-based Distributed Authoring and Versioning) functionality in Apache.
|-
|[http://httpd.apache.org/docs/2.2/mod/mod_dav.html mod_dav_fs]||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Acts as a support module for mod_dav and provides access to resources located in the server's file system.
|-
|[http://people.apache.org/~rjung/mod_define/ mod_define]||Version 1.3 and newer||Third Party|| ||[[Apache License]], Version 2.0|| Definition of variables for arbitrary directives.
|-
|[[mod_deflate]]||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_dir||Version 1.3 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_disk_cache||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_dumpio||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0|| allows for the logging of all input received by Apache and/or all output sent by Apache to be logged (dumped) to the error.log file.
|-
|mod_echo||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||This module provides an example protocol module to illustrate the concept. It provides a simple echo server. Telnet to it and type stuff, and it will echo it.
|-
|mod_env||Version 1.1 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||This module allows for control of internal environment variables that are used by various Apache HTTP Server modules. These variables are also provided to CGI scripts as native system environment variables, and available for use in SSI pages. Environment variables may be passed from the shell which invoked the httpd process. Alternatively, environment variables may be set or unset within the configuration process.
|-
|http://www.zdziarski.com/blog/?page_id=442|mod_evasive|| ||Third-party module || || ||Evasive maneuvers module for Apache to provide evasive action in the event of an HTTP DoS or DDoS attack or brute force attack. Also designed to be a detection and network management tool.
|-
|mod_example||Version 1.2 and newer||Experimental Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||The example module is an actual working module. If you link it into your server, enable the &quot;example-handler&quot; handler for a location, and then browse to that location, you will see a display of some of the tracing the example module did as the various callbacks were made.
|-
|mod_expires||Version 1.2 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_ext_filter||Version 1.3 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_extract|| || || || ||
|-
|mod_fcgid||Version 2.0, 2.2 and 2.4||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||mod_fcgid is a high performance alternative to mod_cgi or mod_cgid, which starts a sufficient number instances of the CGI program to handle concurrent requests, and these programs remain running to handle further incoming requests. It is favored by the PHP developers, for example, as a preferred alternative to running mod_php in-process, delivering very similar performance.
|-
|mod_fastcgi|| || || || ||This 3rd party module provides support for the FastCGI protocol. FastCGI is a language independent, scalable, open extension to CGI that provides high performance and persistence without the limitations of server specific APIs.
|-
|mod_file_cache||Version 2.0 and newer||Experimental Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|http://journal.paul.querna.org/articles/2006/07/11/mod_flvx/|mod_flvx|| || || || ||Stream Flash video
|-
|mod_frontpage|| || || || ||Starts a service for Microsoft FrontPage.
|-
|mod_frontpage Mirfak|| || || ||[[Apache License]], Version 2.0||Mirfak is an open-source mod_frontpage reimplementation that is more secure, and can be used with a binary installation of Apache (possibly including mod_ssl, php, etc.). The module is licenced under the Apache license.
|-
|http://www.maxmind.com/app/mod_geoip|mod_geoip|| || || || ||Looks up the IP address of the client end user. Can be used to perform redirection based on country.
|-
|[https://mod.gnutls.org/ mod_gnutls]|| || || ||[[Apache License]], Version 2.0||Similar to mod_ssl in purpose, but it supports some features and protocols that mod_ssl does not, and it does not use [[OpenSSL]].
|-
|mod_gzip|| || || || ||
|-
|mod_headers||Version 1.2 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_h264_streaming|| ||Third-party module || || ||
|-
|mod_ibm_ssl|| || || || ||
|-
|mod_imagemap|| || || || ||
|-
|mod_imap||Version 1.2-2.0||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|[http://httpd.apache.org/docs/2.2/mod/mod_include.html mod_include]||Version 1.3 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Enables server-side includes.
|-
|mod_indent|| || || || ||
|-
|mod_info||Version 1.1 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_isapi||Version 1.3 and newer, [[Win32]] only||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|[[mod_jk]]|| || || || || Tomcat redirector module.
|-
|mod_ldap||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|[[mod_lisp]]|| || || || ||
|-
|mod_log_config||Version 1.2 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Provides flexible logging of client requests in a customizable format.
|-
|mod_log_forensic||Version 1.3 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_logio||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Provides the logging of input and output number of bytes received/sent per request.
|-
|[http://people.apache.org/~fabien/mod_macro/ mod_macro]||Version 1.3 and newer||Third party|| ||[[Apache License]] postcard variant||Allows to define and use macros within Apache runtime configuration files.
|-
|mod_magnet|| || || || ||
|-
|mod_mem_cache||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_mime||Version 1.3 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|[http://httpd.apache.org/docs/2.2/mod/mod_mime_magic.html mod_mime_magic]||Version 1.3 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Determines the MIME type of files in the same way the Unix file(1) command works: it looks at the first few bytes of the file. Intended as a &quot;second line of defense&quot; for cases that mod_mime can't resolve.
|-
|[[mod_mono]]|| || || || ||
|-
|mod_musicindex|| || || || ||
|-
|mod_mysql|| || || || ||
|-
|mod_negotiation||Version 1.3 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_nibblebill|| || || || ||
|-
|http://directory.fedoraproject.org/wiki/Mod_nss|mod_nss|| || || || ||SSL provider derived from the mod_ssl module for the Apache web server that uses the Network Security Services.
|-
|mod_ntlm|| || || || ||
|-
|mod_ntlm_winbind|| || || || ||
|-
|mod_ntlm2|| || || || ||
|-
|mod_nw_ssl||Version 2.0 and newer, Netware only||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_oc4j|| || || || ||
|-
|[[mod_openpgp]]|| || || || ||
|-
|mod_ossl|| || || || ||
|-
|mod_owa|| || || || ||
|-
|mod_pagespeed|| Version 2.2 and newer || Third Party Module || Google || [[Apache License]], Version 2.0 || Rewrites web pages to reduce latency and bandwidth.  See [http://modpagespeed.com]
|-
|[[mod_parrot]]|| || || || ||
|-
|[[mod_perl]]||Version 1.3 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Allows usage of  [[Perl]] within Apache
|-
|[[mod_php]] / libphp5||Version 1.3 and newer&lt;ref&gt;{{cite web|url=https://modules.apache.org/search.php?id=80 |title=mod_php |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt; ||Third-party module||The PHP Group||[[PHP License]] ||Enables usage of [[PHP]] within Apache
|-
|[http://plackperl.org/ mod_psgi]||Version 2.2 and newer|| || ||[[Apache License]], Version 2.0||Implements the [[PSGI]] specification within Apache
|-
|[[mod_python]]||Version 2.0 and newer || Third-party module ||Gregory Trubetskoy et al. ||[[Apache License]], Version 2.0 || Allows usage of [[Python (programming language)|Python]] within Apache&lt;ref&gt;{{cite web|url=https://modules.apache.org/search.php?id=220 |title=mod_python |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
|-
|[[mod_proxy]]||Version 1.1 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0|| 
|-
|mod_proxy_connect||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_proxy_ftp||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_proxy_html||Version 2.4 and newer, available as a third-party module for earlier 2.x versions||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0|| Rewrite HTML links in to ensure they are addressable from Clients' networks in a proxy context.
|-
|mod_proxy_http||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|[[mod_qos]]||Version 2.2||Third-party module||Pascal Buchbinder||[[GNU General Public License#Version_2|GNU General Public License, Version 2]]||Controls access to the web server to avoid resource oversubscription.
|-
|mod_rails|| || || || ||
|-
|[[mod_rewrite]]||Version 1.2 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|-[[mod_ruby]]||Version 1.3 and newer||Third-Party Extension||Shugo Maeda ||[[Apache License]], Version 2.0 ||Embeds a [[Ruby (programming language)|Ruby]] interpreter in Apache
|-
|[http://www.modsecurity.org/ mod_security]|| ||Third-party module ||[[Trustwave]] [[SpiderLabs]] ||[[Apache License]], Version 2.0|| Native implementation of the web application firewall, working as an Apache module. Both major Apache branches are supported. 
|-
|mod_setenvif||Version 1.3 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_setenvifplus||Version 2.2 and newer||Third-party module||Pascal Buchbinder||[[Apache License]], Version 2.0||Allows Apache to set [[Environment variable]]s based on different parts of a request parsed by [[regular expressions]]&lt;ref&gt;{{cite web|url=http://modsetenvifplus.sourceforge.net/ |title=mod_setenvifplus |author=Pascal Buchbinder |publisher=[[SourceForge|SourceForge.net]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
|-
|mod_so||Version 1.3 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_speling||Version 1.3 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Attempts to correct mistaken URLs that users might have entered by ignoring capitalization and by allowing up to one misspelling
|-
|[[mod_ssl]]||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_sslcrl||Version 2.2||Third-party module||Pascal Buchbinder||[[Apache License]], Version 2.0||Verifies the validity of client certificates against Certificate Revocation Lists (CRL) &lt;ref&gt;{{cite web|url=http://modsslcrl.sourceforge.net/|title=mod_sslcrl |author=Pascal Buchbinder |publisher=[[SourceForge|SourceForge.net]] |date= |accessdate=2012-11-18}}&lt;/ref&gt;
|-
|mod_sspi|| || || || ||
|-
|mod_status||Version 1.1 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Provides information on server activity and performance
|-
|mod_substitute||Version 2.2.7 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Perform search and replace operations on response bodies
|-
|[[suEXEC|mod_suexec]]||Version 2.0 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Allows users to run [[Common Gateway Interface|CGI]] and [[Server Side Includes|SSI]] applications as a different user
|-
|mod_suphp|| || || || ||
|-
|mod_throttle|| || || || ||
|-
|mod_tidy|| || || || ||
|-
|mod_tile|| || || || ||
|-
|http://www.outoforder.cc/projects/apache/mod_transform/|mod_transform|| || || || ||Filter module that allows Apache to do dynamic XSL Transformations on either static XML documents, or XML documents generated from another Apache module or CGI program.
|-
|mod_unique_id||Version 1.3 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_upload|| || || || ||
|-
|mod_uploader|| || || || ||
|-
|[http://httpd.apache.org/docs/2.2/mod/mod_userdir.html mod_userdir]||Version 1.3 and newer||Included by Default||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Allows user-specific directories to be accessed using the http://example.com/~user/ syntax.
|-
|mod_usertrack||Version 1.2 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||
|-
|mod_version||Version 2.0.56 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Allows version dependent configuration with the container &lt;IfVersion&gt;
|-
|[http://httpd.apache.org/docs/2.2/mod/mod_vhost_alias.html mod_vhost_alias]||Version 1.37 and newer||Stable Extension||[[Apache Software Foundation]]||[[Apache License]], Version 2.0||Creates dynamically configured virtual hosts, by allowing the IP address and/or the Host: header of the HTTP request to be used as part of the pathname to determine what files to serve.
|-
|mod_virgule|| || || || ||
|-
|mod_vmd|| || || || ||
|-
|mod_wl_20|| || || || ||
|-
|mod_wl_22|| || || || ||
|-
|[[mod_wsgi]]|| || || || ||
|-
|mod_xsendfile|| || || || ||
|-
|http://apache.webthing.com/mod_xml2enc/|mod_xml2enc|| || || || ||Transcoding module that can be used to extend the internationalisation support of libxml2-based filter modules by converting encoding before and/or after the filter has run. Thus an unsupported input charset can be converted to UTF-8, and output can also be converted to another charset if required.
|-
|mod_xml|| || || || ||
|-
|mod_xslt|| || || || ||
|-
|mod_xml_curl|| || || || ||
|-
|mod_xmlrpc|| || || || ||
|-
|mod_xrv|| || || || ||
|-
|mod_zlib|| || || || ||
|}

&lt;ref&gt;{{cite web|url=http://h264.code-shop.com/trac/wiki/Mod-H264-Streaming-Apache-Version2 |title=Mod-H264-Streaming-Apache-Version2 – h264 |publisher=H264.code-shop.com |date= |accessdate=2011-08-19}}&lt;/ref&gt;
&lt;ref&gt;{{cite web|url=http://httpd.apache.org/docs/1.3/mod/ |title=Module Index - Apache HTTP Server |work=Apache HTTP Server 1.3 Documentation |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
&lt;ref&gt;{{cite web|url=http://httpd.apache.org/docs/2.0/mod/ |title=Module Index - Apache HTTP Server |work=Apache HTTP Server 2.0 Documentation |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-19}}&lt;/ref&gt;
&lt;ref&gt;{{cite web|url=http://httpd.apache.org/docs/2.2/mod/ |title=Module Index - Apache HTTP Server |work=Apache HTTP Server 2.2 Documentation |publisher=[[Apache Software Foundation]] |date= |accessdate=2011-08-27}}&lt;/ref&gt;
&lt;ref&gt;{{cite web|url=http://www.modsecurity.org/ |title=Open Source Web Application Firewall |publisher=ModSecurity |date= |accessdate=2011-08-19}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=docs-dev (at) perl.apache.org |url=http://perl.apache.org/ |title=mod_perl: Welcome to the mod_perl world |publisher=Perl.apache.org |date= |accessdate=2011-08-19}}&lt;/ref&gt;
&lt;ref&gt;{{cite web|url=http://www.modpython.org/ |title=Mod_python - Apache/Python Integration |publisher=Modpython.org |date=2010-10-28 |accessdate=2011-08-19}}&lt;/ref&gt;

==References==
{{Reflist}}

{{DEFAULTSORT:List Of Apache Modules}}
[[Category:Apache httpd modules| ]]
[[Category:Apache Software Foundation]]</text>
      <sha1>t1aqucy0bczdkdbkpqw2j570rau9x6g</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache MyFaces</title>
    <ns>0</ns>
    <id>7449978</id>
    <revision>
      <id>586649396</id>
      <parentid>580177689</parentid>
      <timestamp>2013-12-18T15:15:27Z</timestamp>
      <contributor>
        <username>ChrisGualtieri</username>
        <id>16333418</id>
      </contributor>
      <minor/>
      <comment>/* External links */Remove stub template(s). Page is start class or higher. Also check for and do General Fixes + Checkwiki fixes using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="5087">{{ Infobox Software
| name                   = Apache MyFaces
| logo                   = [[Image:MyFaces logo.jpg|200px]]
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version =  2.1.12
| latest release date    = {{release date|2013|06|06}}&lt;ref&gt;{{cite web | url=https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=10600&amp;version=12324285 | title=Release Notes - MyFaces Core - Version 2.1.12 - HTML format | accessdate=2013-10-08}}&lt;/ref&gt;
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Web Framework]]
| license                = [[Apache License]] 2.0
| website                = http://myfaces.apache.org
}}
'''Apache MyFaces''' is an [[Apache Software Foundation]] project that creates and maintains an [[open-source]] [[JavaServer Faces]] implementation, along with several libraries of JSF components that can be deployed on the core implementation. The project is divided into several sub-projects:

* '''Core''': an implementation of the JSF 1.1, JSF 1.2, JSF 2.0 and JSF 2.1 specification and components as specified by [http://www.jcp.org/en/jsr/detail?id=127 JSR 127], [http://www.jcp.org/en/jsr/detail?id=252 JSR 252] and [http://www.jcp.org/en/jsr/detail?id=314 JSR 314] respectively
* '''Portlet Bridge''': an implementation of the JSF Portlet Bridge specification as specified by [http://www.jcp.org/en/jsr/detail?id=301 JSR 301]
* '''Tomahawk''': a set of JSF components created by the MyFaces development team prior to its donation to Apache
* '''[[MyFaces Trinidad|Trinidad]]''': a set of JSF components contributed to MyFaces by [[Oracle Corporation|Oracle]], where it was known as [[Oracle Application Development Framework|ADF]] Faces
* '''Tobago''': a set of JSF components contributed to MyFaces by [http://www.atanion.com Atanion GmbH]
* '''Orchestra''': a framework used to manage persistence sessions across various scopes
* '''Extensions Validator''': a JSF centric validation framework, which is based on annotations
* '''Others''': As well as these subprojects, MyFaces has a number of smaller subprojects, for example '''MyFaces Commons''' or '''MyFaces Shared'''. These subprojects can be found [http://myfaces.apache.org/otherProjects.html here].

The standard is based on the [[Model–view–controller|Model View Controller]] paradigm, but is also component-based and event-oriented. Different templating languages can be used. In the standard, JSP is used, other options include XML based templating languages like Shale Clay or Facelets.&lt;ref&gt;http://myfaces.apache.org/jsfintro.html&lt;/ref&gt;

==Core==

The MyFaces Core subproject is an [[open-source]] implementation of the [[JavaServer Faces]] specifications 1.1, 1.2, 2.0 and 2.1. The latest version 2.1.12 was released on 2013-06-06.&lt;ref&gt;{{cite web|url=http://myfaces.apache.org|title=Apache MyFaces Website|accessdate=October 8, 2013| archiveurl= http://web.archive.org/web/20130806010024/http://myfaces.apache.org/| archivedate= 6 August 2013 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; Each major release of MyFaces Core is certified against the Sun [[Technology Compatibility Kit]] to ensure compliance.&lt;ref&gt;{{cite web|url=http://myfaces.apache.org/core12/index.html|title=Apache MyFaces Core 1.2 Description|accessdate=December 7, 2009| archiveurl= http://web.archive.org/web/20091213150811/http://myfaces.apache.org/core12/index.html| archivedate= 13 December 2009 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

The core subproject is divided into two submodules:

* '''MyFaces API''' implements all of the classes that are defined in the specification.
* '''MyFaces Impl''' provides &quot;invisible&quot; support classes that user code does not directly invoke, but which is needed for a working JSF framework. Examples are the renderer classes for the standard JSF components.

These two submodules are distributed in two libraries, myfaces-api.jar and myfaces-impl.jar. Both of them are needed to be able to deploy a JSF based web application.

The latest release of MyFaces Core is 2.1.12. It requires [[Java (programming language)|Java]] 1.5 or later, [[JavaServer Pages|JSP]] 2.1, [[JavaServer Pages Standard Tag Library|JSTL]] 1.2 and a Java [[Servlet]] 2.5 implementation.&lt;ref&gt;{{cite web|url=http://myfaces.apache.org/core21/index.html|title=Apache MyFaces Core 2.1 Description|accessdate=2013-10-08}}&lt;/ref&gt;

==See also==
{{Portal|Free software}}
* [[JavaServer Faces]]
* [[Facelets]]

==References==
{{reflist}}

==External links==
* [http://myfaces.apache.org Apache MyFaces website]
* [http://myfaces.apache.org/trinidad/index.html Apache Trinidad (former Oracle ADF Faces)]
* [http://myfaces.apache.org/tobago/index.html Apache Tobago]
* [http://myfaces.apache.org/extensions/validator/ Apache MyFaces Extensions Validator]

{{apache}}

{{DEFAULTSORT:Myfaces}}
[[Category:JavaServer Faces]]
[[Category:Apache Software Foundation]]
[[Category:Java enterprise platform]]</text>
      <sha1>dqlnuxul0shnlon8p903590gsx3mdl1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Sling</title>
    <ns>0</ns>
    <id>19060939</id>
    <revision>
      <id>514321006</id>
      <parentid>424726764</parentid>
      <timestamp>2012-09-24T14:08:51Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>clean up, References after punctuation per [[WP:REFPUNC]] and [[WP:PAIC]] using [[Project:AWB|AWB]] (8434)</comment>
      <text xml:space="preserve" bytes="3841">{{ Infobox Software
| name                   = Apache Sling
| logo                   = [[Image:Apache Sling Logo.png]]
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 6
| latest release date    = {{Release date|2011|04|18}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Web application framework]]
| license                = [[Apache License]] 2.0
| website                = http://sling.apache.org
}}
'''Apache Sling''' is an [[open source]] [[Web framework]] for the [[Java platform]] designed to create content-centric applications on top of a  [[JSR-170]]-compliant (aka JCR) [[content repository]] such as [[Apache Jackrabbit]].&lt;ref&gt;http://www.infoworld.com/article/08/06/27/Apache-looks-to-bring-fun-back-to-Java_1.html&lt;/ref&gt; Apache Sling allows developers to deploy their application components as [[OSGi]] bundles or as scripts and templates in the content repository. Supported scripting languages are [[JavaServer Pages|JSP]], server-side [[JavaScript]], [[Ruby (programming language)|Ruby]], [[Apache Velocity|Velocity]]. The goal of Apache Sling is to expose content in the content repository as [[Http|HTTP]] resources, fostering a [[Representational State Transfer|RESTful]] style of application architecture.

Sling is different from a lot of other [[Web application framework]]s in the
sense that it truly focuses on the web aspect of the &quot;web application&quot; development and 
through its development paradigm suggests an intuitive [[Representational State Transfer|RESTful]] 
development of a true web application.
Other frameworks focus more on the application development and therefore are ideal extensions to Sling.
.&lt;ref&gt;http://dev.day.com/microsling/content/blogs/main/slingspringspling.html&lt;/ref&gt; 

The Sling project was started on August 27, 2007,&lt;ref&gt;http://markmail.org/message/67zkwcxzwgnbfjjz&lt;/ref&gt; when [[Day Software]] proposed to donate the source base of its internal web framework powering the Day Communiqué WCM to the [[Apache Software Foundation]] The project was accepted to the [[Apache Incubator]] with [[Apache Jackrabbit]] being the sponsoring project.  On June 18, 2009 &lt;ref&gt;http://markmail.org/message/elioenbv2wid55c5&lt;/ref&gt; the project graduated as Apache top-level project.

==Features==
* Content resolution that maps a request [[Uniform Resource Locator|URL]] to a content node in the content repository
* Servlet resolution that maps a content node and a request method to a Servlet handling the request
* Default servlets supporting [[WebDAV]], content creation from web forms and [[JSON]] representation
* A Javascript client library, allowing access to the content repository through [[AJAX]]
* Support for server-side scripting with [[Javascript]], [[JavaServer Pages|JSP]], [[Ruby (programming language)|Ruby]], [[Apache Velocity|Velocity]] and [[Scala (programming language)|Scala]]
* [[OSGi]]-based extensibility through [[Apache Felix]] - the Felix Web Console was originally developed by the Apache Sling project

==See also==
* [[Apache Jackrabbit]]
* [[Apache Felix]]

== References ==
{{Reflist|1}}

==External links==
* {{official website|http://sling.apache.org/}}
*{{google video|4185433226971588276|Apache Sling}} by [[David Nüscheler]]
**[http://www.slideshare.net/uncled/webtuesday-zurich?src=embed David Nüscheler's presentation]
*[http://dev.day.com/content/docs/en/cq/current/developing/sling_cheatsheet/_jcr_content/par/image.img.png Apache Sling Cheat Sheet]
{{apache}}

[[Category:Apache Software Foundation|Sling]]
[[Category:Java enterprise platform]]
[[Category:Web application frameworks]]</text>
      <sha1>a6eseo8rcd5lwlfdemg7dgxhs0kikx1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Continuum</title>
    <ns>0</ns>
    <id>9605075</id>
    <revision>
      <id>595320046</id>
      <parentid>595306260</parentid>
      <timestamp>2014-02-13T16:51:55Z</timestamp>
      <contributor>
        <username>Gaijin42</username>
        <id>1414762</id>
      </contributor>
      <comment>not a how to</comment>
      <text xml:space="preserve" bytes="1833">{{multiple issues|
{{notability|Products|date=February 2014}}
{{primary sources|date=February 2014}}
{{ref improve|date=February 2014}}
}}
{{Infobox software
| name                   = Apache Continuum
| logo                   =
| screenshot             =
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version = 1.4.1 
| latest release date    = {{release date|2013|01|07}}
| latest preview version = 1.4.0 (Beta)
| latest preview date    = {{release date|2010|05|06}}
| operating system       = [[Cross-platform]] 
| programming language   = [[Java (programming language)|Java]] 
| genre                  = [[Continuous integration]] 
| license                = [[Apache License|Apache 2.0 licence]] 
| website                = {{URL|http://continuum.apache.org/}}
}}
'''Apache Continuum''', a partner to [[Apache Maven]], is a [[continuous integration]] server, which runs builds on a configurable schedule.&lt;ref name=Ching2009&gt;{{cite web|last=Brett Porter, Maria Odea Ching|title=Apache Continuum: Ensuring the Health of your Source Code|url=http://www.packtpub.com/article/apache-continuum-ensuring-health-of-source-code-part2|publisher=Packt|accessdate=13 February 2014}}&lt;/ref&gt;&lt;ref name=Smart2006&gt;{{cite web|last=Smart|first=John Ferguson|title=Continuous Integration with Continuum|url=https://today.java.net/pub/a/today/2006/05/30/continuous-integration-with-continuum.html|publisher=Java.net|accessdate=13 February 2014}}&lt;/ref&gt;  Much like [[CruiseControl]], Continuum emails developers when the build is broken, requesting that the culprit fix the problem. 

==References==
{{reflist}}


{{apache}}
{{software-stub}}

[[Category:Compiling tools]]
[[Category:Java development tools]]
[[Category:Continuous integration]]
[[Category:Apache Software Foundation|Continuum]]</text>
      <sha1>9evgw8ijbo1kgk8qcwdn462aum3jjdn</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Empire-db</title>
    <ns>0</ns>
    <id>23503030</id>
    <revision>
      <id>562084697</id>
      <parentid>557542436</parentid>
      <timestamp>2013-06-29T08:43:52Z</timestamp>
      <contributor>
        <username>David Gerard</username>
        <id>36389</id>
      </contributor>
      <comment>not really adlike - this is just a very tedious and enterprisey area of IT</comment>
      <text xml:space="preserve" bytes="7952">{{Unreferenced|date=May 2011}}

{{ Infobox Software
| name                   = Apache Empire-db
| logo                   = [[File:Empire-db.gif|250px]]
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = incubation
| latest release version = 2.2.0 
| latest release date    = {{release date|2011|11|02}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Persistence Framework]]
| license                = [[Apache License]] 2.0
| website                = {{url|http://empire-db.apache.org/}}
}}

'''Apache Empire-db''' is a Java library that provides a high level object oriented API for accessing [[Relational database management system]]s (RDBMS) through [[JDBC]]. Apache Empire-db is Open Source and provided under the Apache 2.0 license from the [[Apache Software Foundation]].

Compared to [[Object-relational mapping]] (ORM) or other data persistence solutions such as [[Hibernate (Java)|Hibernate]], [[iBATIS]] or [[TopLink]] Empire-db does not use XML files or Java annotations to provide a mapping of plain (old) java object ([[POJO]]'s) to database tables, views and columns. Instead Empire-db uses a Java object model to describe the underlying data model and an API that works almost solely with object references rather than string literals.

Empire-db's aim is to provide better software quality and improved maintainability through increased compile-time safety and reduced redundancy of metadata. Additionally applications may benefit from better performance due to full control over [[SQL]] statements and their execution by the developer compared to most OR-Mapping solutions.

== Major benefits ==

Empire-db’s key strength is its API for dynamic SQL generation for arbitrary select, update, insert or delete statements, purely by using Java methods which reference the model objects. This provides type-safety and almost entirely eliminates the use of string literals for names or expressions in code. Additionally DBMS independence is achieved through a pluggable driver model.

Using references to table and column objects significantly improves compile-time safety and thus reduces the amount of testing. As a positive side effect the IDE’s code completion can be used to browse the data model, increases productivity and eliminates the need for other external tools or IDE-plugins.

Further the object model also provides safe and easy access to meta-information of the data model such as field data type, maximum field length, whether a field is mandatory and a finite choice of options for a field’s values. Metadata is user-extensible and not limited to DBMS related metadata. Availability of meta-information encourages more generic code and eliminates redundancies throughout application layers.

== Features at a glance ==

* Data model definition through a Java object model omits the need to learn XML schemas or annotations and easily allows user interceptions and extensions.
* Portable RDBMS independent record handling and command definition with support for a variety of relational databases such as [[Oracle Database|Oracle]], [[Microsoft SQL Server]], [[MySQL]], [[Apache Derby|Derby]], [[H2 (DBMS)|H2]] and [[HSQLDB]] (as of version 2.0.5)
* DDL generation for target DBMS from object definition, either for the entire database or for individual objects such as tables, views, columns and relations.
* Type-safe API for dynamic SQL command generation allows dynamic building of SQL statements using API methods and object references only instead of string literals. This provides a high degree of type-safety which simplifies testing and maintenance.
* Reduced amount of Java code and powerful interception of field and metadata access through dynamic beans as an alternative to POJOs. This even allows data model changes (DDL) at runtime.
* Automatic tracking of record state and field modification (aka &quot;dirty checking&quot;) to only insert/ update modified fields.
* Support for optimistic locking through timestamp column.
* No need to always work with full database entities. Build queries to provide you with the data exactly as you need it, and obtain the result for example as a list of any type of POJO with matching property setters or constructor.
* Lightweight and passive library with zero configuration footprint that allows simple integration with any architecture or framework.

== Example ==

As an example consider a database with two tables called ''Employees'' and ''Departments'' for which a list of employees in a particular format, with certain constraints and a given order should be retrieved.

The corresponding Oracle syntax SQL statement is assumed to be as follows:

&lt;source lang=&quot;sql&quot;&gt;
    SELECT t1.EMPLOYEE_ID, 
           t1.LASTNAME || ', ' || t1.FIRSTNAME AS NAME, 
           t2.DEPARTMENT
    FROM (EMPLOYEES t1 
          INNER JOIN DEPARTMENTS t2 ON t1.DEPARTMENT_ID = t2.DEPARTMENT_ID)
    WHERE upper(t1.LASTNAME) LIKE upper('Foo%') 
      AND t1.RETIRED=0
    ORDER BY t1.LASTNAME, t1.FIRSTNAME
&lt;/source&gt;

This SQL statement can be created using Empire-db's command API using object model references like this:

&lt;source lang=&quot;java&quot;&gt;
    SampleDB db = getDatabase();
    // Declare shortcuts (not necessary but convenient)
    SampleDB.Employees EMP = db.EMPLOYEES;
    SampleDB.Departments DEP = db.DEPARTMENTS;
    // Create a command object
    DBCommand cmd = db.createCommand();
    // Select columns
    cmd.select(EMP.EMPLOYEE_ID);
    cmd.select(EMP.LASTNAME.append(&quot;, &quot;).append(EMP.FIRSTNAME).as(&quot;NAME&quot;));
    cmd.select(DEP.DEPARTMENT);
    // Join tables
    cmd.join  (EMP.DEPARTMENT_ID, DEP.DEPARTMENT_ID);
    // Set constraints
    cmd.where(EMP.LASTNAME.likeUpper(&quot;Foo%&quot;));
    cmd.where(EMP.RETIRED.is(false));
    // Set order
    cmd.orderBy(EMP.LASTNAME);
    cmd.orderBy(EMP.FIRSTNAME);
&lt;/source&gt;

In order to execute the query and retrieve a list of POJO's holding the query result the following code may be used:

&lt;source lang=&quot;java&quot;&gt;
    // Class definition for target objects
    public class EmployeeInfo {
        private int employeeId;
        private String name;
        private String department;
        // Getter's and Setters for all properties
        // or a public Constructor using fields
        public get...
        public set...
    }

    // Retrieve employee list using the cmd object created above
    DBReader reader = new DBReader();
    try {
      reader.open(cmd, getConnection());
      List&lt;EmployeeInfo&gt; empList = reader.getBeanList(EmployeeInfo.class);
    } finally {
      reader.close()
    }
&lt;/source&gt;

Empire-db also supports field access through object references or obtaining query results as XML.

== History ==

Empire-db was originally developed at ESTEAM Software a German software development company which used Empire-db to develop various applications for a variety of different branches.

In January 2008 Empire-db was made officially Open Source and first published though SourceForge.Net. 

In June 2008 a proposal was submitted to the Apache Software Foundation for Empire-db to become an Apache Incubator project. In July 2008 Empire-db got accepted for incubation and all rights over the Software were transferred to the Apache Foundation.

In October 2008 Empire-db 2.0.4 was the first official Apache incubator release with all package names changed to begin with org.apache.empire.

==See also==
{{Portal|Java}} 
*[[Java Database Connectivity]]
*[[Object-relational mapping]] 
*[[Hibernate (Java)|Hibernate]]
*[[iBATIS]]
*[[TopLink]]
*[[Apache Struts]]

==External links ==
*[http://incubator.apache.org/empire-db Empire-db project page at the Apache Foundation]  
{{apache}}

[[Category:Apache Software Foundation|Empire-db]]
[[Category:Computer languages]]</text>
      <sha1>l1rqgd5rw05xctg4hostak0f0w4igyf</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Jelly</title>
    <ns>0</ns>
    <id>20905300</id>
    <revision>
      <id>601223518</id>
      <parentid>593407658</parentid>
      <timestamp>2014-03-25T17:43:13Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <minor/>
      <comment>changed {{Notability}} to {{Notability|Products}} &amp; [[WP:AWB/GF|general fixes]] using [[Project:AWB|AWB]] (10002)</comment>
      <text xml:space="preserve" bytes="2248">{{multiple issues|
{{refimprove|date=January 2009}}
{{notability|Products|date=February 2014}}
}}

{{ Infobox Software
| name                   = Apache Jelly
| logo                   = 
| screenshot             =
| caption                =
| founder                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version =
| latest release date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = 
| license                = [[Apache License]] 2.0
| website                = http://commons.apache.org/jelly
}}
'''Apache Jelly''' is a [[Java (programming language)|Java]] and [[XML]] based scripting and processing engine for turning [[XML]] into executable code.&lt;ref&gt;http://commons.apache.org/jelly/&lt;/ref&gt; Jelly is a component of [[Apache Commons]].

Custom XML languages are commonly created to perform some kind of processing action. Jelly is intended to provide a simple XML based processing engine that can be extended to support various custom actions.&lt;ref&gt;http://commons.apache.org/jelly/overview.html&lt;/ref&gt;

==Use by Clarity Software==
Clarity PPM Software, a product of [[CA Technologies, Inc.]] uses Jelly and an additional custom tag library extensively in the implementation of its XML Open Gateway application architecture. The Clarity language is known as GEL (Generic Execution Language) and is a scripting language that is based on the Jelly libraries.

The following example shows how Clarity implements the classical &quot;[[Hello World]]&quot; application.&lt;ref&gt;https://support.ca.com/cadocs/1/m000471e.pdf Clarity Integration Guide 8.1&lt;/ref&gt; 
&lt;source lang=&quot;xml&quot;&gt;
&lt;gel:script xmlns:j=&quot;jelly:core&quot; xmlns:gel=&quot;jelly:com.niku.union.gel.GELTagLibrary&quot;&gt;
  &lt;j:forEach indexVar=&quot;i&quot; begin=&quot;1&quot; end=&quot;3&quot;&gt;
    &lt;gel:out&gt;Hello World ${i}!&lt;/gel:out&gt;
  &lt;/j:forEach&gt;
&lt;/gel:script&gt;
&lt;/source&gt;

==See also==
* [[Apache Ant]]
* [[Apache Commons]]
* [[Apache Maven]]

== References ==
&lt;references/&gt;

== External links ==
*[http://commons.apache.org/jelly/ Apache Jelly]
*[http://commons.apache.org/ Apache Commons Homepage]
{{Apache}}

[[Category:Java platform]]
[[Category:Apache Software Foundation|Jelly]]</text>
      <sha1>168fqk63l2egtokah3s493q767qxrrn</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Mahout</title>
    <ns>0</ns>
    <id>18706674</id>
    <revision>
      <id>593692607</id>
      <parentid>589096703</parentid>
      <timestamp>2014-02-03T06:26:11Z</timestamp>
      <contributor>
        <username>Smarthi</username>
        <id>1077571</id>
      </contributor>
      <text xml:space="preserve" bytes="3478">{{Infobox Software
| name                   = Apache Mahout
| logo                   = 
| screenshot             = 
| caption                = 
| collapsible            = yes
| developer              = [[Apache Software Foundation]] 
| status                 = Active
| latest release version = 0.9
| latest release date    = {{release date|2014|02|01|df=yes}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| size                   = 
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[machine learning]]
| license                = [[Apache License|Apache 2.0 Licence]] 
| website                = {{url|http://mahout.apache.org}}
}}
'''Apache Mahout''' is a project of the [[Apache Software Foundation]] to produce [[free software|free]] implementations of [[distributed computing|distributed]] or otherwise [[Scalability|scalable]] [[machine learning]] algorithms focused primarily in the areas of [[collaborative filtering]], clustering and classification. Many of the implementations use the Apache [[Hadoop]] platform.&lt;ref&gt;{{cite web |url= http://www.ibm.com/developerworks/java/library/j-mahout/ |title=Introducing Apache Mahout |work=ibm.com |year=2011  |accessdate=13 September 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://www.infoq.com/news/2009/04/mahout |title=InfoQ: Apache Mahout: Highly Scalable Machine Learning Algorithms |work=infoq.com |year=2011 |accessdate=13 September 2011}}&lt;/ref&gt; Mahout also provides Java libraries for common math operations (focused on linear algebra and statistics) and primitive Java collections. Mahout is a work in progress; the number of implemented algorithms has grown quickly,&lt;ref&gt;{{cite web |url= https://cwiki.apache.org/confluence/display/MAHOUT/Algorithms |title=Algorithms - Apache Mahout - Apache Software Foundation|work=cwiki.apache.org |year=2011 |accessdate=13 September 2011}}&lt;/ref&gt; but various algorithms are still missing.

While Mahout's core algorithms for [[Cluster analysis|clustering]], classification and batch based collaborative filtering are implemented on top of Apache Hadoop using the [[map/reduce]] paradigm, it does not restrict contributions to Hadoop based implementations. Contributions that run on a single node or on a non-Hadoop cluster are also welcomed. For example, the 'Taste' collaborative-filtering recommender component of Mahout was originally a separate project and can run stand-alone without Hadoop. Integration with initiatives such as the Pregel-like Giraph are actively under discussion.{{When|date=May 2013}}

==References==
{{Reflist}}

==External links==
* {{Official website|http://mahout.apache.org/}}
* [https://console.aws.amazon.com/ec2/home?region=us-east-1a#launchAmi=ami-95bc7dfc EC2 AMI with Hadoop and Mahout]
* [http://incubator.apache.org/giraph/ Giraph] - a Graph processing infrastructure that runs on Hadoop (see Pregel).
* [http://portal.acm.org/citation.cfm?id=1582723 Pregel] - Google's internal graph processing platform, released details in ACM paper.

==Resources==
* [http://bhagyas.github.io/spring-mahout-demo/ A Spring based Java demo application that demonstrates a simple recommender using Apache Mahout]
* [http://www.nexttovisit.com Demo of travel recommendations using anonymous user-based recommender of Mahout]

{{Apache}}

[[Category:Apache Software Foundation|Mahout]]
[[Category:Hadoop|Mahout]]
[[Category:Data mining and machine learning software]]</text>
      <sha1>0ae4hw5zmty3we08k9t2rnswynvhj7h</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Solr</title>
    <ns>0</ns>
    <id>10443665</id>
    <revision>
      <id>598192435</id>
      <parentid>597948002</parentid>
      <timestamp>2014-03-05T01:12:45Z</timestamp>
      <contributor>
        <username>Vieque</username>
        <id>20650448</id>
      </contributor>
      <minor/>
      <comment>Disambiguated: [[Scala]] → [[Scala (programming language)]]</comment>
      <text xml:space="preserve" bytes="14227">{{Infobox software
| name                   = Solr
| logo                   = [[File:Solr.png|130px|Solr logo]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 4.7.0
| latest release date    = {{release date and age|2014|02|26}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| platform               =
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Search algorithm|Search]] and [[index (search engine)|index]] [[Application programming interface|API]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://lucene.apache.org/solr/}}
}}

'''Solr''' (pronounced &quot;solar&quot;) is an [[open source]] [[enterprise search]] platform from the '''Apache [[Lucene]]''' project. Its major features include [[Full text search|full-text search]], hit highlighting, [[faceted search]], dynamic clustering, database integration, and rich document (e.g., Word, PDF) handling. Providing distributed search and index replication, Solr is highly scalable.&lt;ref&gt;[http://lucene.apache.org/solr/#intro What is Solr?]&lt;/ref&gt; Solr is the most popular enterprise search engine.&lt;ref&gt;[http://db-engines.com/en/ranking/search+engine DB-Engines Ranking of Search Engines]&lt;/ref&gt; Solr 4 adds [[NoSQL]] features.&lt;ref&gt;http://searchhub.org/2012/05/21/solr-4-preview/&lt;/ref&gt;

Solr is written in [[Java (programming language)|Java]] and runs as a standalone full-text search server within a [[servlet]] container such as [[Apache Tomcat]] or [[Jetty (web server)|Jetty]]. Solr uses the [[Lucene]] Java search library at its core for full-text indexing and search, and has [[REST]]-like [[HTTP]]/[[XML]] and [[JSON]] APIs that make it usable from most popular programming languages. Solr's powerful external configuration allows it to be tailored to many types of application without Java coding, and it has a plugin architecture to support more advanced customization.

Apache [[Lucene]] and Apache Solr are both produced by the same Apache Software Foundation development team since the two projects were merged in 2010.  It is common to refer to the technology or products as Lucene/Solr or Solr/Lucene.

== History ==
In 2004, Solr was created by Yonik Seeley at [[CNET Networks]] as an in-house project to add search capability for the company website. Yonik Seeley along with Grant Ingersoll and Erik Hatcher went on to launch [[LucidWorks]] (formerly Lucid Imagination), a company providing commercial support, consulting and training for Apache Solr search technologies.

In January 2006, CNET Networks decided to openly publish the source code by donating it to the [[Apache Software Foundation]] under the Lucene top-level project.&lt;ref&gt;[http://issues.apache.org/jira/browse/SOLR-1 Source code that CNET is granting to the ASF for the Solr project]&lt;/ref&gt; Like any new project at [[Apache Software Foundation]] it entered an incubation period which helped solve organizational, legal, and financial issues.

In January 2007, Solr graduated from incubation status and grew steadily with accumulated features, thereby attracting a robust community of users, contributors, and committers. Although quite new as a public project, it powers several high-traffic websites.&lt;ref&gt;[http://wiki.apache.org/solr/PublicServers Public Websites that use Solr]&lt;/ref&gt;

In September 2008, Solr 1.3 was released with many enhancements including distributed search capabilities and performance enhancements among many others.&lt;ref&gt;[http://lucene.apache.org/solr/#15+September+2008+-+Solr+1.3.0+Available Solr 1.3 Announcement]&lt;/ref&gt;

November 2009 saw the release of Solr 1.4 This version introduces enhancements in indexing, searching and faceting along with many other improvements such as Rich Document processing ([[PDF]], [[Microsoft Word|Word]], [[HTML]]), Search Results clustering based on [[Carrot2]] and also improved database integration. The release also features many additional plug-ins.&lt;ref&gt;[http://lucene.apache.org/solr/#10+November+2009+-+Solr+1.4+Released Solr 1.4 Announcement]&lt;/ref&gt;

In March 2010, the [[Lucene]] and Solr projects merged.  Separate downloads will continue, but the products are now jointly developed by a single set of committers.

In 2011 the Solr version number scheme was changed in order to match that of Lucene. After Solr 1.4, the next release of Solr was labeled 3.1, in order to keep Solr and Lucene on the same version number.&lt;ref&gt;[http://wiki.apache.org/solr/Solr3.1 Solr3.1 - Solr Wiki]. Wiki.apache.org (2013-05-16). Retrieved on 2013-07-21.&lt;/ref&gt;

In October 2012 Solr version 4.0 was released, including the new SolrCloud feature.&lt;ref&gt;[http://lucene.apache.org/solr/solrnews.html Apache Lucene]. Lucene.apache.org. Retrieved on 2013-07-21.&lt;/ref&gt;

2013 has seen a number of Solr releases already, including 4.1 in January, 4.2 in March, 4.2.1 in April and 4.3.0 in May, including many improvements in the Solr Cloud features such as custom sharding.

== Features ==
* Uses the Lucene library for full-text search
* [[Faceted search|Faceted navigation]]
* Hit highlighting
* Query language supports structured as well as textual search
* [[JSON]], [[XML]], [[PHP]], [[Ruby (programing language)|Ruby]], [[Python (programming language)|Python]], [[XSLT]], [[Apache Velocity|Velocity]] and custom Java binary output formats over HTTP
* HTML administration interface
* Replication to other Solr servers - enables scaling [[Queries per second|QPS]]
* Distributed Search through [[Sharding]] - enables scaling content volume
* Search results clustering based on [[Carrot2]]
* Extensible through plugins
* Flexible relevance - boost through function queries
* Caching - queries, filters, and documents
* Embeddable in a Java Application
* Geo-spatial search
* Automated management of large clusters through [[Apache ZooKeeper|ZooKeeper]]
* More function queries
* Field Collapsing  &lt;ref&gt;[http://www.lucidimagination.com/blog/2010/09/16/2446/ Solr Result Grouping]&lt;/ref&gt;
* A new auto-suggest component

== Community and future ==
Solr has an active development community, both individuals and companies, who contribute new features and bug fixes. 
&lt;ref&gt;[http://stackoverflow.com/questions/tagged/solr?sort=votes StackOverflow questions tagged solr]&lt;/ref&gt;
&lt;ref&gt;[http://lucenerevolution.org/ lucenerevolution.org]&lt;/ref&gt;
&lt;ref&gt;[http://www.meetup.com/SFBay-Lucene-Solr-Meetup/ SF Bay Area Lucene &amp; Solr Meetup Community]&lt;/ref&gt;
&lt;ref&gt;[http://www.meetup.com/Oslo-Solr-Community/ Oslo Solr Community]&lt;/ref&gt;
&lt;ref&gt;[http://www.linkedin.com/groups/Solr-1557747 LinkedIn Solr Group]&lt;/ref&gt;

== Integrating Solr ==

{{Expand section|date=July 2012}}

There are many community projects for integrating Solr with many programming languages and environments.

{|class=&quot;wikitable plainrowheaders sortable&quot;
|-
!scope=col|Language
!scope=col class=unsortable|Name
|-
!scope=row rowspan=3 style=&quot;text-align:center;&quot;|[[Ruby (programming language)|Ruby]]
|align=center|'''Ruby Response Format'''&lt;ref&gt;[http://wiki.apache.org/solr/Ruby%20Response%20Format Ruby Response Format - Solr Wiki]. Wiki.apache.org (2011-11-22). Retrieved on 2013-07-21.&lt;/ref&gt;
|-
|align=center|[[rsolr]]&lt;ref&gt;[https://github.com/mwmitchell/rsolr/ Rsolr: A Ruby client for Apache Solr]&lt;/ref&gt;
|-
|align=center|'''Solr's efficient binary javabin'''&lt;ref&gt;[https://github.com/kennyj/java_bin Apache Solr JavaBin format (binary format) implementation for Ruby]&lt;/ref&gt;
|-
!scope=row rowspan=4 style=&quot;text-align:center;&quot;|[[PHP]]
|align=center|'''Solr's PHP response format'''&lt;ref&gt;[http://wiki.apache.org/solr/SolPHP#Solr.27s_PHP_response_format Solr's PHP response format]&lt;/ref&gt;
|-
|align=center|[[SolrPhpClient]]&lt;ref&gt;[http://code.google.com/p/solr-php-client/downloads/list PHP library for indexing and searching Solr]&lt;/ref&gt;
|-
|align=center|[[Apache Solr PHP Extension]]&lt;ref&gt;[http://pecl.php.net/package/solr Apache Solr PHP Extension]&lt;/ref&gt;
|-
|align=center|Solarium&lt;ref&gt;[http://wiki.solarium-project.org/index.php/Main_Page Solarium: A Solr client library for PHP applications]&lt;/ref&gt;
|-
! scope=&quot;row&quot; style=&quot;text-align:center;&quot;|[[Java (programming language)|Java]]
|align=center|[[SolrJ]]&lt;ref&gt;[http://wiki.apache.org/solr/SolJava SolrJ]&lt;/ref&gt;
|-
! scope=&quot;row&quot; style=&quot;text-align:center;&quot;|[[Scala (programming language)|Scala]]
|align=center|[[scalikesolr]]&lt;ref&gt;[https://github.com/seratch/scalikesolr Apache Solr Client for Scala/Java]&lt;/ref&gt;
|-
!scope=row rowspan=6 style=&quot;text-align:center;&quot;|[[Python (programming language)|Python]]
|align=center|[[solrpy]]&lt;ref&gt;[https://code.google.com/p/solrpy/ solrpy]&lt;/ref&gt;
|-
|align=center|[[PySolr]]&lt;ref&gt;[https://github.com/toastdriven/pysolr PySolr]&lt;/ref&gt;
|-
|align=center|[[insol]]&lt;ref&gt;[http://github.com/mdomans/insol insol]&lt;/ref&gt;
|-
|align=center|sunburnt&lt;ref&gt;[http://pypi.python.org/pypi/sunburnt sunburnt]&lt;/ref&gt;&lt;ref&gt;[http://github.com/tow/sunburnt sunburnt @ GitHub]&lt;/ref&gt;
|-
|align=center|[[txSolr]]&lt;ref&gt;[https://launchpad.net/txsolr txSolr]&lt;/ref&gt;
|-
|align=center|[[mySolr]]&lt;ref&gt;[http://mysolr.redtuna.org/ mySolr]&lt;/ref&gt;
|-
! scope=&quot;row&quot; style=&quot;text-align:center;&quot;|[[JSON]]
|align=center|'''JSON Query Response Format'''&lt;ref&gt;[http://wiki.apache.org/solr/SolJSON#JSON_Query_Response_Format JSON Query Response Format]&lt;/ref&gt;
|-
!scope=row rowspan=5 style=&quot;text-align:center;&quot;|[[.NET Framework|.NET]]
|align=center|[[SolrSharp]] ([[C Sharp (programming language)|C#]])&lt;ref&gt;[http://www.codeplex.com/solrsharp SolrSharp]&lt;/ref&gt;
|-
|align=center|[[SolrContrib]] ([[C Sharp (programming language)|C#]])&lt;ref&gt;[http://solrcontrib.codeplex.com/ SolrContrib]&lt;/ref&gt;
|-
|align=center|[[EasyNet Solr]]&lt;ref&gt;[http://easynet.codeplex.com/wikipage?title=EasyNet%20Solr EasyNet Solr]&lt;/ref&gt;
|-
|align=center|[[Deveel Solr Client]]&lt;ref&gt;[http://code.google.com/p/deveel-solr/ Deveel Solr Client]&lt;/ref&gt;
|-
|align=center|[[SolrNet]]&lt;ref&gt;[http://code.google.com/p/solrnet/ SolrNet]&lt;/ref&gt;
|-
!scope=row rowspan=2 style=&quot;text-align:center;&quot;|[[Perl]]
|align=center|[[SolPerl]]&lt;ref&gt;[http://github.com/bricas/webservice-solr SolPerl]&lt;/ref&gt;
|-
|align=center|[[Solr.pm]]&lt;ref&gt;[https://metacpan.org/module/Solr Solr.pm]&lt;/ref&gt;
|-
!scope=row style=&quot;text-align:center;&quot;|[[JavaScript]]
|align=center|[[AJAX Solr]]&lt;ref&gt;[http://wiki.github.com/evolvingweb/ajax-solr AJAX Solr]&lt;/ref&gt;
|-
|align=center|[[TYPO3]]
|align=center|built-in integration
|}

== See also ==
{{Portal|Free software}}
* [[Search oriented architecture]]
* [[eGranary Digital Library]]
* [[List of information retrieval libraries]]

==References==
{{Reflist|2}}

==Bibliography==
{{Refbegin}}
* {{cite book
| first1      = David
| last1       = Smiley
| first2      = Eric
| last2       = Pugh
| first3      = Kranti
| last3       = Parisa
| first4      = Matt
| last4       = Mitchell
| date        = February, 2014
| title       = Apache Solr 4 Enterprise Search Server 
| publisher   = [[Packt|Packt Publishing]]
| edition     = 1st
| page       = 451 
| isbn        = 9781782161363
| url         = http://www.packtpub.com/apache-solr-4-enterprise/book
}}
* {{cite book
| first1      = Alfredo
| last1       = Serafini
| date        = December, 2013
| title       = Apache Solr Beginner’s Guide
| publisher   = [[Packt|Packt Publishing]]
| edition     = 1st
| page       = 324 
| isbn        = 9781782162520
| url         = http://www.packtpub.com/apache-solr-beginners-guide/book
}}
* {{cite book
| first1      = Alexandre
| last1       = Rafalovitch
| date        = June, 2013
| title       = Instant Apache Solr for Indexing Data How-to 
| publisher   = [[Packt|Packt Publishing]]
| edition     = 1st
| page       = 90 
| isbn        = 9781782164845
| url         = http://www.packtpub.com/apache-solr-for-indexing-data/book
}}
* {{cite book
| first1      = Rafał
| last1       = Kuć
| date        = January, 2013
| title       = Apache Solr 4 Cookbook
| publisher   = [[Packt|Packt Publishing]]
| edition     = 1st
| page       = 328 
| isbn        = 9781782161325
| url         = http://www.packtpub.com/apache-solr-4-cookbook/book
}}
* {{cite book
| first1      = David
| last1       = Smiley
| first2      = Eric
| last2       = Pugh
| date        = November 20, 2011
| title       = Apache Solr 3 Enterprise Search Server 
| publisher   = [[Packt|Packt Publishing]]
| edition     = 1st
| page       = 418 
| isbn        = 1-84951-606-5
| url         = http://www.packtpub.com/apache-solr-3-enterprise-search-server/book
}}
* {{cite book
| first1      = Rafal 
| last1       = Ku
| date        = July 22, 2011
| title       = Apache Solr 3.1 Cookbook
| publisher   = [[Packt|Packt Publishing]]
| edition     = 1st
| page       = 300 
| isbn        = 1-84951-218-3
| url         = http://www.packtpub.com/solr-3-1-enterprise-search-server-cookbook/book
}}
* {{cite book
| first1      = David
| last1       = Smiley
| first2      = Eric
| last2       = Pugh
| date        = August 19, 2009
| title       = Solr 1.4 Enterprise Search Server
| publisher   = [[Packt|Packt Publishing]]
| edition     = 1st
| page       = 336
| isbn        = 1-84719-588-1
| url         = http://www.packtpub.com/solr-1-4-enterprise-search-server
}}
{{Refend}}

==External links==
* [http://lucene.apache.org/solr Solr homepage]
* [http://lucene.apache.org/solr/tutorial.html Solr tutorial]
* [http://wiki.apache.org/solr Solr wiki]
* [http://svn.apache.org/repos/asf/lucene/dev/trunk/solr/example/exampledocs/ Apache Solr Test Files]
* [http://www.xml.com/pub/a/2006/08/09/solr-indexing-xml-with-lucene-andrest.html Solr: Indexing XML with Lucene and REST]
* [http://www.ibm.com/developerworks/library/j-solr1/index.html?S_TACT=105AGX44&amp;S_CMP=EDU Search smarter with Apache Solr, Part 1]
* [http://www.ibm.com/developerworks/library/j-solr2/index.html?S_TACT=105AGX44&amp;S_CMP=EDU Search smarter with Apache Solr, Part 2]
* [http://www.ibm.com/developerworks/java/library/j-solr-update/ What's new with Apache Solr]
* http://darughachi.blogspot.com/2013/11/1-2-3-to-integrate-apache-nutch-1.html
{{Apache}}

[[Category:Apache Software Foundation|Solr]]
[[Category:Free software programmed in Java]]
[[Category:Free search engine software]]
[[Category:Search engine software]]</text>
      <sha1>6ar8qv0y3rq3ut8kmrz5u2fxr8wyaze</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Tuscany</title>
    <ns>0</ns>
    <id>9073093</id>
    <revision>
      <id>569771673</id>
      <parentid>565623934</parentid>
      <timestamp>2013-08-22T20:24:58Z</timestamp>
      <contributor>
        <ip>15.211.201.82</ip>
      </contributor>
      <text xml:space="preserve" bytes="3110">{{Confusing|date=May 2008}}
{{Infobox Software 
| name                   = Apache Tuscany
| logo                   =
| screenshot             = 
| caption                =  
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = SCA Java 1.6.2 (April 2011), SCA Java 2.0-Beta2 (February 2011), SDO Java 1.1.1 (July 2008), DAS Java 1.0-incubating-beta2 (Oct 2007), SCA Native Incubator-M3 (May 2007) 
| operating system       = [[Cross-platform]] 
| programming language   = [[C++]] and [[Java (programming language)|Java]]
| genre                  = [[Service-oriented architecture|SOA]]
| license                = [[Apache License]] 2.0
| website                = http://tuscany.apache.org/
}}
'''Apache Tuscany''' provides a [[service-oriented architecture]] (SOA) and the infrastructure for easily developing and running applications using a service-oriented approach. This lightweight runtime is designed to be embedded in, or provisioned to, a number of different host environments. Apache Tuscany implements [[Service component architecture]] (SCA) which defines a flexible, service-based model for construction, assembly and deployment of network of services (existing and new ones). 

With SCA as its foundation, Tuscany reduces the cost of developing SOA based solutions because it pushes handling of protocol out of the application business logic into pluggable bindings. As a result, protocols can be changed at only one time with minimal configuration changes. Tuscany also removes the need for applications to deal with infrastructure concerns such as security and transaction and handles this declaratively. This enables SOA solutions to be flexible and adaptable to change with minimal configuration changes. 

Tuscany provides support for SCA 1.0 specification in Java. It also provides a wide range of bindings (web services, web20 bindings, etc.), implementation types (Spring, BPEL, Java, etc.) as well as integration with technologies such as web20 and [[OSGi]]. Tuscany is working on implementing SCA 1.1 that is being standardized at OASIS.

Apache Tuscany also implements [[Service Data Objects]] (SDO) which provides a uniform interface for handling different forms of data, including XML documents, that can exist in a network of services and provides the mechanism for tracking changes. Tuscany supports the SCO and the SDO (2.01 for C++ / 2.1 for Java) specification.

== External links ==
* [http://tuscany.apache.org/ Apache Tuscany Project Home Page]
****
* [http://osoa.org/display/Main/Home Open Service Oriented Architecture -- official site for information about the SCA &amp; SDO technology]
* [http://www.oasis-opencsa.org/sca SCA Home Page at OASIS web site]
* [http://www.infoq.com/articles/tuscanysca &quot;Introduction to SCA bindings&quot; from Ch 7 of Tuscany SCA in Action]
{{apache}}

[[Category:Apache Software Foundation|Tuscany]]
[[Category:Enterprise application integration]]
[[Category:Service-oriented architecture-related products]]
[[Category:SCA runtime]]
[[Category:SCA infrastructure]]

{{Soft-eng-stub}}</text>
      <sha1>lz0m8c6vg8dbjtunngdj092h8jht277</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Velocity</title>
    <ns>0</ns>
    <id>2285690</id>
    <revision>
      <id>575004691</id>
      <parentid>556366940</parentid>
      <timestamp>2013-09-29T14:16:17Z</timestamp>
      <contributor>
        <username>Cedar101</username>
        <id>374440</id>
      </contributor>
      <minor/>
      <comment>/* Code example */</comment>
      <text xml:space="preserve" bytes="4702">{{for|the Microsoft project codenamed Velocity|Velocity (memory cache)}}
{{More footnotes|date=March 2010}}
{{Infobox Software
| name                   = Apache Velocity
| logo                   = [[File:Jakarta Velocity Logo.png|Jakarta Velocity Logo]]
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 1.7
| latest release date    = {{release date|mf=yes|2010|11|29}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Template engine (web)|template engine]]
| license                = [[Apache License]] 2.0
| website                = http://velocity.apache.org/
}}
'''Apache Velocity''' is a [[Java (programming language)|Java]]-based [[template engine (web)|template engine]] that provides a [[Web template|template language]] to reference [[object (computer science)|object]]s defined in Java code. It is an [[open source]] software project directed by the [[Apache Software Foundation]] and aims to ensure clean separation between the presentation tier and business tiers in a [[Web application]] (the [[model–view–controller]] design pattern).

== Uses ==
Some common types of applications that use Velocity are:
* [[Web application]]s: [[Web designer]]s create [[HTML]] pages with placeholders for dynamic information. The page is processed with ''VelocityViewServlet'' or any of a number of frameworks which support Velocity.
* [[Source code]] generation: Velocity can be used to generate Java source code, [[SQL]], or [[PostScript]], based on [[Web template|templates]]. A number of [[open source]] and commercial development [[software package (installation)|software packages]] use Velocity in this manner.&lt;ref&gt;{{cite web|url=http://wiki.apache.org/velocity/PoweredByVelocity |title=PoweredByVelocity | work = Velocity Wiki |publisher=Wiki.apache.org |date=2009-12-30 |accessdate=2010-03-29| archiveurl= http://web.archive.org/web/20100323223306/http://wiki.apache.org/velocity/PoweredByVelocity| archivedate= 23 March 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;
* Automatic [[email]]s: Many applications generate automatic emails for account signup, [[password]] reminders, or automatically sent reports. Using Velocity, the email [[web template|template]] can be stored in a [[text file]], rather than directly embedded in Java code.
* [[XML]] transformation: Velocity provides an [[Apache Ant|Ant]] task, called Anakia, which reads an XML file and makes it available to a Velocity template. A common application is to convert [[documentation]] stored in a generic &quot;xdoc&quot; format into a styled HTML document.

== Code example ==
The following [[web template|template]]:
&lt;source lang=&quot;html4strict&quot;&gt;
## Velocity Hello World
&lt;html&gt;
    &lt;body&gt;
       #set( $foo = &quot;Velocity&quot; )
       ## followed by
       Hello $foo World!
    &lt;/body&gt;
&lt;/html&gt;
&lt;/source&gt;

processed by Velocity will produce the following HTML:
&lt;source lang=&quot;html4strict&quot;&gt;
&lt;html&gt;
    &lt;body&gt;
     Hello Velocity World!
    &lt;/body&gt;
&lt;/html&gt;
&lt;/source&gt;

The syntax and overall concept of the Apache Velocity templates is very similar to the syntax of the older [[WebMacro]] template engine which is now also an open source project. {{Citation needed|date=January 2011}}

== See also ==
{{Portal|Free software}}
* [[Apache Torque]]
* [[FreeMarker]]
* [[JavaServer Pages]]
* [[Thymeleaf]]

== References ==
{{reflist}}

== Bibliography ==
{{refbegin}}
*{{citation
| first1     = Rob
| last1      = Harrop
| first2     = Ian
| last2      = Darwin
| date       = August 30, 2004
| title      = Pro Jakarta Velocity: From Professional to Expert
| edition    = 1st
| publisher  = [[Apress]]
| page      = 370
| isbn       = 978-1-59059-410-0
| url        = http://www.apress.com/book/view/9781590594100
}}
*{{citation
| first1     = Jim
| last1      = Cole
| first2     = Joseph
| last2      = D. Gradecki
| date       = July 16, 2003
| title      = Mastering Apache Velocity
| edition    = 1st
| publisher  = [[John Wiley &amp; Sons|Wiley]]
| page      = 372
| isbn       = 978-0-471-45794-7
| url        = http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471457949.html
}}
{{refend}}

== External links ==
* [http://velocity.apache.org/ Velocity at Apache]
* [http://wiki.apache.org/velocity/ Velocity wiki]
* [http://www.javaworld.com/javaworld/jw-11-2007/jw-11-java-template-engines.html Java templates comparison]

{{Apache}}

[[Category:Apache Software Foundation|Velocity]]
[[Category:Java libraries]]
[[Category:Template engines]]

{{programming-software-stub}}</text>
      <sha1>hfsmsgz0wjf8l995no2lptauirzvp0r</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache ZooKeeper</title>
    <ns>0</ns>
    <id>26039352</id>
    <revision>
      <id>601435428</id>
      <parentid>599490461</parentid>
      <timestamp>2014-03-27T00:33:12Z</timestamp>
      <contributor>
        <username>Steve Podell</username>
        <id>21063352</id>
      </contributor>
      <minor/>
      <comment>Updated link from older wiki, to the newer confluence page</comment>
      <text xml:space="preserve" bytes="2931">{{Infobox software
| name                   = ZooKeeper
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 3.4.5
| latest release date    = {{release date|2012|11|19}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| platform               =
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Distributed computing]]
| license                = [[Apache License]] 2.0
| website                = {{url|http://zookeeper.apache.org}}
}}

'''Apache ZooKeeper''' is a software project of the [[Apache Software Foundation]], providing an [[open source]] distributed configuration service, synchronization service, and naming registry for large [[distributed systems]].{{Clarify|date=March 2012}} ZooKeeper was a sub project of [[Hadoop]] but is now a top-level project in its own right.

ZooKeeper's architecture supports [[High-availability cluster|high-availability]] through redundant services. The clients can thus ask another ZooKeeper master if the first fails to answer. ZooKeeper nodes store their data in a hierarchical name space, much like a file system or a [[trie]] datastructure. Clients can read and write from/to the nodes and in this way have a shared configuration service. Updates are [[total order|totally ordered]].&lt;ref&gt;{{cite web |url=https://cwiki.apache.org/confluence/display/ZOOKEEPER/ProjectDescription|title=Zookeeper Overview}}&lt;/ref&gt;

ZooKeeper is used by companies including [[Rackspace]], [[Yahoo!]]&lt;ref&gt;{{cite web |url=http://wiki.apache.org/hadoop/ZooKeeper/PoweredBy |title=ZooKeeper/Powered By}}&lt;/ref&gt; and [[eBay]]  as well as [[open source]] [[enterprise search]] systems like [[Solr]].&lt;ref&gt;{{cite web |url=https://cwiki.apache.org/confluence/display/solr/SolrCloud|title=SolrCloud}}&lt;/ref&gt;

==Typical use cases==
* [[Name service|Naming service]]
* [[Configuration management]]
* [[Synchronization]]
* [[Leader election]]
* [[Message Queue]]
* [[Notification system]]

==See also==
{{Portal|Java}}
* [[Hadoop]]

==References==
{{Reflist}}

==External links==
* [http://zookeeper.apache.org/ ZooKeeper homepage]
* [http://highscalability.com/blog/2008/7/15/zookeeper-a-reliable-scalable-distributed-coordination-syste.html Article in highscalability.com]
* [http://www.sdtimes.com/ZOOKEEPER_SERVICES_COORDINATOR_MOVES_TO_APACHE/About_SERVICECOORDINATION_and_ZOOKEEPER_and_APACHE_and_YAHOO/33011 Software Development Times article of ZooKeeper moving to Apache]
* [http://wiki.eclipse.org/index.php?title=Zookeeper_Based_ECF_Discovery  Eclipse ECF Discovery based on Apache ZooKeeper]

{{Apache}}

{{DEFAULTSORT:Apache Zookeeper}}
[[Category:Apache Software Foundation|ZooKeeper]]
[[Category:Configuration management]]
[[Category:Free software programmed in Java]]
[[Category:Hadoop|ZooKeeper]]</text>
      <sha1>8mnia0f5opc1uwz8yash74ze7rsv526</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Rampart module</title>
    <ns>0</ns>
    <id>11378675</id>
    <revision>
      <id>409765706</id>
      <parentid>347045312</parentid>
      <timestamp>2011-01-24T14:48:48Z</timestamp>
      <contributor>
        <username>Borkificator</username>
        <id>13816101</id>
      </contributor>
      <comment>new key for [[Category:Apache Software Foundation]]: &quot;Rampart&quot; using [[WP:HC|HotCat]]</comment>
      <text xml:space="preserve" bytes="1961">{{Infobox Software
| name                   = Apache Rampart Module
| logo                   = [[Image:ASF-logo.svg|160px]]
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| released               = 2006
| latest release version = 1.5
| latest release date    = {{release date|mf=yes|2010|02|01}}
| programming language   = [[Java (programming language)]]
| operating system       = [[Cross-platform]]
| language               = English
| genre                  = [[Axis2 module]]
| license                = [[Apache License]] 2.0
| website                = http://ws.apache.org/rampart/
}}

'''Apache Rampart''' is an implementation of the [[WS-Security]] standard for the [[Axis2]] [[Web services]] engine by the [[Apache Software Foundation]]. It supplies security features to web services by implementing the following specifications: 

* [[WS-Security]]
* [[WS-SecurityPolicy]]
* [[WS-Trust]]
* [[WS-SecureConversation]]
* [[SAML 1.1]]
* [[SAML 2.0]]

== See also ==
* [[Free Software Foundation]]
* [[Open Software Foundation]]

==External links==
* [http://ws.apache.org/rampart/ Apache Rampart] at the Apache Software Foundation
* [http://ws.apache.org/rampart/c/ Apache Rampart C-language implementation] at the Apache Software Foundation
* [http://ws.apache.org/axis2/ Apache AXIS2] at the Apache Software Foundation
* [http://download.boulder.ibm.com/ibmdl/pub/software/dw/specs/ws-secpol/ws-secpol.pdf Web Services Security Policy Language V1.1 specification]
* [http://www.ibm.com/developerworks/webservices/library/specification/ws-secmap Security in a Web Services World: A Proposed Architecture and Roadmap]
* [http://docs.oasis-open.org/ws-sx/ws-securitypolicy/200702/ws-securitypolicy-1.2-spec-os.html WS-SecurityPolicy 1.2 OASIS Standard]

[[Category:Computer security software]]
[[Category:Apache Software Foundation|Rampart]]
[[Category:Web services]]

{{web-software-stub}}</text>
      <sha1>iuzzcec5w7x9khyuocdb8cinkdn49ai</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Shale</title>
    <ns>0</ns>
    <id>9079196</id>
    <revision>
      <id>544661381</id>
      <parentid>512430688</parentid>
      <timestamp>2013-03-16T15:14:25Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q773211]]</comment>
      <text xml:space="preserve" bytes="1887">{{no footnotes|date=August 2012}}
{{ref improve|date=August 2012}}
{{Infobox software
| name = Apache Shale
| logo = [[File:ASF-logo.svg|frameless]]
| screenshot =
| caption =
| developer = [[Apache Software Foundation]]
| status = Retired
| latest release version = 1.0.4
| latest release date = {{release date|2007|12|19}}
| latest preview version =
| latest preview date =
| operating system = [[Cross-platform]]
| programming language = [[Java (programming language)|Java]]
| genre = [[web framework|Web Framework]]
| license = [[Apache License]] 2.0
| website = {{URL|http://shale.apache.org/}}
}}
'''Shale''' is a [[web application framework]] maintained by the [[Apache Software Foundation]]. It is fundamentally based on [[JavaServer Faces]]. As of May 2009 Apache Shale has been retired and moved to the [[Apache Attic]].

==See also==
{{Portal|Java}}
* [[Apache Struts]]

==References==
{{refbegin}}
* {{citation |url=http://www.ibm.com/developerworks/library/j-shale0228/ |title=All Hail Shale: Shale isn't Struts |first=Brett D. |last=McLaughlin |work=[[DeveloperWorks]] |date=February 28, 2006 |accessdate=August 3, 2012 }}
* {{citation |url=http://www.itworld.com/development/69162/apache-shale-web-framework-project-retired |title=Apache Shale Web framework project retired |first=Paul |last=Krill |work=[[InfoWorld]] |date=June 11, 2009 |accessdate=August 3, 2012 }}
{{refend}}

==External links==
* [http://shale.apache.org/ Shale project homepage]
* [http://www.jsfcentral.com/articles/mcclanahan-05-05.html JSF Central Interviews Craig McClanahan about Shale]
* [http://www.javaworld.com/javaworld/jw-06-2009/061109-apache-shale-web-framework-project.html Apache Shale Web framework project retired]

{{Application frameworks}}
{{apache}}

{{Software-stub}}

[[Category:Apache Software Foundation]]
[[Category:JavaServer Faces]]
[[Category:Web application frameworks]]</text>
      <sha1>nxwxi1ye3dgndcf2lv12t5aresagk36</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Sigar (software)</title>
    <ns>0</ns>
    <id>8253497</id>
    <revision>
      <id>549110827</id>
      <parentid>546614222</parentid>
      <timestamp>2013-04-07T06:13:54Z</timestamp>
      <contributor>
        <username>Steel1943</username>
        <id>2952402</id>
      </contributor>
      <comment>Disambiguated: [[programming]] → [[Computer programming]]</comment>
      <text xml:space="preserve" bytes="835">'''Sigar''' is a [[free software]] library (under the [[Apache License]]) that provides a [[cross-platform]], cross-language [[Computer programming|programming]] interface to low-level information on computer hardware and [[operating system]] activity.  The library provides bindings for many popular [[computer languages]] and has been ported to over 25 different operating system/hardware combinations.  Sigar stands for System Information Gatherer And Reporter and was originally developed by [[Doug MacEachern]], the author of the popular [[mod_perl]] module for the [[Apache web server]].

==External links==
*[http://sigar.hyperic.com hyperic.com], SIGAR home page.
*{{Freshmeat|hyperic-sigar|SIGAR}}

{{Compu-prog-stub}}

[[Category:Apache Software Foundation]]
[[Category:Computer programming]]
[[Category:Computing platforms]]</text>
      <sha1>8pi0nqpuqzebsk7slhlrvw6og98zfmx</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Roller</title>
    <ns>0</ns>
    <id>30854540</id>
    <revision>
      <id>544225710</id>
      <parentid>541261605</parentid>
      <timestamp>2013-03-14T23:42:37Z</timestamp>
      <contributor>
        <username>Jwalden</username>
        <id>85222</id>
      </contributor>
      <text xml:space="preserve" bytes="2310">{{Primary sources|date=August 2008}}
{{Infobox software
| name                   = Apache Roller
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 5.0.1&lt;ref&gt;http://roller.apache.org/downloads/downloads.html Apache Roller Releases&lt;/ref&gt;
| latest release date    = {{release date|2012|06|23}}
| latest preview version = 5.0 Beta1
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[weblog]] server
| license                = [[Apache License]] 2.0&lt;ref name=&quot;ohloh.net&quot;&gt;http://www.ohloh.net/p/roller OhLoh Apache Roller Summary information page at Ohloh&lt;/ref&gt;
| website                = http://roller.apache.org/
}}
{{Portal|Free software}}

'''Apache Roller''' is a [[Java (programming language)|Java]]-based [[Open Source]] &quot;full-featured, [[Multi-blog]], [[Multi-user]], and [[Multi-blog|group-blog]] server suitable for [[weblog|blog sites]] large and small&quot;.&lt;ref&gt;http://roller.apache.org/ Apache Roller official website&lt;/ref&gt; Roller was originally written by Dave Johnson in 2002 for a magazine article on open source development tools,&lt;ref&gt;http://onjava.com/pub/a/onjava/2002/04/17/wblogosj2ee.html&lt;/ref&gt; but became popular at FreeRoller.net (now JRoller.com) and was later chosen to drive the employee blogs at [[Sun Microsystems, Inc.]] and [[IBM developerWorks]] blogs.&lt;ref name=&quot;ohloh.net&quot;/&gt; 

On April 23, 2007, Roller project graduated from incubator, so it became an official project of the [[Apache Software Foundation]] and it was released 3.1 version, first official release.&lt;ref&gt;{{cite web|url=http://www.apachenews.org/archives/000992.html |title=Apache News Online, 23 april 2007 - Announcing project graduation and new Apache Roller 3.1 release |publisher=Apache News |date=2007-04-23 |accessdate=2010-11-21}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
* [http://roller.apache.org Roller website]
{{Apache}}

[[Category:Apache Software Foundation|Roller]]
[[Category:Free content management systems]]
[[Category:Blog software]]
[[Category:Free software programmed in Java]]

{{cms-software-stub}}
{{web-software-stub}}</text>
      <sha1>4usfnd1dgrwuks3u9xkqbysfcx5zoxd</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache OpenEJB</title>
    <ns>0</ns>
    <id>30858021</id>
    <revision>
      <id>546099715</id>
      <parentid>536648553</parentid>
      <timestamp>2013-03-21T22:06:47Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q553029]]</comment>
      <text xml:space="preserve" bytes="6117">{{ Infobox Software
| name                   = Apache OpenEJB 
| logo                   = &lt;!-- Deleted image removed: [[Image:Apache OpenEJB Logo.gif|150px|Apache OpenEJB Logo]]  --&gt;
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 4.5.1
| latest release date    = {{release date|2012|12|14}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[EJB|EJB Container System]] 
| license                = [[Apache License]] 2.0
| website                = http://openejb.apache.org
}}

'''OpenEJB''' is an [[open source]], embeddable and lightweight [[EJB]] Container System and EJB Server, released under the [[Apache Software License|Apache 2.0 License]]. OpenEJB has been integrated with Java EE application servers such as [[Apache Geronimo|Geronimo]],&lt;ref&gt;[http://www-128.ibm.com/developerworks/opensource/library/os-ag-renegade14/ The Geronimo renegade: What's new in OpenEJB 3.0]&lt;/ref&gt; and [[WebObjects]].&lt;ref&gt;[http://www.ibm.com/developerworks/opensource/library/os-ag-renegade7/index.html?S_TACT=105AGX44&amp;S_CMP=ART OpenEJB and Apache Geronimo's EJB implementation An Interview with David Blevins]&lt;/ref&gt;

==History==

OpenEJB was founded by Richard Monson-Haefel and David Blevins in December 1999.  At the time there were new vendors moving in the [[Business|enterprise]] [[Java (programming language)|Java]] space seemingly every week.  Rather than join the space as a competitor, the project was focused entirely on providing these new platforms with a way to quickly get EJB compliance via plugging OpenEJB into their application server.

The first to integrate OpenEJB in this fashion was Apple's [[WebObjects]] in late 2000, released in 2001.  When the project moved to [[Source Forge]] in 2002 an [[Apache Tomcat]] integration was created.  Again rather than follow what most in the industry were doing and putting Tomcat into OpenEJB, the project decided to follow its vision and provide an integration that allowed Tomcat users to plug in OpenEJB to gain EJB support in the Tomcat platform.  It was in this same vein of putting an EJB container into a Web server that the project developed the [http://openejb.apache.org/collapsed-ear.html Collapsed EAR] concept of putting EJBs inside the .war file.

As part of the work that OpenEJB did to prepare for the integration with Apple's WebObjects, a very large integration test suite was developed.  The test suite was developed as a generic application since it would need to be run against both [[WebObjects]] and other platforms that integrated OpenEJB.  For simplicity in the build the test suite, based on [[JUnit]], was run with OpenEJB right inside the tests rather than as a separate process, which was easy to do as the container was designed to be plugged into other platforms and make as little assumptions about its environment as possible.  It was from this work that the concept of combing an EJB application with plain unit tests and an embeddable EJB container was born.  Originally referred to as a &quot;local&quot; EJB container and what lead the project to describe itself as being able to run in two modes: Local and Remote.

In August 2003 the project helped launch the [[Apache Geronimo]] application server.  Originally a new version of OpenEJB was developed ground up based on Geronimo's GBean architecture and released as OpenEJB 2.0 which lived throughout the Geronimo 1.x cycle.  In 2006 when EJB 3.0 was released which had a focus on simplicity, the project went back to its roots and [http://blog.dblevins.com/2008/04/openejb-revival.html revived the OpenEJB 1.0 codebase], ported select bits of the 2.0 codebase, and eventually brought it up to the EJB 3.0 spec level in what is now called OpenEJB 3.0.
{| class=&quot;wikitable&quot;  style=&quot;margin:auto; margin:0 0 0 2em; font-size:85%;&quot;
|+ Apache OpenEJB Versions
|-
! Version
! Release Date
! Description
|-
| 0.01 (initial release)
| December 1999
| Born in [[Exolab]]
|-
| 
| January 2002
| Moved to [[SourceForge.net]]
|-
| 
| March 2004
| Moved to [[Codehaus]]
|-
|
| September 29, 2006
| Moved into the Apache Incubator
|-
|
| June 1, 2007
| Graduated as Apache OpenEJB
|}

==Major features==
* Supports EJB 3.0, 2.1, 2.0, 1.1 in all modes; embedded, standalone or otherwise.
* Partial EJB 3.1 support
* [[JAX-WS]] support
* [[Java Message Service|JMS]] support
* [[Java EE Connector Architecture|J2EE connector]] support
* Can be dropped into Tomcat 5 or 6 adding various JavaEE 5 and EJB 3.0 features to a standard Tomcat install.
* [[Container-Managed Persistence|CMP]] support is implemented over [[Java Persistence API|JPA]] allowing to freely mix CMP and JPA usage.
* Complete support for [[GlassFish]] descriptors allowing those users to embedded test their applications.
* Incredibly flexible [[JNDI]] name support allows you to specify formats at macro and micro levels and imitate the format of other vendors.
* Allows for easy testing and debugging in [[Integrated development environment|IDE]]s such as [[Eclipse (software)|Eclipse]], [[IntelliJ IDEA]] or [[NetBeans]] with no [[Plug-in (computing)|plugin]]s required.
* Usable in ordinary [[JUnit]] or other style [[test case]]s without complicated setup or external processes.
* Validates applications entirely and reports all failures at once, with three selectable levels of detail, avoiding several hours worth of &quot;fix, recompile, redeploy, fail, repeat&quot; cycles.
* [[OSGi]] support&lt;ref&gt;[http://www.infoq.com/news/2008/05/openejb-3.0-release InfoQ: OpenEJB 3.0 Supports DI of Enums and Collections, OSGi and EJB 3.0 features&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

==See also==
*[[EJB|Enterprise Java Bean (EJB)]]

==References==
&lt;references /&gt;

==External links==
* {{official website|http://openejb.apache.org}}

{{Java Persistence API}}
{{apache}}

{{DEFAULTSORT:Apache Openejb}}
[[Category:Apache Software Foundation|OpenEJB]]
[[Category:Java enterprise platform]]
[[Category:Free software application servers]]</text>
      <sha1>n7uj4ht0tam4la928lbempwn5zwu830</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache ActiveMQ</title>
    <ns>0</ns>
    <id>30874771</id>
    <revision>
      <id>601058247</id>
      <parentid>596161143</parentid>
      <timestamp>2014-03-24T17:13:44Z</timestamp>
      <contributor>
        <ip>65.200.90.182</ip>
      </contributor>
      <comment>remove &quot;the obvious&quot; as it sounds belittling to the reader and adds nothing to the statement</comment>
      <text xml:space="preserve" bytes="3950">{{Infobox software
| name                   = Apache ActiveMQ
| logo                   = [[File:Apache-activemq-logo.png|150px|Apache ActiveMQ Logo]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 5.9.0
| latest release date    = {{release date|2013|10|21}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Java Message Service]] [[Message-oriented middleware]] [[Enterprise Messaging System]] [[Service-oriented architecture|SOA]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://activemq.apache.org}}
}}
'''Apache ActiveMQ''' is an [[open source]] [[message broker]] written in Java together with a full [[Java Message Service]] (JMS) client. It provides &quot;Enterprise Features&quot; which in this case means fostering the communication from more than one client or server. Supported clients include Java via JMS 1.1 as well as several other &quot;cross language&quot; clients.&lt;ref&gt;[http://activemq.apache.org/cross-language-clients.html Apache ActiveMQ - Cross Language Clients]&lt;/ref&gt; The communication is managed with features such as [[computer clustering]] and ability to use any [[database]] as a JMS [[persistence (computer science)|persistence]] provider besides [[virtual memory]], [[cache (computing)|cache]], and [[journal (computing)|journal]] persistency.&lt;ref&gt;[http://activemq.apache.org/features.html Apache ActiveMQ - Features&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

ActiveMQ is used in [[enterprise service bus]] implementations such as [[Apache ServiceMix]] and [[Mule (software)|Mule]]. Other projects using ActiveMQ include [[Apache Camel]] and [[Apache CXF]] in [[Service-Oriented Architecture|SOA]] infrastructure projects.&lt;ref&gt;[http://activemq.apache.org/projects-using-activemq.html Apache ActiveMQ - Projects using ActiveMQ]&lt;/ref&gt;

Coinciding with the release of Apache ActiveMQ 5.3, the world's first results for the SPECjms2007 industry standard benchmark were announced. Four results were submitted to the [[SPEC]] and accepted for publication. The results cover different topologies to analyze the scalability of Apache ActiveMQ in two dimensions.&lt;ref&gt;[http://www.dvs.tu-darmstadt.de/research/performance/jmsmom/amqjms2007.html Worlds first SPECjms2007 Results using ActiveMQ 5.3]&lt;/ref&gt;&lt;ref&gt;[http://www.spec.org/jms2007/results/res2009q4/ SPECjms2007 Results]&lt;/ref&gt;

==See also==
{{Portal|Free software|Java}}
* [[StormMQ]]
* [[Apache Qpid]]
* [[Message-oriented middleware]]
* [[Enterprise Messaging System]]
* [[Enterprise Integration Patterns]]
* [[Service-Oriented Architecture]]
* [[Event-driven SOA]]

==References==
{{Reflist}}

==Bibliography==
{{Refbegin}}
*{{citation
| first1     = Bruce
| last1      = Snyder
| first2     = Dejan
| last2      = Bosanac
| first3     = Rob
| last3      = Davies
| date       = March 28, 2010
| title      = ActiveMQ in Action
| edition    = 1st
| publisher  = [[Manning Publications]]
| pages      = 375
| isbn       = 978-1-933988-94-8
| url        = 
}}
{{Refend}}

==External links==
* [http://activemq.apache.org/ Apache ActiveMQ website]
* [http://searchsoa.techtarget.com/feature/Book-Excerpt-ActiveMQ-in-Action &quot;Apache ActiveMQ&quot; excerpt from ActiveMQ in Action]
* [http://www.javabeat.net/articles/267-deploying-activemq-for-large-numbers-of-concurrent-appl-1.html &quot;Deploying ActiveMQ for large numbers of concurrent applications&quot; excerpt from ActiveMQ in Action]
* [http://www.spec.org/jms2007/results/res2009q4/ SPECjms2007 Results]
{{apache}}

{{DEFAULTSORT:Apache Activemq}}
[[Category:Message-oriented middleware]]
[[Category:Apache Software Foundation|ActiveMQ]]
[[Category:Service-oriented architecture-related products]]
[[Category:Enterprise application integration]]
[[Category:Java enterprise platform]]</text>
      <sha1>2s1zg5ugz88fhuvornentdtak9rfbhb</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Commons Daemon</title>
    <ns>0</ns>
    <id>3233639</id>
    <revision>
      <id>544208478</id>
      <parentid>478926911</parentid>
      <timestamp>2013-03-14T22:18:14Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q868574]]</comment>
      <text xml:space="preserve" bytes="1810">{{noref|date=November 2010}}
{{ Infobox Software
| name                   = Commons Daemon
| logo                   = 
| screenshot             =
| caption                =
| founder                = 
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 1.0.1
| latest release date    = &lt;!-- {{release date|YYYY|MM|DD}} --&gt;
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = 
| license                = [[Apache License]] 2.0
| website                = http://commons.apache.org/daemon
}}
'''Commons Daemon''', formerly known as '''JSVC''', is a [[Java (Sun)|Java]] [[Library (computer science)|software library]] belonging to the [[Apache Software Foundation|Apache]] [[Apache Commons|Commons Project]].

Daemon provides a portable means of starting and stopping a [[Java Virtual Machine]] (JVM) that is running server-side applications.  Such applications often have additional requirements compared to client-side applications.  For example, the servlet container [[Apache Tomcat|Tomcat 4]] would need to serialize sessions and shutdown web applications before the JVM process terminates.

Daemon comprises 2 parts: a native library written in  [[C (programming language)|C]] that interfaces with the operating system, and the library that provides the Daemon API, written in Java.

There are two ways to use Commons Daemon: by implementing the daemon interface or by calling a class that provides the required methods for daemon. For example, Tomcat-4.1.x uses the daemon interface and Tomcat-5.0.x provides a class whose methods are called by JSVC directly.


{{Apache}}
[[Category:Java libraries]]
[[Category:Apache Software Foundation|Commons Daemon]]</text>
      <sha1>5z2py8lpyf44n4b8v3ndi92oq7vv2ej</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Jakarta Cactus</title>
    <ns>0</ns>
    <id>7985307</id>
    <revision>
      <id>541660872</id>
      <parentid>509976414</parentid>
      <timestamp>2013-03-02T05:49:01Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 3 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q379683]]</comment>
      <text xml:space="preserve" bytes="1914">{{ Infobox Software
| name                   = Jakarta Cactus
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| status                 = Retired
| latest release version = 1.8.1
| latest release date    = 
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Unit Test]]
| license                = [[Apache License]] 2.0
| website                = http://jakarta.apache.org/cactus/
}}
'''Cactus''' is a simple test framework for unit testing server-side [[Java (programming language)| Java]] code ([[Servlets]], [[Enterprise JavaBean|EJB]]s, [[Tag library|Tag libs]], ...) from the [[Jakarta Project]]. The intent of Cactus is to lower the cost of writing tests for server-side code. It uses [[JUnit]] and extends it. Cactus implements an in-container strategy, meaning that tests are executed inside the container.

== Project Status ==
The Jakarta Cactus project was retired on August 5, 2011.

The Jakarta Cactus project announced the new 1.8.1 version [http://jakarta.apache.org/cactus/changes-report.html] on January 18, 2009. Version 1.8.1 still does not support [[JUnit]] 4.x, although a workaround of sorts is documented [https://issues.apache.org/jira/browse/CACTUS-252]. As of 1.8, Cactus uses [http://cargo.codehaus.org/ Cargo] for all server-related manipulation.
As of 08/05/2011, cactus has been retired. http://jakarta.apache.org/cactus/mock_vs_cactus.html

== External links ==
*[http://jakarta.apache.org/cactus/ Official Page of Jakarta Cactus]
*[http://cargo.codehaus.org/ Official Page of the Codehaus Cargo Project]

{{Apache}}

[[Category:Apache Software Foundation|Jakarta Cactus]]
[[Category:Unit testing frameworks]]
[[Category:Java development tools]]</text>
      <sha1>envgls182j4kos7vzbovjoigpgu6ysn</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache JMeter</title>
    <ns>0</ns>
    <id>1389448</id>
    <revision>
      <id>589320847</id>
      <parentid>584365192</parentid>
      <timestamp>2014-01-05T18:23:12Z</timestamp>
      <contributor>
        <username>Milamberspace</username>
        <id>12744467</id>
      </contributor>
      <text xml:space="preserve" bytes="4749">{{primary sources|date=September 2013}}
{{ Infobox Software
| name                   = Apache JMeter
| logo                   = [[Image:Jakarta jmeter logo.jpg|221px|Jakarta JMeter Logo]]
| screenshot             = [[Image:Jakarta jmeter screenshot.png|250px]]
| caption                = Jakarta JMeter screenshot
| collapsible            = yes
| developer              = [[Apache Software Foundation]] 
| latest release version = 2.11
| latest release date    = {{release date|2014|01|05}}
| latest preview version = [http://people.apache.org/builds/jakarta-jmeter/nightly/ nightly build]
| latest preview date    = nightly build
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[load testing|Load Testing]]
| license                = [[Apache License]] 2.0
| Type                   = [Open source software]
| website                = http://jmeter.apache.org/
}}
{{Portal|Software Testing}}
'''Apache JMeter''' is an [[Apache Software Foundation|Apache]] project that can be used as a [[load testing]] tool for analyzing and measuring the performance of a variety of services, with a focus on [[web application]]s.

JMeter can be used as a unit test tool for JDBC database connections,&lt;ref&gt;{{cite web|url=http://jakarta.apache.org/jmeter/usermanual/build-db-test-plan.html |title=Apache JMeter - User's Manual: Building a Database Test Plan |publisher=Jakarta.apache.org |date= |accessdate=2013-09-20}}&lt;/ref&gt; FTP,&lt;ref&gt;{{cite web|url=http://jmeter.apache.org/usermanual/build-ftp-test-plan.html |title=Apache JMeter - User's Manual: Building an FTP Test Plan |publisher=Jmeter.apache.org |date= |accessdate=2013-09-20}}&lt;/ref&gt; LDAP,&lt;ref&gt;[http://jmeter.apache.org/jmeter/usermanual/build-ldap-test-plan.html ]{{dead link|date=September 2013}}&lt;/ref&gt; Webservices,&lt;ref&gt;{{cite web|url=http://jmeter.apache.org/usermanual/build-ws-test-plan.html |title=Apache JMeter - User's Manual: Building a WebService Test Plan |publisher=Jmeter.apache.org |date= |accessdate=2013-09-20}}&lt;/ref&gt; JMS,&lt;ref&gt;{{cite web|url=http://jmeter.apache.org/usermanual/build-jms-topic-test-plan.html |title=Apache JMeter - User's Manual: Building a JMS (Java Messaging Service) Test Plan |publisher=Jmeter.apache.org |date= |accessdate=2013-09-20}}&lt;/ref&gt; HTTP,&lt;ref&gt;{{cite web|url=http://jmeter.apache.org/usermanual/build-web-test-plan.html |title=Apache JMeter - User's Manual: Building a Web Test Plan |publisher=Jmeter.apache.org |date= |accessdate=2013-09-20}}&lt;/ref&gt; generic TCP connections and OS Native processes. JMeter can also be configured as a monitor,&lt;ref&gt;{{cite web|url=http://jmeter.apache.org/usermanual/build-monitor-test-plan.html |title=Apache JMeter - User's Manual: Building a Monitor Test Plan |publisher=Jmeter.apache.org |date= |accessdate=2013-09-20}}&lt;/ref&gt; although this is typically considered an ''ad hoc'' solution in lieu of advanced monitoring solutions. It can be used for some functional testing as well.&lt;ref&gt;{{cite web|url=http://jmeter.apache.org/usermanual/intro.html#history |title=Apache JMeter - User's Manual: Introduction |publisher=Jmeter.apache.org |date= |accessdate=2013-09-20}}&lt;/ref&gt;

JMeter supports variable parameterization, assertions (response validation), per thread cookies, configuration variables and a variety of reports. 

JMeter architecture is based on [[plugins]]. Most of its &quot;out of the box&quot; features are implemented with plugins. Off-site developers can easily extend JMeter with custom plugins.

== Releases ==
{| class=&quot;wikitable sortable&quot;  style=&quot;margin:auto; margin:0 0 0 2em;&quot;
|+ Apache JMeter versions
|-
! Version
! Release Date
! Description
|-
| 1.0.2
| 2001-03-09
| earliest in archive
|-
| ... 
| ...
| 
|-
| 2.3RC3 
| 2007-07-11
|
|-
| 2.3RC4
| 2007-09-02
|
|-
| 2.3 
| 2007-09-24
| 
|-
| 2.3.1
| 2007-11-28
|
|-
| 2.3.2
| 2008-06-10
|
|-
| 2.3.3
| 2009-05-24
|
|-
| 2.3.4
| 2009-06-21
| Java 1.4+
|-
| 2.4
| 2010-07-14
| Java 5+
|-
| 2.5
| 2011-08-17
| Java 5
|-
| 2.5.1
| 2011-10-03
| Java 5+
|-
| 2.6
| 2012-02-01
| Java 5+
|-
| 2.7
| 2012-05-27
| Java 5+
|-
| 2.8
| 2012-10-06
| Java 5+
|-
| 2.9
| 2013-01-28
| Java 6+
|-
| 2.10
| 2013-10-21
| Java 6+
|-
| 2.11
| 2014-01-05
| Java 6+
|}

==See also==
*[[iMacros]]
*[[Performance Engineering]]
*[[Selenium (software)]]
*[[Software performance testing]]
*[[Software testing]]
*[[Web server benchmarking]]

== References ==
{{Reflist}}

== External links ==
*[http://jmeter.apache.org/ Official JMeter website]
*[http://jmeter-plugins.org/ Example of JMeter Custom Plugins]

{{apache}}

[[Category:Apache Software Foundation|JMeter]]
[[Category:Java development tools]]
[[Category:Java enterprise platform]]
[[Category:Load testing tools]]

{{Network-software-stub}}</text>
      <sha1>cfk9k9kwehewfoyg2heqpvyrczq4ond</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Category:Apache Software Foundation members</title>
    <ns>14</ns>
    <id>31555734</id>
    <revision>
      <id>450533126</id>
      <parentid>425149077</parentid>
      <timestamp>2011-09-14T21:00:59Z</timestamp>
      <contributor>
        <username>Grossenhayn</username>
        <id>12441884</id>
      </contributor>
      <minor/>
      <comment>Adding {{Commons cat}}</comment>
      <text xml:space="preserve" bytes="91">{{Commons cat|Apache Software Foundation members}}

[[Category:Apache Software Foundation]]</text>
      <sha1>juuigbwzun04f51srxu7ppc48plyetr</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache ODE</title>
    <ns>0</ns>
    <id>15603215</id>
    <revision>
      <id>529999482</id>
      <parentid>512909218</parentid>
      <timestamp>2012-12-27T16:34:41Z</timestamp>
      <contributor>
        <username>Andrewman327</username>
        <id>15138</id>
      </contributor>
      <minor/>
      <comment>clean up of articles listed as &quot;needing cleanup&quot; using [[Project:AWB|AWB]] (8759)</comment>
      <text xml:space="preserve" bytes="4333">{{Context|date=October 2009}}

{{Infobox software
| name                   = Apache ODE
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version =  1.3.5
| latest release date    = {{release date|2011|02|06|}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Workflow engine]], [[Middleware]]
| standard               = [[WS-BPEL]], [[WSDL]], [[SOAP]], [[JBI]]
| license                = [[Apache License]] 2.0
| website                = http://ode.apache.org/
}}

'''Apache ODE''' ('''Apache &lt;u&gt;O&lt;/u&gt;rchestration &lt;u&gt;D&lt;/u&gt;irector &lt;u&gt;E&lt;/u&gt;ngine''') 'executes' or 'runs' one or more [[business processes]] which have been expressed in the Web Services Business Process Execution Language ([[WS-BPEL]]). It principally communicates with one or more [[Web services]], sending and receiving messages, manipulating data and handling exceptions ('errors') as defined by any given process. The engine is capable of running both long and short living processes to coordinate all the services that make up a service or application ([[Orchestration (computing)|orchestration]]).

WS-BPEL itself is based upon the [[XML]] language and includes a number of ways in which business processes can be expressed. These include conditional clauses, repeating loops, calls to web services and the exchange of messages. Where interfaces with web services are required, it makes use of Web Services Description Language ([[WSDL]]) to express them. Messages can be handled in a flexible way by reading either part or all of the message into variables, which can then be used for onward communication.

The engine has two communication layers, with which it interacts with the outside world. :&lt;ref&gt;{{cite web
| accessdate = 2011-05-16
| location = http://ddweerasiri.blogspot.com/
| publisher = Denis's Blog
| title = How to deploy an Axis2 Web service programatically in ODE during the initialization of ODE Runtime
| quote = Apache ODE (Orchestration Director Engine) executes business processes written following the WS-BPEL standard. It has two communication layers. One is Axis2 integration layer and the other one is based on JBI standard. Those integration layers are used by ODE BPEL Engine Runtime for interact with the outside world. Axis2 integration layer supports for communicate via Web Service interactions. JBI integration layer supports for communicate via JBI messages.
| url = http://ddweerasiri.blogspot.com/2009/05/how-to-deploy-axis2-web-service.html}}&lt;/ref&gt;
* [[Apache Axis2]] integration layer: supports the communication over [[Web services]].
* Layer based on the [[JBI]] standard: supports communication via JBI messages.

==Features==
* Side-by-side support for both the WS-BPEL 2.0 [[OASIS (organization)|OASIS]] standard and the legacy BPEL4WS 1.1 vendor specification.
* Supports 2 communication layers: one based on [[Axis2]] (Web Services http transport) and another one based on the [[JBI]] standard (using [[ServiceMix]]).
* Support for the HTTP WSDL binding, allowing invocation of [[REST]]-style web services.
* Possibility to map process variables externally to a database table of your choice.
* High level API to the engine that allows you to integrate the core with virtually any communication layer.
* Hot-deployment of your processes.
* Compiled approach to [[BPEL]] that provides detailed analysis and validation at the command line or at deployment.
* Management interface for processes, instances and messages.

==Embedding==
Apache ODE is embeded and an important part of the [[Jboss]] projects '''RiftSaw''' ([[WS-BPEL]] 2.0 engine) and also in the follow-up '''SwitchYard''', which is a service delivery [[Software framework|framework]] for service-oriented applications.

==See also==
{{Portal|Free Software}}
*[[WS-BPEL]]

==References==
{{Reflist|2}}

==External links==
*[http://ode.apache.org/ Apache ODE home page]
*[http://www.jboss.org/riftsaw RiftSaw]
*[http://www.jboss.org/switchyard SwitchYard]
{{apache}}

&lt;!--Interwikies--&gt;

&lt;!--Categories--&gt;
[[Category:Apache Software Foundation]]
[[Category:Beta software]]</text>
      <sha1>cxclo2w8a7nejlj8pexga18tu6xjo8y</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache MINA</title>
    <ns>0</ns>
    <id>32517945</id>
    <revision>
      <id>600874716</id>
      <parentid>565087831</parentid>
      <timestamp>2014-03-23T13:33:00Z</timestamp>
      <contributor>
        <ip>184.183.181.24</ip>
      </contributor>
      <comment>/* Alternatives */ Add link to Project Grizzly</comment>
      <text xml:space="preserve" bytes="2963">{{Infobox software
| name                   = Apache MINA
| logo                   = [[File:Apache-MINA-logo.png]]
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest release version =  2.0.7
| latest release date    = {{release date|2012|10|12}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Enterprise Integration Patterns]] [[Message Oriented Middleware]]
| license                = [[Apache License]] 2.0
| website                = {{URL|mina.apache.org}}
}}

'''Apache MINA''' ('''M'''ultipurpose '''I'''nfrastructure for '''N'''etwork '''A'''pplications)&lt;ref&gt;https://mina.apache.org/mina-project/faq.html#what-does-mina-mean&lt;/ref&gt; is an [[open source]] [[Java (programming language)|Java]] network [[application framework]]. MINA can be used to create [[Scalability|scalable]], high performance [[Computer network|network applications]]. MINA provides unified [[Application programming interface|API]]s for various transports like [[Transmission Control Protocol|TCP]], [[User Datagram Protocol|UDP]], [[serial communication]]. It also makes it easy to make an implementation of custom transport type. MINA provides both high-level and low-level network APIs.

A user application interacts with MINA APIs, shielding the user application from low level [[I/O]] details. MINA internally uses I/O APIs to perform the actual I/O functions. This makes it easy for the users to concentrate on the application logic and leave the I/O handling to Apache MINA.&lt;ref&gt;{{cite web|title=Apache MINA - FAQ|url=http://mina.apache.org/faq.html|accessdate=2011-07-24| archiveurl= http://web.archive.org/web/20110725012454/http://mina.apache.org/faq.html| archivedate= 25 July 2011 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

==Advantages==
*Unified APIs for various transports (TCP/UDP etc.)&lt;ref&gt;{{cite web|title=Apache MINA features|url=http://mina.apache.org/features.html|accessdate=2011-07-24}}&lt;/ref&gt;
*Provides high/low level APIs
*Customizable Thread Model
*Easy Unit Testing using [[Mock object|Mock Objects]]
*Integration with DI frameworks like [[Spring Framework|Spring]], [[Google Guice]], picocontainer
*JMX Manageability

==Tooling==
Graphical tools such as [[Eclipse IDE]], [[IntelliJ IDEA]] can be used.

==Alternatives==
*[[Project_Grizzly_(software)|Grizzly]]
*[[Netty_(software)|Netty 3]]
*QuickServer
*xSocket

==See also==
{{Portal|Free software}}
*[[Apache Camel]]
*[[Enterprise messaging system]]
*[[Message-oriented middleware]]
*[[Service-oriented architecture]]
*[[Event-driven SOA]]

==References==
{{Reflist|colwidth=50em}}

==External links==
*{{Official website|mina.apache.org}}

{{apache}}

[[Category:Apache Software Foundation|MINA]]
[[Category:Message-oriented middleware]]
[[Category:Java platform]]


{{Network-software-stub}}</text>
      <sha1>8jcfurzti8k2uiwnlnvzz2w7n09an8p</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>OpenNLP</title>
    <ns>0</ns>
    <id>30972465</id>
    <revision>
      <id>590412282</id>
      <parentid>584362862</parentid>
      <timestamp>2014-01-12T21:04:54Z</timestamp>
      <contributor>
        <username>The Transhumanist</username>
        <id>1754504</id>
      </contributor>
      <comment>/* See also */ add link</comment>
      <text xml:space="preserve" bytes="2198">{{Infobox software
| name                   = Apache OpenNLP
| logo                   =
| screenshot             = 
| caption                = 
| collapsible            = 
| author                 = 
| developer              = [[Apache Software Foundation]]
| released               = 
| latest release version = 1.5.3
| latest release date    = {{release date|mf=yes|2013|04|17}}
| latest preview version = 
| latest preview date    = 
| frequently updated     = 
| programming language   = [[Java (programming language)|Java]]
| operating system       = [[Cross-platform]]
| platform               = 
| size                   = 
| language               = 
| status                 = Active
| genre                  = [[Natural language processing]]
| license                = [[Apache License|Apache 2.0]]
| website                = [http://opennlp.apache.org/ http://opennlp.apache.org/]
}}

The [[Apache Software Foundation|Apache]] '''OpenNLP''' library is a [[machine learning]] based toolkit for the processing of natural language text. It supports the most common NLP tasks, such as [[tokenization]], [[Sentence boundary disambiguation|sentence segmentation]], [[part-of-speech tagging]], [[Named entity recognition|named entity extraction]], [[Shallow parsing|chunking]], [[Syntactic parsing|parsing]], and [[coreference|coreference resolution]]. These tasks are usually required to build more advanced text processing services. &lt;ref&gt;[http://opennlp.apache.org/index.html Apache OpenNLP Website]&lt;/ref&gt; &lt;ref&gt;[http://wiki.apache.org/incubator/OpenNLPProposal Apache OpenNLP Proposal]&lt;/ref&gt;

==See also== 
{{Portal|Free software}}
* [[List of natural language processing toolkits]]
** [[Unstructured Information Management Architecture]] (UIMA)
** [[General Architecture for Text Engineering]] (GATE)
* [[cTAKES]]

== References ==

&lt;references/&gt;

==External links==
*[http://opennlp.apache.org/index.html Apache OpenNLP Website]

{{Apache}}

[[Category:Natural language processing]]
[[Category:Statistical natural language processing]]
[[Category:Natural language processing toolkits]]
[[Category:Apache Software Foundation]]
[[Category:Java libraries]]
[[Category:Cross-platform software]]</text>
      <sha1>2wkqqcfezv92fkm1st0xr3k2ryllf82</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Ivy</title>
    <ns>0</ns>
    <id>11243260</id>
    <revision>
      <id>594119135</id>
      <parentid>594102375</parentid>
      <timestamp>2014-02-05T23:22:34Z</timestamp>
      <contributor>
        <username>Xqbot</username>
        <id>8066546</id>
      </contributor>
      <minor/>
      <comment>Robot: Adding missing &lt;references /&gt; tag</comment>
      <text xml:space="preserve" bytes="3572">{{Infobox software
| name                   = Apache Ivy
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| latest release version = 2.3.0
| latest release date    = {{release date|2013|01|21}} [http://ant.apache.org/ivy/history/2.3.0/release-notes.html]
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| platform               = [[Java platform|Java]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Library dependency]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://ant.apache.org/ivy}}
}}

'''Apache Ivy''' is a [[transitive relation]] [[library dependency|dependency manager]]. It is a sub-project of the [[Apache Ant]] project, with which Ivy works to resolve project dependencies. An external [[XML]] file defines project dependencies and lists the resources necessary to build a project. Ivy then resolves and downloads resources from an artifact repository: either a private repository or one publicly available on the [[Internet]].

To some degree, it competes with [[Apache Maven]], which also manages dependencies. However, Maven is a complete build tool, whereas Ivy focuses purely on managing transitive dependencies.

Newer build tools and continuous integration servers regularly support or include Ivy:
* [[SBT (software)|sbt]], or &quot;simple build tool,&quot; the primary build tool for [[Scala (programming language)|Scala]] projects, incorporates Ivy for its dependency management.
* [[Grails (framework)|Grails]] (until anticipated 3.0 release in 2014)&lt;ref&gt;{{cite web |url=http://grails.org/Roadmap |title=Grails roadmap |author=&lt;!--Staff writer(s); no by-line.--&gt; |website=grails.org |accessdate=5 February 2014}}&lt;/ref&gt;
* [[gradle]]
* [[Jenkins_(software)|Jenkins]]
* [[TeamCity]]

==Features==
* Managing project dependencies
* XML-driven declaration of project dependencies and JAR repositories
* Automatic retrieval of transitive dependency definitions and resources
* Automatic integration to publicly available artifact repositories
* Resolution of dependency closures
* Configurable project state definitions, which allow for multiple dependency-set definitions
* Publishing of artifacts into a local enterprise repository

==History==
[[Jayasoft]] first created Ivy in September, 2004, with [[Xavier Hanin]] serving as the principal architect and developer of the project. Jayasoft moved hosting of Ivy (then at version 1.4.1) to [[Apache Incubator]] in October 2006. Since then, the project has undergone package renaming to reflect its association with the [[Apache Software Foundation]]. Package names prefixes of the form &lt;code&gt;fr.jayasoft.ivy&lt;/code&gt; have become &lt;code&gt;org.apache.ivy&lt;/code&gt; prefixes.

Ivy graduated from the [[Apache Incubator]] in October, 2007. As of 2009 it functions as a sub-project of [[Apache Ant]].

==See also==
*[[Apache Maven]] — An alternative dependency management tool

==References==

&lt;references /&gt;
* Steve Loughran, Erik Hatcher: &lt;cite&gt;Ant in Action&lt;/cite&gt;, Manning Publications Company, ISBN 1-932394-80-X

==External links==
* [http://ant.apache.org/ivy/ Apache Ivy Home]
* [http://www.jaya.free.fr/ Archival Jayasoft website]
* [http://public.dhe.ibm.com/software/dw/java/j-ap05068-a4.pdf Automation for the people: Manage dependencies with Ivy] by Paul Duvall
{{apache}}

[[Category:Java development tools]]
[[Category:Apache Software Foundation]]
[[Category:Build automation]]</text>
      <sha1>q2ys3i675juird0zcbaw9ky9og2c43u</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache James</title>
    <ns>0</ns>
    <id>15896167</id>
    <revision>
      <id>589224736</id>
      <parentid>569468397</parentid>
      <timestamp>2014-01-05T01:45:40Z</timestamp>
      <contributor>
        <ip>92.100.86.159</ip>
      </contributor>
      <comment>Added mail server template reference</comment>
      <text xml:space="preserve" bytes="4650">{{Infobox Software
| name                   = Apache James
| logo                   = 
| screenshot             = 
| caption                = 
| collapsible            = 
| author                 = 
| developer              = [[Apache Software Foundation]]
| released               = 
| latest release version = 2.3.2
| latest release date    = August 2009
| latest preview version = 3.0 beta 4
| latest preview date    = April 2012
| frequently updated     = 
| programming language   = [[Java (programming language)|Java]]
| operating system       = 
| platform               = [[Java virtual machine]]
| language               = 
| status                 = 
| genre                  = [[Mail transfer agent]], [[News server]]
| license                = [[Apache License]]
| website                = http://james.apache.org
}}

'''Apache James''', aka '''Java Apache Mail Enterprise Server''' or some variation thereof, is an [[open source]] [[Simple Mail Transfer Protocol|SMTP]] and [[Post Office Protocol|POP3]] [[mail transfer agent]] and [[Network News Transfer Protocol|NNTP]] [[news server]] written entirely in [[Java (programming language)|Java]].&lt;ref name=&quot;overview&quot;&gt;[http://james.apache.org/server/index.html James Server - Overview]&lt;/ref&gt; James is maintained by contributors to the [[Apache Software Foundation]], with initial contributions by [[Serge Knystautas]].&lt;ref&gt;[http://james.apache.org/weare.html James Project - Who We Are]&lt;/ref&gt;&lt;ref name=&quot;boardminutes&quot;/&gt; [[Internet_Message_Access_Protocol|IMAP]] support has been added as of preview version 3.0-M2,&lt;ref&gt;[http://james.apache.org/imap/index.html 3.0-M2]&lt;/ref&gt; which now requires Java 1.5 or later.

The James project manages the Apache Mailet [[Application programming interface|API]] which defines &quot;matchers&quot; and &quot;mailets&quot;. These allow users to write their own mail-handling code, such as to update a database, build a message archive, or filter [[e-mail spam|spam]].&lt;ref name=&quot;overview&quot;/&gt; A matcher is used to classify messages based on some criteria, and then determines whether the message should be passed to an appropriate mailet for processing. Mailets are so-called due to their conceptual similarity to a [[servlet]],&lt;ref name=&quot;ibm&quot;&gt;{{cite web |url=http://www.ibm.com/developerworks/java/library/j-james1.html |title=Working with James |accessdate=2008-02-22 |author=Claude Duguay |date=2003-06-10 |publisher=[[IBM]]| archiveurl= http://web.archive.org/web/20080129160018/http://www.ibm.com/developerworks/java/library/j-james1.html#resources| archivedate= 29 January 2008 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; and arose because [[Sun Microsystems]] declined a proposal to include mail-handling in the servlet implementation.&lt;ref name=&quot;boardminutes&quot;/&gt; James ships with a variety of pre-written matchers and mailets to serve common tasks.&lt;ref name=&quot;ibm&quot;/&gt; Many sets of mailets and matchers can be combined to produce sophisticated and complex functional behaviour.

The Apache James project also produces pure Java libraries for implementing Sender Policy Framework (SPF), the Sieve mail filtering language, and parsing MIME content streams, independent of Sun's JavaMail API.

==Development==
James was originally formed under the [[Jakarta Project]] as Jakarta-James.

&lt;ref name=&quot;boardminutes&quot;&gt;
{{cite web |url=http://www.apache.org/foundation/records/minutes/2003/board_minutes_2003_01_22.txt |title=Board of Directors Meeting Minutes |accessdate=2008-02-23 |date=2003-01-22 |publisher=[[Apache Software Foundation]]}}&lt;/ref&gt; In January 2003, James was upgraded to a top-level Apache project in a unanimous decision by the ASF Board of Directors under the chairmanship of [[Serge Knystautas]]. 

James is distributed to within the Phoenix container,&lt;ref name=&quot;ibm&quot;/&gt; which implements the [[Apache Avalon]] [[application framework]].

Recent developments include a version which runs in the [[Spring Framework]] application framework.

Version 2.3.0 was released in October 2006.

Version 2.3.1 was released in April 2007.

Version 2.3.2 was released in August 2009.&lt;ref&gt;[http://james.apache.org/newsarchive.html James Project - News Archive]&lt;/ref&gt;

==See also==
* [[Comparison of mail servers]]
* [[List of mail servers]]

==References==
{{reflist}}

==External links==
* [http://james.apache.org/ Official website]
* [http://www.ibm.com/developerworks/java/library/j-james1.html Working with James] at IBM developerWorks

{{apache}}
{{Email servers}}

[[Category:Apache Software Foundation|James]]
[[Category:Free email server software]]
[[Category:Free software programmed in Java]]
[[Category:Email]]
[[Category:Message transfer agents]]
[[Category:Usenet servers]]</text>
      <sha1>iol6tpgtkhhqy9bdw0hicr69eu8wtoh</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache TomEE</title>
    <ns>0</ns>
    <id>34750556</id>
    <revision>
      <id>596359259</id>
      <parentid>593033852</parentid>
      <timestamp>2014-02-20T16:48:21Z</timestamp>
      <contributor>
        <username>RockyMM</username>
        <id>389939</id>
      </contributor>
      <comment>http://tomee.apache.org/download/index.html</comment>
      <text xml:space="preserve" bytes="4115">{{ Infobox Software
| name                   = Apache TomEE
| collapsible            = yes
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 1.6.0
| latest release date    = {{release date and age|2013|11|20}}
| latest preview version = 1.6.1-SNAPSHOT
| latest preview date    =
| operating system       = [[Cross-platform]] ([[JVM]])
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[application server|Web Application Server]]
| license                = [[Apache License]] 2.0
| website                = http://tomee.apache.org
}}
'''Apache TomEE''' (pronounced &quot;Tommy&quot;) is the [[Java Enterprise Edition]] of Apache Tomcat (Tomcat + Java EE = TomEE) that combines several Java enterprise projects including [[Apache OpenEJB]], Apache OpenWebBeans, [[Apache OpenJPA]], [[Apache MyFaces]] and others.&lt;ref&gt;{{cite web|url=http://openejb.apache.org/apache-tomee.html|title=Apache TomEE|publisher=[[Apache OpenEJB]]}}&lt;/ref&gt; In October 2011, the project obtained certification by [[Oracle Corporation]] as a compatible implementation of the Java EE 6 Web Profile.&lt;ref&gt;{{cite web|url=http://www.marketwatch.com/story/the-apache-software-foundation-announces-apache-tomee-certified-as-java-ee-6-web-profile-compatible-2011-10-04|title=The Apache Software Foundation Announces Apache TomEE Certified as Java EE 6 Web Profile Compatible|publisher=[[MarketWatch]]|date=4 Oct 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.infoworld.com/d/application-development/apache-tomee-web-stack-gains-approval-175341|title=Apache TomEE Web stack gains approval|publisher=[[InfoWorld]]|date=7 Oct 2011}}&lt;/ref&gt;

== Components ==

Here a list of the open source components that are included in TomEE.

{| class=&quot;wikitable&quot;
|-
! Component
! Description
|-
| [[Apache Tomcat]] 
| HTTP server and Servlet container supporting [[Java Servlet]] and [[JavaServer Pages]] (JSP).
|-
| [[Apache OpenEJB]] 
| Open source [[Enterprise JavaBean]]s (EJB) Container System.
|-
| [[Apache OpenWebBeans]] 
| Open source [[Java Contexts and Dependency Injection]] (CDI) implementation.
|-
| [[Apache OpenJPA]] 
| Open source [[Java Persistence API]] (JPA) 2.0 implementation.
|-
| [[Apache MyFaces]] 
| Open source [[Java Server Faces]] (JSF) implementation.
|-
| [[Apache ActiveMQ]] 
| Open source [[Java Message Service|Java Message Service (JMS)]] implementation.
|-
| [[Apache CXF]] 
| [[Web Services]] frameworks with variety of protocols such as SOAP, XML/HTTP, [[Representational State Transfer|RESTful]] [[HTTP]].
|-
| [[Apache Derby]] 
| Full-fledged [[relational database management system]] (RDBMS) with native [[Java Database Connectivity]] (JDBC) support.
|}

TomEE JAX-RS is a second distribution that adds support for Java API for RESTful Web Services (JAX-RS).  The full TomEE Plus distribution adds additional support for Java API for XML Web Services (JAX-WS), Java EE Connector Architecture, and Java Messaging Service (JMS).&lt;ref&gt;{{cite web|url=http://tomee.apache.org/comparison.html|title=Apache TomEE comparison}}&lt;/ref&gt;

== See also ==
Other Java EE application servers:
* [[Apache Geronimo]]
* [[JBoss AS]]
* [[IBM WebSphere Application Server|WebSphere AS]]
* [[WebLogic|WebLogic Server]]
* [[GlassFish]]
* [[Comparison of application servers#Java EE|Comparison of application servers]]

==References==
{{reflist}}

==External links==
*[http://tomee.apache.org Apache TomEE]

=== Presentations ===
*[http://www.slideshare.net/stratwine/apache-tomee-tomcat-with-a-kick Apache TomEE - Tomcat with a Kick at JAXLondon 2011]
*[http://www.slideshare.net/dblevins1/2011-java-oneapachetomeejavaee6webprofile Apache TomEE Java EE 6 Web Profile at JavaOne 2011]

=== Apache TomEE Books ===
*[http://www.amazon.com/Apache-TomEE-Cookbook-Administrator/dp/1492201448/ref=sr_1_1?ie=UTF8&amp;qid=1377153544&amp;sr=8-1&amp;keywords=Apache+TomEE+Cookbook Apache TomEE CookBook by Gurkan Erdogdu, Amazon.com]

{{apache}}

[[Category:Apache Software Foundation|TomEE]]
[[Category:Java enterprise platform]]
[[Category:Free software application servers]]</text>
      <sha1>d576bvuauw9wvpaiejdkmwn4byzvnhb</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Httpd.conf</title>
    <ns>0</ns>
    <id>7907742</id>
    <revision>
      <id>598635398</id>
      <parentid>580102943</parentid>
      <timestamp>2014-03-08T02:14:36Z</timestamp>
      <contributor>
        <username>Frap</username>
        <id>612852</id>
      </contributor>
      <text xml:space="preserve" bytes="941">{{multiple issues|
{{notability|date=September 2011}}
{{primary sources|date=September 2011}}
}}

{{lowercase}}
'''httpd.conf''' is a [[configuration file]] which is used by the [[Apache HTTP Server]]. It stores information on various functions of the server, which can be edited by removing or adding a [[number sign]] &quot;#&quot; at the beginning of the line, thus setting values for each directive.

The httpd.conf file can be located on any [[Unix|Unix-based]] system that complies with the [[Filesystem Hierarchy Standard]] under the following path: /etc/httpd/httpd.conf.

This file, httpd.conf was once used in Microsoft's [[Internet Information Services]] (IIS).

==References==
{{reflist}}

==External links==
* [http://httpd.apache.org/docs/2.0/configuring.html Official configuration from Apache project homepage]

{{DEFAULTSORT:Httpd.Conf}}
[[Category:Configuration files]]
[[Category:Apache Software Foundation]]


{{Web-software-stub}}</text>
      <sha1>msrfl97c3mr31kp44kwm8nrpfol6ctd</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache CXF</title>
    <ns>0</ns>
    <id>11787206</id>
    <revision>
      <id>595038468</id>
      <parentid>578386200</parentid>
      <timestamp>2014-02-11T21:29:58Z</timestamp>
      <contributor>
        <ip>66.187.233.206</ip>
      </contributor>
      <text xml:space="preserve" bytes="4975">{{Infobox Software
| name                   = Apache CXF
| logo                   =&lt;br/&gt;&lt;span style=&quot;font-family:verdana; font-weight: bold; font-size: 18px; height:50px; padding:8px 10px;background-color:#336894; background: -moz-linear-gradient(top, #336894, #BDDCFB); color:white;border-radius: 5px;&quot;&gt;Apache CXF&lt;/span&gt;
| screenshot             = 
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 2.7.7
| latest release date    = {{release date|2013|09|24}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Web Services]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://cxf.apache.org/}}
}}

'''Apache CXF''' is an [[Open source software|open-source]], fully featured [[Web services]] framework. It originated as the combination of two [[Open source software|open-source]] projects: [[Celtix]] developed by [[IONA Technologies]] (acquired by [[Progress Software]] in 2008) and [[Codehaus XFire|XFire]] developed by a team hosted at [[Codehaus]]. These two projects were combined by people working together at the [[Apache Software Foundation]] and the new name '''CXF''' was derived by combining &quot;''Celtix''&quot; and &quot;''XFire''&quot;.

The CXF key design considerations include:
*Clean separation of [[Front-end and back-end|front-ends]], like [[Java API for XML Web Services|JAX-WS]], from the core [[Source code|code]].
*Simplicity with, for instance, the creation of [[Client (computing)|clients]] and endpoints without annotations.
*High performance with minimum [[computational overhead]].
*Embeddable Web service component: example embeddings include [[Spring Framework]] and [[Apache Geronimo|Geronimo]].

CXF is often used with [[Apache ServiceMix]], [[Apache Camel]] and [[Apache ActiveMQ]] in [[service-oriented architecture]] (SOA) infrastructure projects.

==Features==
CXF includes a broad feature set, but it is primarily focused on the following areas:
*Web Services Standards Support:
**[[SOAP]]
**[[WS-Addressing]]
**[[WS-Policy]]
**[[WS-ReliableMessaging]]
**[[WS-SecureConversation]]
**[[WS-Security]]
**[[WS-SecurityPolicy]]
*[[JAX-WS]] API for Web service development
**[[Java (software platform)|Java]] first support
**[[Web Services Description Language|WSDL]] first tooling
*[[JAX-RS]] (JSR 311 1.1) API for [[Representational State Transfer|RESTful]] Web service development
*[[JavaScript]] programming model for service and client development
*[[Apache Maven|Maven]] tooling
*[[CORBA]] support
*[[HTTP]] and JMS transport layers
*Embeddable Deployment:
**ServiceMix or other [[JBI]] containers
**Geronimo or other [[Java EE]] containers
**[[Apache Tomcat|Tomcat]] or other servlet containers
**[[OSGi]]
*Reference [[OSGi]] Remote Services implementation

==Commercial support==
Enterprise support for CXF is available from independent vendors, including [http://www.redhat.com/support/we-are/ Red Hat], [http://www.jboss.com/products/platforms/application/components/ JBoss], [http://talend.com/ Talend], and [http://www.sosnoski.com Sosnoski Software Associates]. See the [http://cxf.apache.org/support.html CXF Support Page] for details on all support options.

==See also==
{{Portal|Java}}
*[[JAX-WS|JAX-WS RI]] The reference implementation of the JAX-WS specification, used directly by [[GlassFish Metro]]
*The [[Apache Axis|Axis Web Services framework]]
*[[Apache Wink]], a project in incubation with JAX-RS support.
*The [https://jsr311.dev.java.net/ JAX-RS] specification.
*[[List of web service frameworks]]

==Further reading==
*[http://www.javatips.net/blog/2011/09/cxf-web-service-tutorial CXF Web Service Tutorial]
*[http://www.javatips.net/blog/2011/09/create-cxf-client CXF Client]
*[http://www.javatips.net/blog/2012/02/cxf-restful-tutorial CXF Restful Tutorial]
*[http://www.javatips.net/blog/2012/02/cxf-restful-client CXF Restful Client]

==External links==
*[http://cxf.apache.org/ Apache CXF Web site]
*[http://cxf.apache.org/download.html Apache CXF Download]
*[http://cxf.apache.org/docs/index.html Apache CXF Documentation]
*[https://www.jboss.org/products/fuse.html JBoss Fuse Web site]
*[http://xfire.codehaus.org/XFire+and+Celtix+Merge Announcement of the Merger by Codehaus]
*[http://www.methodsandtools.com/tools/tools.php?apachecxf Apache CXF Evaluation]
*[http://www.oreillynet.com/onjava/blog/2007/07/apache_cxf_interview_with_dan.html Apache CXF: Interview with Dan Diephouse and Paul Brown]
*[http://www.talend.com/products/esb-standard-edition.php Talend ESB home page]
*[http://cxf.apache.org/people.html List of Apache CXF Committers]

{{Apache}}
{{Java API for RESTful Web Services}}

[[Category:Apache Software Foundation|CXF]]
[[Category:Web services]]
[[Category:Web service specifications]]
[[Category:Web applications]]
[[Category:Java enterprise platform]]
[[Category:Java libraries]]</text>
      <sha1>6xwe7dwqtmk54m9e53ch8g0crc2s08u</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hortonworks</title>
    <ns>0</ns>
    <id>35142247</id>
    <revision>
      <id>590009793</id>
      <parentid>586408423</parentid>
      <timestamp>2014-01-10T01:30:10Z</timestamp>
      <contributor>
        <ip>141.206.173.2</ip>
      </contributor>
      <text xml:space="preserve" bytes="7842">{{Infobox company
| name             = Hortonworks
| logo             = File:Hortonworks logo.gif
| caption          = 
| type             = Private
| foundation          = 2011
| headquarters     = 
| location_country = 
| location         = [[Palo Alto, California]] 
| locations        = 
| area_served      = 
| key_people       = &lt;!-- Eric Baldeschweiler, Founder &amp; CTO, Rob Bearden, CEO only if notable and cited --&gt;
| industry         = Computer software
| products         = Hortonworks Data Platform
| services         = 
| homepage         = {{URL|www.hortonworks.com}}
}}

'''Hortonworks''' is a business computer software company based in [[Palo Alto, California]]. The company focuses on the development and support of [[Apache Hadoop]], a framework that allows for the [[distributed processing]] of large data sets across clusters of computers.

==History==

Hortonworks was formed in June 2011 funded by $23 million from  [[Yahoo!]] and [[Benchmark Capital]] as an independent company.&lt;ref name=&quot;info&quot;&gt;{{Cite news |title= Hadoop Big Data Startup Spins Out Of Yahoo |work= Information Week |date= June 29, 2011 |author= Charles Babcock |url= http://www.informationweek.com/news/development/database/231000658 &lt;!-- pay site --&gt; |deadurl=no |archiveurl= http://web.archive.org/web/20110704055947/http://www.informationweek.com/news/development/database/231000658 |archivedate= July 4, 2011 |accessdate= October 14, 2013 }}&lt;/ref&gt;  The company employs contributors to the [[open source software]] project [[Apache Hadoop]].&lt;ref&gt;{{cite news |url= http://www.cnbc.com/id/47123495 |title= Big-data investors look for the next Splunk |work= [[CNBC]] |accessdate=24 April 2012}}&lt;/ref&gt; 
The company was named after [[Horton the Elephant]] of the ''[[Horton Hears a Who!]]'' book.
Eric Baldeschweiler was chief executive, and Rob Bearden chief operating officer, formerly from [[SpringSource]].&lt;ref name=&quot;info&quot; /&gt;
Additional investors included a $25 million round led by [[Index Ventures]] in November 2011.&lt;ref&gt;{{Cite news |title= Everything you ever wanted to know about Yahoo’s Hadoop spinoff Hortonworks |author= Derrick Harris |work= Giga Om |date= August 20, 2013 |url= http://gigaom.com/2013/08/20/everything-you-ever-wanted-to-know-about-yahoo-spinoff-hortonworks/ |accessdate= October 14, 2013 }}&lt;/ref&gt;

Hortonworks is a sponsor of the [[Apache Software Foundation]].&lt;ref&gt;[http://www.apache.org/foundation/thanks.html Sponsors], Apache Software Foundation&lt;/ref&gt;

[[Forrester Research]] named Hortonworks “the technology leader and ecosystem builder for the entire Hadoop industry.”&lt;ref&gt;[http://www.marketwatch.com/story/hortonworks-recognized-as-a-leader-in-the-enterprise-hadoop-solutions-industry-by-independent-research-firm-2012-02-06 Hortonworks Recognized as a Leader in the Enterprise Hadoop Solutions Industry by Independent Research Firm], MarketWatch, 2011-2-6&lt;/ref&gt;

Hortonworks' product named Hortonworks Data Platform (HDP) includes Apache Hadoop and is used for storing, processing, and analyzing large volumes of data. The platform is designed to deal with data from many sources and formats.
The platform includes various [[Apache Hadoop]] projects including the Hadoop Distributed File System, [[MapReduce]], Pig, Hive, [[HBase]] and Zookeeper and additional components.&lt;ref&gt;{{Cite news |title= HortonWorks Hones a Hadoop Distribution |work= PC World |date= November 1, 2011 |author= Joab Jackson |url= http://www.pcworld.com/article/242916/hortonworks_hones_a_hadoop_distribution.html |accessdate= October 14, 2013 }}&lt;/ref&gt;

In October 2011 Hortonworks announced [[Microsoft]] would collaborate on a Hadoop distribution for [[Windows Azure]] and [[Windows Server]].&lt;ref&gt;{{Cite news |title= Microsoft climbs onto Hadoop bandwagon  |date= October 12, 2011 |author= Jaikumar Vijayan |work= Computer World |url= http://www.computerworld.com/s/article/9220779/Microsoft_climbs_onto_Hadoop_bandwagon |accessdate= October 14, 2013 }}&lt;/ref&gt;
On February 25, 2013, Hortonworks announced availability of a beta version of the Hortonworks Data Platform for Windows.{{Citation needed |date= October 2013}}

In November 2011 it announced HParser software from [[Informatica]] would be available for free download by its customers.&lt;ref&gt;{{Cite news |title= Informatica, Hortonworks Team Up to Release Free Data Parser for Hadoop |work= e Week |author= Chris Preimesberger  |date= November 2, 2011 |url= http://www.eweek.com/c/a/Enterprise-Applications/Informatica-Hortonworks-Team-Up-to-Release-Free-Data-Parser-for-Hadoop-788698 |accessdate= October 14, 2013 }}&lt;/ref&gt;

In February 2012 [[Teradata]] announced an alliance.&lt;ref&gt;{{Cite news |title= Teradata and Hortonworks Join Forces for a Big Data Boost |date= February 21, 2012 |author= Quentin Hardy |work= Bits blog |publisher= The New York Times |url= http://bits.blogs.nytimes.com/2012/02/21/teradata-and-hortonworks-join-forces-for-a-big-data-boost |accessdate= October 14, 2013 }}&lt;/ref&gt;
In October 2012 Teradata's [[Aster Data Systems]] division announced an appliance supporting Hortonworks' distribution,&lt;ref&gt;{{Cite news |title= It's happening: Hadoop and SQL worlds are converging |author= Tony Baer |date= October 26, 2012 |work= ZDNet |url=http://www.zdnet.com/its-happening-hadoop-and-sql-worlds-are-converging-7000006455/ |accessdate= October 14, 2013 }}&lt;/ref&gt;&lt;ref&gt;{{Cite news |title= Teradata Big Analytics Appliance Enables New Business Insights on All Enterprise Data |date= October 17, 2012 |publisher= Teradata Aster  |work= Press release  |url= http://www.asterdata.com/news/teradata-aster-big-analytics-appliance-breaks-down-barriers.php |accessdate= October 14, 2013 }}&lt;/ref&gt;
and [[Impetus Technologies]] announced a partnership.&lt;ref&gt;{{Cite news |title= Impetus and Hortonworks Strategic Partnership |work= Press release |date=  October 2, 2012  |author= Pletus |url= http://finance.yahoo.com/news/impetus-hortonworks-strategic-partnership-103000189.html |accessdate= October 14, 2013 }}&lt;/ref&gt;

In June 2013 Hortonworks raised another $50 million in financing, from  previous investors and adding [[Tenaya Capital]] and Dragoneer Investment Group.&lt;ref&gt;{{Cite news |title= Hortonworks Raises $50M For Expansion And Development In Growing Hadoop Oriented Data Analytics Market |author= Alex Williams |date= June 25, 2013 |work= Tech Crunch |url= http://techcrunch.com/2013/06/25/hortonworks-raises-50m-for-expansion-and-development-in-growing-hadoop-oriented-data-analytics-market/ |accessdate= October 14, 2013 }}&lt;/ref&gt;
In September 2013 [[SAP AG]] announced it would resell the Hortonworks distribution (as well as another one).&lt;ref&gt;{{Cite news |title= SAP looks to boost 'big data' position with Hadoop deals, new apps |quote= SAP agrees to resell Hadoop distributions from Intel and Hortonworks; unveils specialized use case apps |date= September 11, 2013 |author= Chris Kanaracus |work= Computer World |url= http://www.computerworld.com/s/article/9242337/SAP_looks_to_boost_big_data_position_with_Hadoop_deals_new_apps |accessdate= October 14, 2013 }}&lt;/ref&gt;

Hortonworks hosts the Hadoop Summit community event, along with Yahoo!.&lt;ref&gt;{{Cite web |title= Sponsors |work= Hadoop Summit 2013 web site |deadurl=no |url= http://hadoopsummit.org/san-jose/sponsors/ |archiveurl= http://web.archive.org/web/20130430202257/http://hadoopsummit.org/san-jose/sponsors/ |archivedate= April 30, 2013 |accessdate= October 14, 2013 }}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
* {{Official |www.hortonworks.com}}

[[Category:Software companies of the United States]]
[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Companies based in Sunnyvale, California]]
[[Category:Privately held companies in the United States]]
[[Category:Hadoop]]
[[Category:Apache Software Foundation]]
[[Category:Companies established in 2011]]</text>
      <sha1>1o4iatfuf3k02b4a6rxce5jaky83h1v</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>OpenCMIS</title>
    <ns>0</ns>
    <id>35624889</id>
    <revision>
      <id>572858546</id>
      <parentid>572733575</parentid>
      <timestamp>2013-09-14T08:14:29Z</timestamp>
      <contributor>
        <username>RussBot</username>
        <id>279219</id>
      </contributor>
      <minor/>
      <comment>Robot: Change redirected category [[:Category:Open Source|Open Source]] to [[:Category:Free software|Free software]]</comment>
      <text xml:space="preserve" bytes="978">'''OpenCMIS''' is a subproject of the Apache Chemistry project of the Apache Software Foundation (ASF).
It is an [[Open Source]] collection of Java libraries, frameworks and tools around the [[Content Management Interoperability Services|CMIS]] specification.

The goal of OpenCMIS is to make CMIS simple for client and server Java developers. It hides the binding details and provides APIs and SPIs on different abstraction levels. It also includes test tools for content repository developers and client application developers.

The OpenCMIS products are:

* A server-side Java library
* A client-side Java library
* A client-side Android library
* CMIS Workbench: a CMIS heavy client, that allows all CMIS operations, and allows to run the TCK unit tests
* OpenCMIS Server Webapps: a CMIS server and web user interface
* OpenCMIS JCR Repository: A wrapper to use JCR repositories via CMIS
* OpenCMIS Bridge


[[Category:Apache Software Foundation]]
[[Category:Free software]]</text>
      <sha1>8udub2rdzau8642megsrcsuilrl9fss</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Struts</title>
    <ns>0</ns>
    <id>453331</id>
    <revision>
      <id>599264507</id>
      <parentid>596865225</parentid>
      <timestamp>2014-03-12T09:48:29Z</timestamp>
      <contributor>
        <username>Jmartinlll</username>
        <id>20612567</id>
      </contributor>
      <comment>Splitting again (see talk page)</comment>
      <text xml:space="preserve" bytes="6181">{{More footnotes|date=April 2009}}
{{Infobox software
| name                   = Apache Struts
| logo                   = [[File:Struts logo.gif|frameless|Apache Struts Logo]]
| author                 = [[Craig McClanahan]]
| developer              = [[Apache Software Foundation]]
| released               = {{Start date|2000|05}}
| discontinued           = {{Start date and age|2013|04|05}}&lt;ref&gt;[http://struts.apache.org/struts1eol-press.html Apache Struts 1 EOL Press Release]&lt;/ref&gt;
| latest release version = 1.3.10
| latest release date    = {{release date|2008|12|08}}
| status                 = [[End-of-life (product)|End-of-life]]&lt;ref&gt;[http://struts.apache.org/struts1eol-announcement.html Apache Struts 1 EOL Announcement]&lt;/ref&gt;
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| platform               = [[Cross-platform]] ([[Java Virtual Machine|JVM]])
| license                = [[Apache License]] 2.0
| website                = {{URL|http://struts.apache.org/development/1.x/}}
}}
'''Apache Struts''' was an [[open-source]] [[web application framework]] for developing [[Java EE]] [[web application]]s. It uses and extends the [[Java Servlet]] [[application programming interface|API]] to encourage developers to adopt a [[model–view–controller]] (MVC) architecture. It was originally created by [[Craig McClanahan]] and donated to the [[Apache Foundation]] in May, 2000. Formerly located under the Apache [[Jakarta Project]] and known as '''Jakarta Struts''', it became a top-level Apache project in 2005.

The [[WebWork]] framework spun off from Apache Struts aiming to offer enhancements and refinements while retaining the same general architecture of the original Struts framework. However, it was announced in December 2005 that Struts would re-merge with WebWork. WebWork 2.2 has been adopted as [[Apache Struts 2]], which reached its first full release in February 2007.

== Design goals and overview ==
In a standard [[Java Platform, Enterprise Edition|Java EE]] web application, the client will typically call to the server via a [[Form (web)|web form]]. The information is then either handed over to a [[Java Servlet]] which interacts with a database and produces an [[HTML]]-formatted response, or it is given to a [[JavaServer Pages]] (JSP) document that intermingles HTML and Java code to achieve the same result.
Both approaches are often considered inadequate for large projects because they mix application logic with presentation and make maintenance difficult.

The goal of Struts is to separate the ''model'' (application logic that interacts with a database) from the ''view'' (HTML pages presented to the client) and the ''controller'' (instance that passes information between view and model). Struts provides the controller (a servlet known as &lt;code&gt;ActionServlet&lt;/code&gt;) and facilitates the writing of templates for the view or presentation layer (typically in JSP, but [[XML]]/[[Extensible Stylesheet Language Transformations|XSLT]] and [[Jakarta Velocity|Velocity]] are also supported). The web application programmer is responsible for writing the model code, and for creating a central configuration file &lt;code&gt;struts-config.xml&lt;/code&gt; that binds together model, view, and controller.

Requests from the client are sent to the controller in the form of &quot;Actions&quot; defined in the configuration file; if the controller receives such a request it calls the corresponding Action class that interacts with the application-specific model code. The model code returns an &quot;ActionForward&quot;, a string telling the controller what output page to send to the client. Information is passed between model and view in the form of special [[JavaBeans]]. A powerful custom tag library allows it to read and write the content of these beans from the presentation layer without the need for any embedded Java code.

Struts is categorized as a [[Model 2]] request-based web application framework.&lt;ref&gt;{{cite web|first = Tony|last = Shan|year = 2006|accessdate = 2010-10-10|url = http://portal.acm.org/citation.cfm?id=1190953|title = Taxonomy of Java Web Application Frameworks|publisher = Proceedings of 2006 IEEE International Conference on e-Business Engineering (ICEBE 2006)}}&lt;/ref&gt;

Struts also supports [[Internationalization and localization|internationalization]] by web forms, and includes a template mechanism called &quot;Tiles&quot; that (for instance) allows the presentation layer to be composed from independent header, footer, menu navigation and content components.

== See also ==
[[Comparison of web application frameworks]]

== References ==
{{Reflist}}

== Bibliography ==
{{Refbegin}}
* [[James Holmes (programmer)|James Holmes]]: &lt;cite&gt;Struts: The Complete Reference&lt;/cite&gt;, McGraw-Hill Osborne Media, ISBN 0-07-223131-9
* Bill Dudney and Jonathan Lehr: &lt;cite&gt;Jakarta Pitfalls&lt;/cite&gt;, Wiley, ISBN 978-0-471-44915-7
* [[Bill Siggelkow]]: &lt;cite&gt;Jakarta Struts Cookbook&lt;/cite&gt;, O'Reilly, ISBN 0-596-00771-X
* [[James Goodwill]], [[Richard Hightower]]: &lt;cite&gt;Professional Jakarta Struts&lt;/cite&gt;, [[Wrox Press]], ISBN 0-7645-4437-3
* John Carnell and [[Rob Harrop]]: &lt;cite&gt;Pro Jakarta Struts, Second Edition&lt;/cite&gt;, Apress, ISBN 1-59059-228-X
* John Carnell, [[Jeff Linwood]] and [[Maciej Zawadzki]]: &lt;cite&gt;Professional Struts Applications: Building Web Sites with Struts, ObjectRelationalBridge, Lucene, and Velocity&lt;/cite&gt;, Apress, ISBN 1-59059-255-7
* [[Ted Husted]], etc.: &lt;cite&gt;Struts in Action&lt;/cite&gt;, Manning Publications Company, ISBN 1-930110-50-2
* [http://www.softwaresummit.com/2003/speakers/AshleyStrutsView.pdf Struts View Assembly and Validation], (PDF format).
* Stephan Wiesner: &lt;cite&gt;Learning Jakarta Struts 1.2&lt;/cite&gt;, Packt Publishing, 2005 ISBN 1-904811-54-X
{{Refend}}

== External links ==
* {{Official website|http://struts.apache.org/development/1.x/}}
{{Application frameworks}}
{{apache}}

[[Category:Apache Software Foundation|Struts]]
[[Category:Cross-platform free software]]
[[Category:Free software programmed in Java]]
[[Category:Java enterprise platform|Struts]]
[[Category:Web application frameworks|Struts]]
[[Category:Software using the Apache license]]</text>
      <sha1>aegtiee6irvzjyn061kghroiqanuxcb</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>CouchDB</title>
    <ns>0</ns>
    <id>13427539</id>
    <revision>
      <id>593863372</id>
      <parentid>592316138</parentid>
      <timestamp>2014-02-04T09:32:35Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>/* External links */Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated date parameter errors]]</comment>
      <text xml:space="preserve" bytes="18278">{{Infobox software
| name                   = Apache CouchDB
| logo                   = 
| screenshot             = [[File:Couchdb screenshot.png|300px]]
| caption                = CouchDB's Futon Administration Interface, User database
| collapsible            =
| author                 = Damien Katz, Jan Lehnardt, Noah Slater, Christopher Lenz, J. Chris Anderson, Paul Davis, Adam Kocoloski, Jason Davies, Benoît Chesneau, Filipe Manana, Robert Newson
| developer              = [[Apache Software Foundation]]
| status                 = Active
| released               = 2005
| frequently updated- Release version update? Don't edit this page, just click on the version number! --&gt;
| programming language   = [[Erlang (programming language)|Erlang]]
| operating system       = [[Cross-platform]]
| latest release version = 1.5.0
| latest release date    = {{release date|2013|11|05}} &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| latest preview version = 
| latest preview date    = 
| language               = 
| genre                  = [[Document-oriented database]]
| license                = [[Apache License]]
| website                = {{URL|http://couchdb.apache.org/}}
}}
[[File:LYME_software_bundle.svg|thumb|300px|The '''[[LYCE (software bundle)]]''' is based on [[Erlang (programming language)|Erlang]] and comprises '''CouchDB'''. It's entirely composed of [[free and open-source software]].]]

'''Apache CouchDB''', commonly referred to as '''CouchDB''', is an [[Open-source software|open source]] database that focuses on ease of use and on being &quot;a database that completely embraces the web&quot;.&lt;ref name=&quot;official-website&quot;&gt;{{cite web|last=Apache Software Foundation|title=Apache CouchDB|url=http://couchdb.apache.org/|accessdate=15 April 2012}}&lt;/ref&gt; It is a [[NoSQL]] database that uses [[JSON]] to store data, [[JavaScript]] as its query language using [[MapReduce]], and [[HTTP]] for an [[API]].&lt;ref name=official-website/&gt;  One of its distinguishing features is [[multi-master replication]]. CouchDB was first released in 2005 and later became an [[Apache Software Foundation|Apache]] project in 2008.

Unlike in a [[relational database]], CouchDB does not store data and relationships in tables. Instead, each database is a collection of independent documents. Each document maintains its own data and self-contained schema. An application may access multiple databases, such as one stored on a user's mobile phone and another on a server. Document metadata contains revision information, making it possible to merge any differences that may have occurred while the databases were disconnected.

CouchDB implements a form of Multi-Version Concurrency Control ([[Multiversion concurrency control|MVCC]]) in order to avoid the need to lock the database file during writes. Conflicts are left to the application to resolve. Resolving a conflict generally involves first merging data into one of the documents, then deleting the stale one.&lt;ref&gt;{{cite web|last=Smith|first=Jason|title=What is the CouchDB replication protocol? Is it like Git?|url=http://stackoverflow.com/a/4766398/395287|work=StackOverflow|publisher=Stack Exchange|accessdate=14 April 2012}}&lt;/ref&gt;

Other features include document-level [[ACID]] semantics with [[eventual consistency]], (incremental) [[MapReduce]], and (incremental) replication. Administration is supported with a built-in web application called Futon.

== History ==
'''CouchDB''' (''Couch'' is an acronym for ''cluster of unreliable commodity hardware'')&lt;ref&gt;[http://www.ibm.com/developerworks/opensource/library/os-couchdb/index.html Exploring CouchDB], article from IBM Developer Works&lt;/ref&gt;  is a project created in April 2005 by [[Damien Katz]], former [[Lotus Notes]] developer at [[IBM]].  Damien Katz defined it as a &quot;storage system for a large scale object database&quot;. His objectives for the database were to become the database of the Internet and that it would be designed from the ground up to serve web applications. He self-funded the project for almost two years and released it as an open source project under the [[GNU General Public License]].

In February 2008, it became an [[Apache Incubator]] project and the license was changed to the [[Apache License]].&lt;ref&gt;[http://mail-archives.apache.org/mod_mbox/incubator-general/200802.mbox/%3c3d4032300802121136p361b52ceyfc0f3b0ad81a1793@mail.gmail.com%3e Apache mailing list announcement] on mail-archives.apache.org&lt;/ref&gt; A few months after, it graduated to a top-level project.&lt;ref&gt;[http://mail-archives.apache.org/mod_mbox/incubator-couchdb-dev/200811.mbox/%3c3F352A54-5FC8-4CB0-8A6B-7D3446F07462@jaguNET.com%3e Re: Proposed Resolution: Establish CouchDB TLP] on mail-archives.apache.org&lt;/ref&gt; This led to the first stable version being released in July 2010.&lt;ref&gt;[http://www.pcworld.com/businesscenter/article/201046/couchdb_nosql_database_ready_for_production_use.html &quot;CouchDB NoSQL Database Ready for Production Use&quot;], article from PC World of Jully 2010&lt;/ref&gt;

In early 2012, Damien Katz left the project to focus on [[Couchbase Server]].&lt;ref&gt;{{cite web|last=Katz|first=Damien|title=The future of CouchDB|url=http://damienkatz.net/2012/01/the_future_of_couchdb.html|accessdate=15 April 2012}}&lt;/ref&gt;

Since the departure of Damien Katz, the Apache CouchDB project has continued, releasing 1.2 in April 2012 and 1.3 in April 2013. In July 2013, the CouchDB community merged the codebase for [[BigCouch]], [[Cloudant]]'s clustered version of CouchDB, into the Apache project. The BigCouch clustering framework is prepared to be included in an upcoming release of Apache CouchDB.&lt;ref&gt;{{cite web|last=Slater|first=Noah|title=Welcome BigCouch|url=https://blogs.apache.org/couchdb/entry/welcome_bigcouch|accessdate=25 July 2013}}&lt;/ref&gt;

== Main features ==
; Document Storage
: CouchDB stores data as &quot;documents&quot;, as one or more field/value pairs expressed as [[JSON]]. Field values can be simple things like strings, numbers, or dates; but [[Array data structure|ordered lists]] and [[associative array]]s can also be used. Every document in a CouchDB database has a unique id and there is no required document schema.
; ACID Semantics
: CouchDB provides [[atomicity, consistency, isolation, durability|ACID]] semantics.&lt;ref name=&quot;ACID&quot;&gt;[http://couchdb.apache.org/docs/overview.html CoachDB, Technical Overview]&lt;/ref&gt; It does this by implementing a form of [[Multi-Version Concurrency Control]], meaning that CouchDB can handle a high volume of concurrent readers and writers without conflict.
; Map/Reduce Views and Indexes
: The stored data is structured using views. In CouchDB, each view is constructed by a [[JavaScript]] function that acts as the Map half of a [[map (higher-order function)|map]]/reduce operation. The function takes a document and transforms it into a single value which it returns. CouchDB can index views and keep those indexes updated as documents are added, removed, or updated.
; Distributed Architecture with Replication
: CouchDB was designed with bi-direction replication (or synchronization) and off-line operation in mind. That means multiple replicas can have their own copies of the same data, modify it, and then sync those changes at a later time.
; REST API
: All items have a unique URI that gets exposed via HTTP. [[REST]] uses the [[Hypertext Transfer Protocol#Request methods|HTTP methods]] POST, GET, PUT and DELETE for the four basic [[Create, read, update and delete|CRUD]] (Create, Read, Update, Delete) operations on all resources.
; Eventual Consistency
: CouchDB guarantees [[eventual consistency]] to be able to provide both availability and partition tolerance.
; Built for Offline
: CouchDB can replicate to devices (like smartphones) that can go offline and handle data sync for you when the device is back online.

CouchDB also offers a built-in administration interface accessible via web called Futon.&lt;ref&gt;[http://guide.couchdb.org/draft/tour.html#welcome &quot;Welcome to Futon&quot;] from &quot;CouchDB The Definitive Guide&quot;&lt;/ref&gt;

== Use cases &amp; production deployments ==
Replication and synchronization capabilities of CouchDB make it ideal for using it in mobile devices, where network connection is not guaranteed but the application must keep on working offline.

CouchDB is well suited for applications with accumulating, occasionally changing data, on which pre-defined queries are to be run and where versioning is important (CRM, CMS systems, by example). Master-master replication is an especially interesting feature, allowing easy multi-site deployments.&lt;ref name=&quot;Analyse Kristof Kovacs&quot;&gt;[http://kkovacs.eu/cassandra-vs-mongodb-vs-couchdb-vs-redis Cassandra vs MongoDB vs CouchDB vs Redis vs Riak vs HBase comparison] from Kristóf Kovács&lt;/ref&gt;

=== Enterprises that use CouchDB ===

A few examples of enterprises that used or are using CouchDB are:
* [[Ubuntu (operating system)|Ubuntu]] began using it in 2009 for its synchronization service &quot;Ubuntu One&quot;&lt;ref&gt;[http://mail-archives.apache.org/mod_mbox/couchdb-dev/200910.mbox/%3C4AD53996.3090104@canonical.com%3E Email from Elliot Murphy (Canonical)] to the CouchDB-Devel list&lt;/ref&gt; but stopped using it in November 2011.&lt;ref&gt;[http://linux.slashdot.org/story/11/11/22/171228/canonical-drops-couchdb-from-ubuntu-one Canonical Drops CouchDB From Ubuntu One (Slashdot)]&lt;/ref&gt;
* The [[BBC]], for its dynamic content platforms&lt;ref&gt;[http://www.erlang-factory.com/conference/London2009/speakers/endafarrell CouchDB at the BBC as a fault tolerant, scalable, multi-data center key-value store]&lt;/ref&gt;
* [[Credit Suisse]], for internal use at commodities department for their marketplace framework.&lt;ref name=&quot;couchdbinthewild&quot;&gt;[http://wiki.apache.org/couchdb/CouchDB_in_the_wild &quot;CouchDB in the wild&quot;] article of the product's web, a list of software projects and websites using CouchDB&lt;/ref&gt;
* [[Meebo]], for their social platform (web and applications) - Meebo was acquired by Google and was shut down on July 12, 2012.

A more comprehensive list of software projects and web sites that use CouchDB is available on the Apache projects's wiki&lt;ref&gt;[http://wiki.apache.org/couchdb/CouchDB_in_the_wild &quot;CouchDB in the wild&quot;]&lt;/ref&gt;&lt;ref name=&quot;couchdbinthewild&quot; /&gt;

== Data manipulation: documents and views ==
CouchDB manages a collection of [[JSON]] documents. The documents are organised via views. Views are defined with [[aggregate function]]s and filters are computed in parallel, much like [[MapReduce]].

Views are generally stored in the database and their indexes updated continuously. CouchDB supports a view system using external socket servers and a JSON-based protocol.&lt;ref name=&quot;Apache, View Server&quot; &gt;[http://wiki.apache.org/couchdb/ViewServer View Server Documentation] on wiki.apache.org&lt;/ref&gt; As a consequence, view servers have been developed in a variety of languages (JavaScript is the default, but there are also PHP, Ruby, Python and Erlang).

=== Accessing data via HTTP ===
Applications interact with CouchDB via HTTP. The following demonstrates a few examples using [[cURL]], a command-line utility. These examples assume that CouchDB is running on [[localhost]] (127.0.0.1) on port 5984.

{| class=&quot;wikitable&quot; 
|-
! Action !! Request !! Response
|-
| Accessing server information
|| &lt;syntaxhighlight lang=&quot;bash&quot;&gt;curl http://127.0.0.1:5984/&lt;/syntaxhighlight&gt; 
|| &lt;syntaxhighlight lang=&quot;javascript&quot;&gt;{
  &quot;couchdb&quot;: &quot;Welcome&quot;,
  &quot;version&quot;:&quot;1.1.0&quot;
}&lt;/syntaxhighlight&gt;
|-
| Creating a database named '''wiki'''
|| &lt;syntaxhighlight lang=&quot;bash&quot;&gt;curl -X PUT http://127.0.0.1:5984/wiki&lt;/syntaxhighlight&gt; 
|| &lt;syntaxhighlight lang=&quot;javascript&quot;&gt;{&quot;ok&quot;: true}&lt;/syntaxhighlight&gt;
|-
| Attempting to create a second database named '''wiki'''
|| &lt;syntaxhighlight lang=&quot;bash&quot;&gt;curl -X PUT http://127.0.0.1:5984/wiki&lt;/syntaxhighlight&gt; 
|| &lt;syntaxhighlight lang=&quot;javascript&quot; enclose=&quot;div&quot;&gt;{
  &quot;error&quot;:&quot;file_exists&quot;,
  &quot;reason&quot;:&quot;The database could not be created, the file already exists.&quot;
}&lt;/syntaxhighlight&gt;
|-
|| Retrieve information about the '''wiki''' database
|| &lt;syntaxhighlight lang=&quot;bash&quot;&gt;curl http://127.0.0.1:5984/wiki&lt;/syntaxhighlight&gt; 
|| &lt;syntaxhighlight lang=&quot;javascript&quot;&gt;{
  &quot;db_name&quot;: &quot;wiki&quot;,
  &quot;doc_count&quot;: 0,
  &quot;doc_del_count&quot;: 0,
  &quot;update_seq&quot;: 0,
  &quot;purge_seq&quot;: 0,
  &quot;compact_running&quot;: false,
  &quot;disk_size&quot;: 79,
  &quot;instance_start_time&quot;: &quot;1272453873691070&quot;,
  &quot;disk_format_version&quot;: 5
}&lt;/syntaxhighlight&gt;
|-
| Delete the database '''wiki'''
| &lt;syntaxhighlight lang=&quot;bash&quot;&gt;curl -X DELETE http://127.0.0.1:5984/wiki&lt;/syntaxhighlight&gt; 
| &lt;syntaxhighlight lang=&quot;javascript&quot;&gt;{&quot;ok&quot;: true}&lt;/syntaxhighlight&gt;
|-
| Create a document, asking CouchDB to supply a document id
| &lt;syntaxhighlight lang=&quot;bash&quot;&gt;curl -X POST -H &quot;Content-Type: application/json&quot; --data \
'{ &quot;text&quot; : &quot;Wikipedia on CouchDB&quot;, &quot;rating&quot;: 5 }' \
http://127.0.0.1:5984/wiki
&lt;/syntaxhighlight&gt;
| &lt;syntaxhighlight lang=&quot;javascript&quot;&gt;{
  &quot;ok&quot;: true,
  &quot;id&quot;: &quot;123BAC&quot;,
  &quot;rev&quot;: &quot;946B7D1C&quot;
}&lt;/syntaxhighlight&gt;
|}

==Open source components==
CouchDB includes a number of other open source projects as part of its default package.
{| class=&quot;wikitable sortable&quot; width = &quot;100%&quot;
! Component
! Description
! License
|-
| [[SpiderMonkey (JavaScript engine)|SpiderMonkey]]
| SpiderMonkey is a code name for the first ever [[JavaScript engine]], written by [[Brendan Eich]] at [[Netscape Communications Corporation|Netscape Communications]], later released as open source and now maintained by the [[Mozilla Foundation]].
| [[Mozilla Public License|MPL]]
|-
| [[jQuery]]
| jQuery is a lightweight [[cross-browser]] [[JavaScript library]] that emphasizes interaction between [[JavaScript]] and [[HTML]].
| [[Dual license]]: [[GNU General Public License|GPL]] and [[MIT License|MIT]]
|-
| [[International Components for Unicode|ICU]]
| International Components for Unicode (ICU) is an [[open source]] project of mature [[C (programming language)|C]]/[[C++]] and [[Java (programming language)|Java]] libraries for [[Unicode]] support, software [[internationalization]] and software globalization. ICU is widely portable to many operating systems and environments.
| [[MIT License]]
|-
| [[OpenSSL]]
| OpenSSL is an [[open source]] implementation of the [[Transport Layer Security|SSL and TLS]] protocols.  The core [[library (computer science)|library]] (written in the [[C (programming language)|C programming language]]) implements the basic [[cryptography|cryptographic]] functions and provides various utility functions.
| [[Apache License|Apache]]-like unique
|-
| [[Erlang (programming language)|Erlang]]
| Erlang is a general-purpose [[concurrent computing|concurrent]] [[programming language]] and [[Run time system|runtime]] system. The sequential subset of Erlang is a [[functional language]], with [[strict evaluation]], [[single assignment]], and [[dynamic typing]].
|  Modified [[Mozilla Public License|MPL]]
|}

== See also ==
{{Portal|Free software}}

* [[LYCE (software bundle)]]
* [[Apache Accumulo|Accumulo]]
* [[BigCouch]]
* [[Cassandra (database)]]
* [[Couchbase Server]]
* [[Cloudant]]
* [[Document-oriented database]]
* [[Lotus Notes]]
* [[MongoDB]]
* [[Redis]]
* [[Mnesia]]
* [[OrientDB]]
* [[Riak]]
* [[XML database]]

==References==
{{Reflist}}

==Bibliography==
{{Refbegin}}
* {{citation
| first1 = J. Chris | last1 = Anderson | first2 = Noah | last2 = Slater | first3 = Jan | last3 = Lehnardt | date = November 15, 2009 | title = CouchDB: The Definitive Guide | edition = 1st | publisher = [[O'Reilly Media]] | pages = 300 | isbn = 0-596-15816-5 | url = http://guide.couchdb.org/editions/1/en/index.html }}
* {{citation | first1 = Joe | last1 = Lennon | date = December 15, 2009 | title = Beginning CouchDB | edition = 1st | publisher = [[Apress]] | pages = 300 | isbn = 1-4302-7237-6 | url = http://www.apress.com/book/view/9781430272373 }}
* {{citation | first1 = Bradley | last1 = Holt | date = March 7, 2011 | title = Writing and Querying MapReduce Views in CouchDB | edition = 1st | publisher = [[O'Reilly Media]] | pages = 76 | isbn = 1-4493-0312-9| url = http://oreilly.com/catalog/0636920018247 }}
* {{citation | first1 = Bradley | last1 = Holt | date = April 11, 2011 | title = Scaling CouchDB | edition = 1st | publisher = [[O'Reilly Media]] | pages = 72 | isbn = 1-4493-0343-9 | url = http://oreilly.com/catalog/9781449303433}}
* {{citation | first1 = MC | last1 = Brown | date = October 31, 2011 | title = Getting Started with CouchDB | edition = 1st | publisher = [[O'Reilly Media]] | pages = 50 | isbn = 1-4493-0755-8 | url = http://oreilly.com/catalog/9781449307554}}
* {{citation | first1 = Mick | last1 = Thompson | date = August 2, 2011 | title = Getting Started with GEO, CouchDB, and Node.js | edition = 1st | publisher = [[O'Reilly Media]] | pages = 64 | isbn = 1-4493-0752-3 | url = http://oreilly.com/catalog/9781449307523}}
{{Refend}}

==External links==
*{{Official website|couchdb.apache.org}}
*[http://books.couchdb.org/relax/ CouchDB: The Definitive Guide]
*[http://wiki.apache.org/couchdb/Complete_HTTP_API_Reference Complete HTTP API Reference]
*[https://github.com/1999/couchdb-php Simple PHP5 library to communicate with CouchDB]
*[http://code.google.com/p/async-couchdb-client/ Asynchronous CouchDB client for Java]
*[https://github.com/KimStebel/sprouch Asynchronous CouchDB client for Scala]
*{{cite web|last=Lehnardt|first=Jan|title=Couch DB at 10,000 feet|url=http://video.google.com/videoplay?docid=-3714560380544574985&amp;hl=en#|work=Erlang eXchange 2008|accessdate=15 April 2012|year=2008}}
*{{cite web|last=Lenhardt|first=Jan|title=CouchDB for Erlang Developers|work=Erlang Factory 2009|url=http://www.erlang-factory.com/conference/London2009/speakers/janlehnardt|work=Erlang Factory London 2009|accessdate=15 April 2012|year=2009}}
*{{cite web|last=Katz|first=Damien|title=CouchDB and Me|url=http://www.infoq.com/presentations/katz-couchdb-and-me|work=RubyFringe|publisher=InfoQ|accessdate=15 April 2012|date=January 2009}}

{{Apache}}

[[Category:Erlang programming language]]
[[Category:Free database management systems]]
[[Category:Document-oriented databases]]
[[Category:Distributed computing architecture]]
[[Category:Structured storage]]
[[Category:NoSQL]]
[[Category:Unix network-related software]]
[[Category:Apache Software Foundation]]
[[Category:Free web server software]]
[[Category:Cross-platform software]]</text>
      <sha1>0nld3tu725gzu8wxzh4oefrf37btnni</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Stanbol</title>
    <ns>0</ns>
    <id>38167240</id>
    <revision>
      <id>595794826</id>
      <parentid>593471813</parentid>
      <timestamp>2014-02-16T23:13:45Z</timestamp>
      <contributor>
        <username>PKT</username>
        <id>1382933</id>
      </contributor>
      <comment>Unlinked ambiguous name: [[Sergio Fernández]]</comment>
      <text xml:space="preserve" bytes="10718">{{multiple issues|
{{primary sources|date=August 2013}}
{{notability|Products|date=August 2013}}
}}

{{Infobox software
| name                   = Apache Stanbol
| logo                   = [[File:Apache Stanbol - logo.png]]
| developer              = [[Apache Software Foundation]]
| programming language   = [[Java (programming language)|Java]]
| frequently updated     = yes
| status                 = Active
| license                = [[Apache License]] 2.0
| website                = {{URL|https://stanbol.apache.org/}}
}}

'''Apache Stanbol''' is an open source modular software stack and reusable set of components for semantic content management.
Apache Stanbol components are meant to be accessed over [[Representational state transfer|RESTful]] interfaces to provide semantic services for content management. Thus, one application is to extend traditional content management systems with (internal or external) semantic services.&lt;ref&gt;{{cite web|url=http://en.wikipedia.org/wiki/Apache_Stanbol|publisher=Apache Stanbol|title=Apache Stanbol|accessdate=2013-08-20}}&lt;/ref&gt;

Additionally, Apache Stanbol lets you create new types of content management systems with semantics at their core. The current code is written in [[Java (programming language)|Java]] and based on the [[OSGi]] component framework.
Applications include extending existing content management systems with (internal or external) semantic services, and creating new types of content management systems with semantics at their core.

==History==

In 2008 the [[Salzburg Research]] led, as entity coordinator, a consortium of seven research partners and six industrial partners to the proposal of the IKS project with the aim of receiving funding by the European institutions under the [[Seventh Framework Programme|7th Framework Programme]].&lt;ref&gt;{{cite web|url=http://wiki.iks-project.eu/index.php/FAQ|publisher=IKS|title=IKS FAQ|accessdate=2013-08-20}}&lt;/ref&gt;

The consortium comprised:&lt;ref&gt;{{cite web|url=http://wiki.iks-project.eu/index.php/FAQ|publisher=IKS|title=IKS FAQ|accessdate=2013-08-20}}&lt;/ref&gt;
* [[Salzburg Research]] (Coordinator), Austria
* [[DFKI]] - Forschungsinstitut für Künstliche Intelligenz, Germany
* [[Hochschule St. Gallen]], Switzerland
* [[Consiglio Nazionale delle Ricerche|CNR-ISTC]] - Consiglio Nazionale delle Ricerche, Italy
* Software Quality Lab, [[University of Paderborn]], Germany
* SRDC - Software Research and Development and Consultancy Ltd., Turkey
* [[Furtwangen University of Applied Sciences|Hochschule Furtwangen]], Germany
* [[Nuxeo]], France
* Alkacon Software GmbH, Germany
* TXT Polymedia, Italy
* Pisano Holding GmbH, Germany
* Nemein Oy, Finland
* [[Day Software|Day Software AG]], Switzerland

In January 2009, the [http://www.iks-project.eu/ Interactive Knowledge Stack] (IKS) started partly funded by the [[European Commission]] to provide an &quot;open source technology platform for semantically enhanced content management systems&quot;.&lt;ref&gt;{{cite web|url=http://wiki.iks-project.eu/index.php/Main_Page|title=IKS Project|publisher=IKS|accessdate=2013-08-20}}&lt;/ref&gt; IKS received €6.58m co-funding by the [[European Union]]&lt;ref&gt;{{cite web|url=http://www.iks-project.eu/about-us|publisher=IKS Project|title=About Us|accessdate=2013-08-20}}&lt;/ref&gt; for an overall project duration of 4 years, hence setting the project's end date by the end of 2012.&lt;ref&gt;{{cite web|url=http://www.iks-project.eu/community/partners/iks-eu-research-project|publisher=IKS Project|title=IKS EU Research Project|accessdate=2013-08-20}}&lt;/ref&gt;

Apache Stanbol was founded in November 2010 by members the EU research project [http://www.iks-project.eu/ Interactive Knowledge Stack] (IKS). It was the result of an ongoing discussion about how to ensure that the results, especially the developed software, of the IKS project would be available to vendors of content management systems (CMS) after the project’s official funding period ended in 2012. The members of the IKS project decided to initiate the Apache Stanbol project as part of the incubation program of the [[Apache Software Foundation]] (ASF).

One of the first code imports of Apache Stanbol was the so-called &quot;Furtwangen IKS Semantic Engine&quot; (FISE) which eventually became the Apache Stanbol Enhancer with its Enhancement Engines. Other contributions of code were the KReS (Knowledge Representation and Reasoning) and the RICK (Reference Infrastructure for Content and Knowledge) components. Later on followed the Contenthub, while KReS was split into the Apache Stanbol Ontology Manager and Reasoner components, and the RICK is today known as the Apache Stanbol Entityhub. From that moment Apache Stanbol was developed as an open source software project independent of the IKS research project.

On 15 November 2010 Apache Stanbol enters incubation.&lt;ref&gt;{{cite mailinglist|url=http://mail-archives.apache.org/mod_mbox/incubator-general/201011.mbox/%3CAANLkTimZbpCR2RLbJVzdGkBCV-iE1YhsfC4iPO400WAT@mail.gmail.com%3E|title=Accept Stanbol for incubation|last=Delacretaz|first= Bertrand|mailinglist= incubator-general |date=15 November 2010|accessdate=2013-08-20}}&lt;/ref&gt;

On 9 May 2012 version 0.9.0-incubating is released.&lt;ref&gt;{{cite mailinglist|url=http://markmail.org/message/grzlcpzmvv5wmeht|title=Apache Stanbol 0.9.0-incubating staging|last=Christ|first= Fabian|mailinglist=org.apache.incubator.stanbol-dev|date=9 May 2012|accessdate=2013-08-20}}&lt;/ref&gt;

On 10 July 2012 version 0.10.0-incubating is released.&lt;ref&gt;{{cite mailinglist|url=http://markmail.org/message/myrdbfszfotne6s5|title=Apache Stanbol Entityhub 0.10.0-incubating released|last=Christ|first= Fabian|mailinglist=org.apache.incubator.stanbol-dev|date=10 July 2012|accessdate=2013-08-20}}&lt;/ref&gt;

By the middle of 2012 Apache Stanbol had demonstrated that it has an active community and is able to produce software and releases according to the ASF standards. The board of directors of the ASF accepted the [http://www.apache.org/foundation/records/minutes/2012/board_minutes_2012_09_19.txt formal resolution] to establish Apache Stanbol as a top-level project on 2012-09-19.&lt;ref&gt;{{cite web|url=http://stanbol.apache.org/graduation-resolution.html|title=Graduation Resolution|publisher=Apache Stanbol|accessdate=2013-08-20}}&lt;/ref&gt;

On 5 March 2013 [[Salzburg Research]] announced that 8 entities, among those [[Sebastian Schaffert]] (head of the knowledge and media technologies group&lt;ref&gt;[http://www.salzburgresearch.at/person/schaffert-sebastian/ Dr. Sebastian Schaffert at Salzburg Research]&lt;/ref&gt;), [[Rupert Westenthaler]] (Stanbol initial committer and PMC) and Sergio Fernández (Stanbol commiter) set up an effort to deliver Apache Stanbol and [[Apache Marmotta]] services under the [[RedLink GmbH|Redlink]] brand.&lt;ref&gt;{{cite web|url=http://redlink.co/eight-players-joined-hands-to-commence-redlink-services-in-2013/|title= Eight players joined hands to commence services in April|publisher=Redlink|first=Andrea|last=Volpini|accessdate=2013-08-20}}&lt;/ref&gt;

==Bibliography==
{{Refbegin}}
*{{Cite book
| first1    =  Reto
| last1     = Bachmann-Gmur
| date      = July 26, 2013
| title     = Instant Apache Stanbol
| edition   = 1st
| publisher = [[Packt Publishing]]
| isbn      = 1783281235
| url       = http://www.packtpub.com/apache-stanbol/book
}}
*{{Cite book
| editors = Gurevych, Iryna; Kim, Jungi
| year = 2013
| title     = The People’s Web Meets NLP
| edition   = 1st
| publisher = [[Springer Science+Business Media|Springer]]
| isbn      = 978-3-642-35085-6
| url       = http://www.springer.com/education+%26+language/linguistics/book/978-3-642-35084-9
}}
*{{Cite book
| title     = Media Networks: Architectures, Applications, and Standards
| editors =  Hassnaa, Moustafa; Sherali, Zeadally
| publisher = [[CRC Press]]
| date      = May 14, 2012
| isbn      = 1439877289
| url       = http://www.crcpress.com/product/isbn/9781439877289
}}
*{{Cite book
| title     = Semantic Technologies in Content Management Systems
| editors =  Maass, Wolfgang; Kowatsch, Tobias
| publisher = [[Springer Science+Business Media|Springer]]
| year      = 2012
| isbn      = 978-3-642-24960-0
| url       = http://www.springer.com/business+%26+management/technology+management/book/978-3-642-21549-0
}}
*{{Cite book
| title     = Semantic Mashups
| editors =  Endres-Niggemeyer, Brigitte
| publisher = [[Springer Science+Business Media|Springer]]
| year      = 2013
| page     = 131
| isbn      = 978-3-642-36403-7
| url       = http://www.springer.com/computer/database+management+%26+information+retrieval/book/978-3-642-36402-0?cm_mmc=Google-_-Book+Search-_-Springer-_-0&amp;otherVersion=978-3-642-36403-7
}}
*{{Cite book
| title     = The Semantic Web -- ISWC 2011
| editors =  Aroyo, L.; Welty, C.; Alani, H.; Taylor, J.; Bernstein, A.; Kagal, L.; Noy, N.; Blomqvist, E.
| publisher = [[Springer Science+Business Media|Springer]]
| year      = 2011
| pages     = 191
| isbn      = 978-3-642-25093-4
| url       = http://www.springer.com/computer/communication+networks/book/978-3-642-25092-7?wt_mc=Google-_-Book%20Search-_-Springer-_-EN&amp;token=gbgen
}}
*{{Cite book
| title     = Nosotros, los constructores de la Web Semántica
| language = Spanish
| first1 = Luis
| last1 = Criado-Fernández
| editors = del Amor León-Fariña, María
| date      = 10 June 2013
| asin      = B00DC8IAJA
| asin-tld = es
| url       = http://www.amazon.es/Nosotros-los-constructores-Sem%C3%A1ntica-ebook/dp/B00DC8IAJA
}}
*{{Cite book
| author = Snml-Tng Salzburg Newmedialab
| date     = May 14, 2013
| title     = Zukunft Von Linked Media: Trends, Entwicklungen Und Visionen.
| edition   = 1st
| publisher = [[Salzburg Research|Salzburg Research Forschungsgesellschaft m.b.H]]
| page     = 9
| isbn      = 3902448369
| url       = http://de.slideshare.net/snml/zukunft-von-linked-media-trends-entwicklungen-und-visionen
| language = German
}}
*{{Cite book
| editors = Behrendt, Wernher; Damjanovic, Violeta
|date=March 2013
| title     = Developing Semantic CMS Applications - The IKS Handbook
| edition   = 1st
| publisher = [[Salzburg Research|Salzburg Research Forschungsgesellschaft m.b.H]]
| isbn      = 978-3-902448-35-4
| url       = http://www.iks-project.eu/sites/default/files/IKS%20Handbook%202013.pdf
}}
{{Refend}}

==References==
{{reflist}}

==External links==
* [https://stanbol.apache.org/ Official website]
* [http://wiki.apache.org/incubator/StanbolProposal Stanbol Incubation Proposal]
* [http://stanbol.apache.org/presentations/Stanbol_Overview_2012-04.pdf Presentation of Apache Stanbol]

{{Apache}}

{{DEFAULTSORT:Stanbol}}
[[Category:Apache Software Foundation]]
[[Category:Cross-platform software]]
[[Category:Free software programmed in Java]]
[[Category:Java programming language]]</text>
      <sha1>3c3x09i9p76eb0zrxr2m92pz944ot88</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Jakarta Project</title>
    <ns>0</ns>
    <id>55182</id>
    <revision>
      <id>594809481</id>
      <parentid>591883190</parentid>
      <timestamp>2014-02-10T11:20:50Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>/* Project name */[[WP:CHECKWIKI]] error fixes + other fixes using [[Project:AWB|AWB]] (9930)</comment>
      <text xml:space="preserve" bytes="2800">{{One source|date=February 2012}}
The '''Jakarta Project''' creates and maintains [[open source software]] for the [[Java platform]]. It operates as an umbrella project under the auspices of the [[Apache Software Foundation]], and all Jakarta products are released under the [[Apache License]]. As of December 21, 2011 the Jakarta project has been retired because no subprojects are remaining.

==Subprojects==&lt;!-- This section is linked from [[Comparison of regular expression engines]] --&gt;
Major contributions by the Jakarta Project include tools, [[library (software)|libraries]] and [[Software framework|frameworks]] such as:

*[[Byte Code Engineering Library|BCEL]] - a Java byte code manipulation library
*[[Bean Scripting Framework|BSF]] - a scripting framework
*[[Jakarta Cactus|Cactus]] - a unit testing framework for server-side Java classes
*[[Apache JMeter]] - a load- and stress-testing tool.

The following projects were formerly part of Jakarta, but now form independent projects within the Apache Software Foundation:
*[[Apache Ant|Ant]] - a [[build tool]]
*[[Apache Commons|Commons]] - a collection of useful classes intended to complement Java's standard library.
*[[Apache HiveMind|HiveMind]] - a services and configuration [[microkernel]]
*[[Apache Maven|Maven]] - a project build and management tool 
*[[Apache POI|POI]] - a pure [[Java (programming language)|Java]] port of Microsoft's popular file formats.
*[[Apache Struts|Struts]] - a web application development framework
*[[Jakarta Slide|Slide]] - a content repository primarily using [[WebDAV]].
*[[Tapestry (programming)|Tapestry]] - A component object model based on JavaBeans properties and strong specifications
*[[Apache Tomcat|Tomcat]] - a [[JavaServer Pages|JSP]]/[[Servlet]] container
*[[Apache Turbine|Turbine]] - a rapid development web application framework
*[[Apache Velocity|Velocity]] - a [[Template (programming)|template]] engine

==Project name==
Jakarta is not directly named after the [[Jakarta|capital city]] of [[Indonesia]], nor after the Jakarta blue butterfly species.{{which|date=October 2012}} Instead, it is named after the conference room at [[Sun Microsystems]] where the majority of discussions leading to the project's creation took place.&lt;ref&gt;{{cite web |url=http://www.javaworld.com/javaworld/jw-06-1999/jw-06-sunapache_p.html |title=Sun and Apache team up to deliver servlet and JSP code |author=Jason Hunter |date=June 1999 |work=[[JavaWorld]] |publisher=[[IDG]] |page=1 |accessdate=22 February 2012}}&lt;/ref&gt; This was in turn almost certainly named after the city, which is located on the island of [[Java]].

==References==
{{Reflist}}

==External links==
*[http://jakarta.apache.org/ The Jakarta home page]

{{apache}}

[[Category:Java platform]]
[[Category:Apache Software Foundation]]</text>
      <sha1>rwd6t5yh22o5fvmcmlrsdeqbki9izfn</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Xalan</title>
    <ns>0</ns>
    <id>93466</id>
    <revision>
      <id>598085480</id>
      <parentid>586174276</parentid>
      <timestamp>2014-03-04T09:30:31Z</timestamp>
      <contributor>
        <ip>179.234.179.107</ip>
      </contributor>
      <text xml:space="preserve" bytes="1420">{{ Infobox Software
| name                   = Apache Xalan
| logo                   = 
| screenshot             = 
| caption                = 
| developer              = [[Apache Software Foundation]] 
| latest_release_version = 
| latest_release_date    = 
| latest_preview_version =
| latest_preview_date    = 
| operating_system       = 
| programming_language   = [[C++]] and [[Java (programming language)|Java]]
| genre                  = [[software library]]
| license                = [[Apache License]] 2.0
| website                = http://xalan.apache.org
}}
'''Xalan''' is a popular [[open source]] [[software library]] from the [[Apache Software Foundation]], originally created by [[IBM]] under the name LotusXSL,&lt;ref&gt;[http://www.alphaworks.ibm.com/formula/lotusxsl/ alphaWorks : LotusXSL : Overview]&lt;/ref&gt; that implements the [[XSLT]] 1.0 [[XML]] transformation language and the [[XPath]] 1.0 language. The Xalan XSLT processor is available for both the [[Java (programming language)|Java]] and [[C++]] [[programming language]]s.

==See also==
* [[Java XML]]
* [[Xerces]]
* [[libxml2]]
* [[Saxon XSLT]]
== References ==

{{reflist|2}}

==External links==
*[http://xalan.apache.org Xalan Home page]
{{apache}}

[[Category:XSLT processors]]
[[Category:Apache Software Foundation]]
[[Category:Java platform]]
[[Category:Java libraries]]
[[Category:Software using the Apache license]]


{{compu-library-stub}}</text>
      <sha1>4or2bx1cvn97kpqaxx7i24p9wr5foqw</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Jcrom</title>
    <ns>0</ns>
    <id>39890780</id>
    <revision>
      <id>575632703</id>
      <parentid>564025264</parentid>
      <timestamp>2013-10-03T20:41:51Z</timestamp>
      <contributor>
        <username>Mogism</username>
        <id>16902756</id>
      </contributor>
      <minor/>
      <comment>/* Features */Cleanup/[[WP:AWB/T|Typo fixing]], [[WP:AWB/T|typo(s) fixed]]: eg.  → e.g. using [[Project:AWB|AWB]]</comment>
      <text xml:space="preserve" bytes="2636">{{Orphan|date=July 2013}}

{{ Infobox Software
| name                   = JCROM
| logo                   = [[File:Jcrom-logo.png|200px|JCROM Logo]]
| screenshot             = 
| caption                = 
| developers             = [[Olafur Gauti Gudmundsson,Nicolas Dos Santos]]
| status                 = Active
| latest release version = 2.1.0
| latest release date    = {{Release date|2013|06|19}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Content repository]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://jcrom.googlecode.com/}}
}}
'''JCROM''' is an acronym that stands for [[Java Content Repository]] Object Mapper. It is a simple and lightweight annotation-based framework for mapping [[Plain Old Java Objects]] (POJOs) to/from nodes in a [[Java Content Repository]] (JCR). This is commonly called [[Object Content Mapping]].

JCR specifies an API for application developers (and application frameworks) to use for interaction with modern content repositories that provide content services such as searching, versioning, transactions, etc.

There are object mapping frameworks for JDBC, like Hibernate and the Enterprise JavaBeans spec. There are also solutions for mapping to/from XML. The vision of JCROM is to provide the same for JCR.

==Features==
* Annotation based (needs Java 1.5)
* Lightweight, minimal external dependencies
* Works with any JCR implementation (e.g. Apache Jackrabbit,&lt;ref&gt;[http://jackrabbit.apache.org/ Apache Jackrabbit home page]&lt;/ref&gt; ModeShape,&lt;ref&gt;[http://www.jboss.org/modeshape ModeShape open source project]&lt;/ref&gt; Adobe CQ,&lt;ref&gt;[http://dev.day.com/docs/en/cq/5-5.html Adobe CQ]&lt;/ref&gt; ...)
* DAO support
* Works with the Spring Framework&lt;ref&gt;[http://www.springsource.org/spring-framework Spring Framework home page]&lt;/ref&gt; and Spring Extension JCR&lt;ref&gt;[http://se-jcr.sourceforge.net/guide.html Spring Extension JCR open source project]&lt;/ref&gt;
* Works with Google Guice&lt;ref&gt;[[Google Guice framework home page]]&lt;/ref&gt;

==See also==
*[http://jackrabbit.apache.org/object-content-mapping.html Jackrabbit OCM] - a framework used to persist java objects (pojos) in a JCR repository

==References==
{{reflist}}

==External links==
*[http://www.jcp.org/en/jsr/detail?id=170 JSR-170: Content Repository for Java(TM) Technology API]
*[http://www.jcp.org/en/jsr/detail?id=283 JSR-283: Content Repository for Java(TM) Technology API, version 2.0]

[[Category:Apache Software Foundation|Jackrabbit]]
[[Category:Java enterprise platform]]
[[Category:Structured storage]]</text>
      <sha1>h97l18tuwz9rboqdwq53sdgapad8is1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Subversion</title>
    <ns>0</ns>
    <id>144868</id>
    <revision>
      <id>601989590</id>
      <parentid>601524087</parentid>
      <timestamp>2014-03-30T18:22:49Z</timestamp>
      <contributor>
        <username>Kfogel</username>
        <id>99396</id>
      </contributor>
      <comment>/* History */ Add date of 1.0 release.</comment>
      <text xml:space="preserve" bytes="30590">{{Infobox software
| name                   = Subversion
| title                  = 
| logo                   = [[File:Subversion.png|250px]]
| logo caption           = 
| screenshot             = &lt;!-- [[File: ]] --&gt;
| caption                = 
| collapsible            = 
| author                 = 
| developer              = [[Apache Software Foundation]]
| released               = {{Start date|2000|10|20|df=yes/no}}
| discontinued           = 
| latest release version = 
| latest release date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| frequently updated     = yes&lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| status                 = Active
| programming language   = [[C (programming language)|C]]
| operating system       = [[Cross-platform]]
| platform               = 
| size                   = 
| language               = 
| language count         = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| language footnote      = 
| genre                  = [[Revision control]]
| license                = [[Apache License]]
| alexa                  = 
| website                = {{URL|http://subversion.apache.org/}}
| standard               = 
| AsOf                   = 
}}
'''Apache Subversion''' (often abbreviated '''SVN''', after the command name ''svn'') is a [[software versioning]] and [[revision control]] system distributed as [[free software]] under the [[Apache License]].&lt;ref&gt;{{cite web |url=http://directory.fsf.org/wiki/Subversion |title=Subversion |year=2013 |website=directory.fsf.org |publisher=[[Free Software Directory]] |accessdate=11 September 2013}}&lt;/ref&gt; Developers use Subversion to maintain current and historical versions of files such as [[source code]], web pages, and documentation.  Its goal is to be a mostly compatible successor to the widely used [[Concurrent Versions System]] (CVS).

The [[free software community]] has used Subversion widely: for example in projects such as [[Apache Software Foundation]], [[Free Pascal]], [[FreeBSD]], [[GNU Compiler Collection|GCC]], [[Mono (software)|Mono]] and [[SourceForge]]. [[Google Code]] also provides Subversion hosting for their free software projects. [[CodePlex]] offers access to Subversion as well as to other types of clients.

The corporate world has also started to adopt Subversion. A 2007 report by [[Forrester Research]] recognized Subversion as the sole leader in the Standalone [[Software Configuration Management]] (SCM) category and as a strong performer in the Software Configuration and Change Management (SCCM) category.&lt;ref&gt;{{cite web | url=http://www.collab.net/forrester_wave_report/index.html | title=The Forrester Wave: Software Change and Configuration Management, Q2 2007 | publisher=[[Forrester Research]]}}&lt;/ref&gt;

Subversion was created by [[CollabNet]] Inc. in 2000, and is now a top-level Apache project being built and used by a global community of contributors.

==History==
[[CollabNet]] founded the Subversion project in 2000 as an effort to write an open-source version-control system which operated much like [[Concurrent Versions System|CVS]] but which fixed the bugs and supplied some features missing in CVS. By 2001, Subversion had advanced sufficiently to [[Self-hosting|host its own source code]]&lt;ref&gt;{{cite web |url=http://svnbook.red-bean.com/en/1.7/svn.intro.whatis.html#svn.intro.history |work=Version Control with Subversion (for Subversion 1.7) |title=What is Subversion? &gt; Subversion's History |author=Collins-Sussman, Ben; Brian W. Fitzpatrick; C. Michael Pilato |year=2011 |accessdate=15 March 2012}}&lt;/ref&gt;, and in February, 2004 version 1.0 was released.&lt;ref&gt;{{cite web |url=http://lwn.net/Articles/72498/ |work=Linux Weekly News |title=subversion 1.0 is released |author=Benjamin Zeiss |year=2004 |accessdate=30 March 2014}}&lt;/ref&gt; In November 2009, Subversion was accepted into [[Apache Incubator]]: this marked the beginning of the process to become a standard top-level Apache project.&lt;ref&gt;{{cite web |url=http://www.sdtimes.com/link/33886 |title=Subversion joins forces with Apache |author=Rubinstein, David |publisher=SD Times |date=4 November 2009 |accessdate=15 March 2012}}&lt;/ref&gt; It became a top-level Apache project on February 17, 2010.&lt;ref&gt;{{cite web |url=http://subversion.wandisco.com/component/content/article/1/43.html |archiveurl=https://web.archive.org/web/20110512171259/http://subversion.wandisco.com/component/content/article/1/43.html |archivedate=12 May 2011 |title=Subversion is now Apache Subversion |date=18 February 2010 |accessdate=15 March 2012}}&lt;/ref&gt;

==Features==
* [[Commit (data management)|Commits]] as true [[Atomicity (database systems)|atomic operations]] (interrupted commit operations in CVS would cause repository inconsistency or corruption).
* Renamed/copied/moved/removed files retain full revision history.
* The system maintains [[Software versioning|versioning]] for directories, renames, and file [[metadata]] (but not for timestamps). Users can move and/or copy entire directory-trees very quickly, while retaining full revision history.
* Versioning of [[symbolic link]]s.
* Native support for binary files, with space-efficient binary-diff storage.
* [[Apache HTTP Server]] as network server, [[WebDAV]]/[[WebDAV#Extensions and derivatives|Delta-V]] for [[Protocol (computing)|protocol]]. There is also an independent server [[process (computing)|process]] called svnserve that uses a custom protocol over [[Internet Protocol Suite|TCP/IP]].
* [[Branching (software)|Branching]] is a cheap operation, independent of file size (though Subversion itself does not distinguish between a branch and a directory)
* Natively [[client–server model|client–server]], [[Abstraction layer|layered]] [[Library (computing)|library]] design.
* Client/server protocol sends [[diff]]s in both directions.
* Costs proportional to change size, not to data size.
* [[Parsing|Parsable]] output, including [[XML]] log output.
* [[Free software license]]d – [[Apache License]] since the 1.7 release; prior versions use a derivative of the Apache Software License 1.1.
* [[Internationalization and localization|Internationalized]] program messages.
* [[File locking]] for unmergeable files (&quot;reserved checkouts&quot;).
* Path-based authorization.
* [[Language binding]]s for [[C Sharp (programming language)|C#]], [[PHP]], [[Python (programming language)|Python]], [[Perl]], [[Ruby (programming language)|Ruby]], and [[Java (programming language)|Java]].
* Full [[MIME]] support – users can view or change the MIME type of each file, with the software knowing which MIME types can have their differences from previous versions shown.
* Merge tracking – Merges between branches will be tracked, this allows automatically merging between branches without telling Subversion what (doesn't) need to be merged.
* Changelists to organize commits into commit groups.

===Repository types===
Subversion offers two types of repository storage.

====Berkeley DB (deprecated&lt;ref&gt;http://subversion.apache.org/docs/release-notes/1.8.html#bdb-deprecated&lt;/ref&gt;)====
The original development of Subversion used the [[Berkeley DB]] package.
Subversion has some limitations with Berkeley DB usage when a program that accesses the database crashes or terminates forcibly. No data loss or corruption occurs, but the repository remains offline while Berkeley DB replays the journal and cleans up any outstanding locks. The safest way to use Subversion with a Berkeley DB repository involves a single server-process running as one user (instead of through a shared filesystem).&lt;ref name=&quot;backend&quot;&gt;
{{cite book |author= Ben Collins-Sussman, Brian W. Fitzpatrick, C. Michael Pilato |title= Version Control with Subversion: For Subversion 1.7 |year= 2011  |chapter= Chapter 5: Strategies for Repository Deployment |url= http://svnbook.red-bean.com/en/1.7/svn.reposadmin.planning.html#svn.reposadmin.basics.backends | publisher = O'Reilly}}
&lt;/ref&gt;

====FSFS====
In 2004, a new storage subsystem was developed and named FSFS.
It works faster than the Berkeley DB backend on directories with a large number of files and takes less disk space,
due to less logging.&lt;ref name=&quot;backend&quot;/&gt;

Beginning with Subversion 1.2, FSFS became the default data store for new repositories.

The etymology of &quot;FSFS&quot; is based on Subversion's use of the term &quot;filesystem&quot; for its repository storage system.
FSFS stores its contents directly within the operating system's filesystem, rather than a structured system like Berkeley DB.
Thus, it is a &quot;[Subversion] FileSystem atop the FileSystem&quot;.

===Repository access===
{{Main| Comparison of Subversion clients}}

Access to Subversion repositories can take place by:

# Local filesystem or network filesystem,&lt;ref&gt;[[Berkeley DB]] relies on file locking and thus should not be used on (network) filesystems which do not implement them&lt;/ref&gt; accessed by client directly. This mode uses the &lt;tt&gt;file:///path&lt;/tt&gt; access scheme.
# [[WebDAV]]/Delta-V (over http or https) using the &lt;tt&gt;mod_dav_svn&lt;/tt&gt; module for [[Apache HTTP Server|Apache 2]]. This mode uses the &lt;tt&gt;&lt;nowiki&gt;http://host/path&lt;/nowiki&gt;&lt;/tt&gt; access scheme or &lt;tt&gt;&lt;nowiki&gt;https://host/path&lt;/nowiki&gt;&lt;/tt&gt; for secure connections using ssl.
# Custom &quot;svn&quot; protocol (default [[List of TCP and UDP port numbers|port]] 3690), using plain text or over [[TCP/IP]]. This mode uses either the &lt;tt&gt;&lt;nowiki&gt;svn://host/path&lt;/nowiki&gt;&lt;/tt&gt; access scheme for unencrypted transport or &lt;tt&gt;svn+ssh://host/path&lt;/tt&gt; scheme for tunneling over ssh.

All three means can access both FSFS and Berkeley DB repositories.

Any 1.x version of a client can work with any 1.x server. Newer clients and servers have additional features and performance capabilities, but have fallback support for older clients/servers.&lt;ref&gt;[http://subversion.tigris.org/svn_1.5_releasenotes.html SVN 1.5 release notes]&lt;/ref&gt;

==Layers==
Internally, a Subversion system comprises several libraries arranged as layers. Each performs a specific task and allows developers to create their own tools at the desired level of complexity and specificity.

; Fs : The lowest level; it implements the versioned filesystem which stores the user data.
; Repos : Concerned with the repository built up around the filesystem. It has many helper functions and handles the various &quot;hooks&quot; that a repository may have, e.g. scripts that run when an action is performed. Together, Fs and Repos constitute the &quot;filesystem interface&quot;.
; mod_dav_svn : Provides [[WebDAV]]/Delta-V access through Apache 2.
; Ra : Handles &quot;repository access&quot;, both local and remote. From this point on, repositories are referred to using URLs, e.g.
;* &lt;tt&gt;file:///path/&lt;/tt&gt; for local access,
;* &lt;tt&gt;&lt;nowiki&gt;http://host/path/&lt;/nowiki&gt;&lt;/tt&gt; or &lt;tt&gt;&lt;nowiki&gt;https://host/path/&lt;/nowiki&gt;&lt;/tt&gt; for WebDAV access, or
;* &lt;tt&gt;&lt;nowiki&gt;svn://host/path/&lt;/nowiki&gt;&lt;/tt&gt; or &lt;tt&gt;svn+ssh://host/path/&lt;/tt&gt; for the SVN protocol.
; Client, Wc : The highest level. It abstracts repository access and provides common client tasks, such as authenticating users or comparing versions. Subversion clients use the Wc library to manage the local working copy.

==Filesystem==
[[File:Svn 3D-tree.svg|right|thumb|250px]]

One can view the Subversion filesystem as &quot;two-dimensional&quot;.&lt;ref&gt;[http://svnbook.red-bean.com/nightly/en/svn.branchmerge.basicmerging.html#svn.branchmerge.basicmerging.resurrect Basic Merging&lt;!-- Bot generated title --&gt;]&lt;/ref&gt; Two coordinates are used to unambiguously address filesystem items:
* '''Path''' (regular [[Path (computing)|path]] of [[Unix-like]] OS filesystem)
* '''Revision'''
Each revision in a Subversion filesystem has its own ''[[root directory|root]]'', which is used to access contents at that revision. Files are stored as links to the most recent change; thus a Subversion repository is quite compact. The system consumes storage space proportional to the number of changes made, not to the number of revisions.

The Subversion filesystem uses transactions to keep changes [[Atomicity (database systems)|atomic]]. A transaction operates on a specified revision of the filesystem, not necessarily the latest. The transaction has its own ''root'', on which changes are made. It is then either committed and becomes the latest revision, or is aborted. The transaction is actually a long-lived filesystem object; a client does not need to commit or abort a transaction itself, rather it can also begin a transaction, exit, and then can re-open the transaction and continue using it. Multiple clients can access the same transaction and work together on an atomic change, though no existing clients expose this capability.

==Properties==
One important feature of the Subversion filesystem is properties: simple ''name''=''value'' pairs of text. Properties occur on filesystem entries (i.e., files and directories). These are versioned just like other changes to the filesystem. Users can add any property they wish, and the Subversion client uses a set of properties, which it prefixes with 'svn:'.

; &lt;tt&gt;svn:executable&lt;/tt&gt; : Makes files on [[Unix]]-hosted working copies executable.
; &lt;tt&gt;svn:mime-type&lt;/tt&gt; : Stores the [[Internet media type]] (&quot;MIME type&quot;) of a file. Affects the handling of diffs and merging.
; &lt;tt&gt;svn:ignore&lt;/tt&gt; : A list of filename patterns to ignore in a directory. Similar to [[Concurrent Versions System|CVS]]'s &lt;tt&gt;.cvsignore&lt;/tt&gt; file.
; &lt;tt&gt;svn:keywords&lt;/tt&gt; : A list of ''keywords'' to substitute into a file when changes are made. The file itself must also reference the keywords as &lt;tt&gt;$keyword$&lt;/tt&gt; or &lt;tt&gt;$keyword:...$&lt;/tt&gt;. This is used to maintain certain information (e.g., author, date of last change, revision number) in a file without human intervention.&lt;br /&gt;The keyword substitution mechanism originates from [[Revision Control System|RCS]]&lt;ref&gt;http://www.openbsd.org/cgi-bin/man.cgi?query=rcs&amp;sektion=1#KEYWORD+SUBSTITUTION Keyword substitution keywords in cvs(1)&lt;/ref&gt; and from CVS.
; &lt;tt&gt;svn:eol-style&lt;/tt&gt; : Makes the client convert [[Newline|end-of-line]] characters in text files. Used when the working copy is needed with a specific EOL style. &quot;native&quot; is commonly used, so that EOLs match the user's OS EOL style. Repositories may require this property on all files to prevent inconsistent line endings, which can cause a problem in itself.
; &lt;tt&gt;svn:externals&lt;/tt&gt; : Allows parts of other repositories to be automatically checked-out into a sub-directory.
; &lt;tt&gt;svn:needs-lock&lt;/tt&gt; : Specifies that a file is to be checked out with file permissions set to read-only. This is designed for use with the locking mechanism. The read-only permission reminds one to obtain a lock before modifying the file: obtaining a lock makes the file writable, and releasing the lock makes it read-only again. Locks are only enforced during a commit operation. Locks can be used without setting this property. However, that is not recommended, because it introduces the risk of someone modifying a locked file; they will only discover it has been locked when their commit fails.
; &lt;tt&gt;svn:special&lt;/tt&gt; : This property is not meant to be set or modified directly by users. {{As of | 2010}} only used for having [[symbolic link]]s in the repository. When a symbolic link is added to the repository, a file containing the link target is created with this property set. When a Unix-like system checks out this file, the client converts it to a symbolic link.
; &lt;tt&gt;svn:mergeinfo&lt;/tt&gt; : Used to track merge data (revision numbers) in Subversion 1.5 (or later). This property is automatically maintained by the &lt;tt&gt;merge&lt;/tt&gt; command, and it is not recommended to change its value manually.&lt;ref&gt;[http://svnbook.red-bean.com/en/1.5/svn.ref.properties.html Subversion Properties&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

Subversion also uses properties on revisions themselves. Like the above properties on filesystem entries the names are completely arbitrary, with the Subversion client using certain properties prefixed with 'svn:'. However, these properties are not versioned and can be changed later.

; &lt;tt&gt;svn:date&lt;/tt&gt; : the date and time stamp of a revision
; &lt;tt&gt;svn:author&lt;/tt&gt; : the name of the user that submitted the change(s)
; &lt;tt&gt;svn:log&lt;/tt&gt; : the user-supplied description of the change(s);

==Branching and tagging==
Subversion uses the inter-file branching model from [[Perforce]]&lt;ref&gt;[http://www.perforce.com/perforce/branch.html Inter-File Branching: A Practical Method for Representing Variants]&lt;/ref&gt; to implement [[Branching (software)|branches]] and [[Revision tag|tagging]]. A branch is a separate line of development.&lt;ref&gt;[http://tortoisesvn.net/docs/release/TortoiseSVN_en/tsvn-dug-branchtag.html Branching / Tagging — TortoiseSVN]&lt;/ref&gt; Tagging refers to labeling the repository at a certain point in time so that it can be easily found in the future. In Subversion, the only difference between branches and tags is how they are used.

A new branch or tag is set up by using the &quot;&lt;tt&gt;svn copy&lt;/tt&gt;&quot; command, which should be used in place of the native operating system mechanism. The copied directory is linked to the original in the repository to preserve its history, and the copy takes very little extra space in the repository.

All the versions in each branch maintain the history of the file up to the point of the copy, plus any changes made since. One can &quot;merge&quot; changes back into the [[Trunk (software)|trunk]] or between branches.

[[File:Subversion project visualization.svg|650px|thumb|center|Visualization of a simple Subversion project]]

==Limitations and problems==
A known problem in Subversion affects the implementation of the file and directory '''rename''' operation. {{As of | 2013}}, Subversion implements the renaming of files and directories as a &quot;copy&quot; to the new name followed by a &quot;delete&quot; of the old name. Only the names change, all data relating to the edit history remains the same, and Subversion will still use the old name in older revisions of the &quot;tree&quot;. However, Subversion may become confused when a move conflicts with edits made elsewhere,&lt;ref&gt;[http://subversion.tigris.org/issues/show_bug.cgi?id=898 Implement true renames]&lt;/ref&gt; both for regular commits and when merging branches.&lt;ref&gt;[http://svnbook.red-bean.com/en/1.7/svn.branchmerge.advanced.html#svn.branchmerge.advanced.moves Advanced Merging]&lt;/ref&gt; The Subversion 1.5 release addressed some of these scenarios while others remained problematic.&lt;ref&gt;[http://subversion.apache.org/docs/release-notes/1.5.html#copy-move-improvements Copy/move-related improvements in Subversion 1.5]&lt;/ref&gt; The Subversion 1.8 release finally addressed these problems by making moves a first-class operation.&lt;ref&gt;[http://subversion.apache.org/docs/release-notes/1.8.html#moves Working copy records moves as first-class operation in Subversion 1.8]&lt;/ref&gt;

{{As of | 2013}}, Subversion lacks some repository-administration and management features. For instance, someone may wish to edit the repository to permanently remove all historical records of certain data. Subversion does not have built-in support to achieve this simply.&lt;ref&gt;
[http://subversion.tigris.org/issues/show_bug.cgi?id=516 svn obliterate]
&lt;/ref&gt;

Subversion stores additional copies of data on the local machine, which can become an issue with very large projects or files, or if developers work on multiple branches simultaneously. In versions prior to 1.7 these &lt;tt&gt;.svn&lt;/tt&gt; directories on the client side could become corrupted by ill-advised user activity like global search/replace operations.&lt;ref&gt;[http://stackoverflow.com/a/579442]&lt;/ref&gt; Starting with version 1.7 Subversion uses a single centralized &lt;tt&gt;.svn&lt;/tt&gt; folder per working area.&lt;ref&gt;[http://subversion.apache.org/docs/release-notes/1.7.html#wc-ng Working Copy Metadata Storage Improvements (client)]&lt;/ref&gt;

Subversion does not store the modification times of files. As such, a file checked out of a Subversion repository will have the 'current' date (instead of the modification time in the repository), and a file checked into the repository will have the date of the check-in (instead of the modification time of the file being checked in). This might not always be what is wanted.&lt;ref&gt;[http://subversion.tigris.org/issues/show_bug.cgi?id=1256 Issue 1256] at Tigris.org
&lt;/ref&gt;
To mitigate this third-party tools exist that allow for preserving modification time and other filesystem meta-data.&lt;ref&gt;[http://sourceforge.net/projects/freezeattrib/files/ FreezeAttrib (saves/restores file attributes using properties)]&lt;/ref&gt;
&lt;ref&gt;[http://fsvs.tigris.org/ FSVS (Fast System VerSioning)]&lt;/ref&gt;
However, giving checked out files a current date is important as well — this is how tools like [[make (software)|make(1)]] will take notice of a changed file for rebuilding it.

Subversion uses a centralized [[revision control]] model. Ben Collins-Sussman, one of the designers of Subversion, believes a centralised model would help prevent &quot;insecure programmers&quot; from hiding their work from other team members.&lt;ref&gt;[http://blog.red-bean.com/sussman/?p=96 Programmer Insecurity @ iBanjo&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;  Some users of version control systems see the centralised model as detrimental; famously, [[Linus Torvalds]] attacked&lt;ref&gt;[https://www.youtube.com/watch?v=4XpnKHJAok8 Google Tech Talk video] and its [https://web.archive.org/web/20110725034129/https://git.wiki.kernel.org/index.php/LinusTalk200705Transcript transcript]&lt;/ref&gt; Subversion's model and its developers.

While Subversion stores filenames as [[Unicode]], it does not specify if ''[[Precomposed character|precomposition]]'' or ''[[Combining character|decomposition]]'' is used for certain accented characters (such as é). Thus, files added in SVN clients running on some operating systems (such as {{nowrap|Mac OS X}}) use ''decomposition'' encoding, while clients running on other operating systems (such as GNU/Linux) use ''precomposition'' encoding, with the consequence that those accented characters do not display correctly if the local SVN client is not using the same encoding as the client used to add the files.&lt;ref&gt;[http://subversion.tigris.org/issues/show_bug.cgi?id=2464 subversion: Issue 2464&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;&lt;ref&gt;[http://www.syntevo.com/smartsvn/techarticles.html?page=problems.macos-special-characters SmartSVN - Subversion/SVN Client: Problems with umlauts in file names on Mac OS X&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;

===Subversion tags and branches===
Revision numbers are difficult to remember in any version-control system. For this reason, most systems offer symbolic ''tags'' as user-friendly references to them. Subversion does not have such a feature and what its documentation recommends to use instead is very different in nature. Instead of implementing tags as ''references'' to points in history, Subversion recommends making snapshot ''copies'' into a well-known subdirectory (&quot;&lt;code&gt;tags/&lt;/code&gt;&quot;) in the space of the repository tree. Only a few predefined ''references'' are available: HEAD, BASE, PREV and COMMITTED.

This history-to-space projection has multiple issues:

1. When a snapshot is taken, the system does not assign any special meaning to the name of the tag/snapshot. This is the difference between a ''copy'' and a ''reference''. The revision is recorded and the snapshot can be accessed by URL. This makes some operations less convenient and others impossible. For instance, a naïve &lt;code&gt;svn diff -r tag1:tag2 myfile&lt;/code&gt; does not work; it is slightly more complicated than that to achieve, requiring the user to know and input URL/paths to the snapshots instead of just the names: &lt;code&gt;svn diff &lt;URL-TO-TAG1&gt;/myfile &lt;URL-TO-TAG2&gt;/myfile&lt;/code&gt;. Other operations like for instance &lt;code&gt;svn log -r tag1:tag2 myfile&lt;/code&gt; are just impossible.

2. When two (ideally independent) object types live in the repository tree, a &quot;fight to the top&quot; can ensue. In other words, it is often difficult to decide at which level to create the &quot;&lt;code&gt;tags/&lt;/code&gt;&quot; subdirectory:

{|
|
 '''trunk'''/component'''foo'''/
         /component'''bar'''/
 '''tags'''/1.1/component'''foo'''/
         /component'''bar'''/
| &amp;nbsp;or&amp;nbsp;
|
 component'''foo'''/'''trunk'''/
             /'''tags'''/1.1/
 component'''bar'''/'''trunk'''/
             /'''tags'''/1.1/
|}

3. Tags, by their conventional definition are both read-only and light-weight, on the repository and client.  Subversion copies are not read-only, and while they are light-weight on the repository, they are incredibly heavy-weight on the client.

To address such issues, posters on the Subversion mailing lists have suggested a new feature called &quot;labels&quot; or &quot;aliases&quot;.&lt;ref&gt;[http://subversion.apache.org/mailing-lists.html Subversion mailing lists]&lt;/ref&gt;
SVN labels would more closely resemble the &quot;tags&quot; of other systems such as [[Concurrent Versions System|CVS]] or [[Git (software)|Git]]. The fact that Subversion has global revision numbers opens the way to a very simple label-&gt;revision implementation. Yet as of 2013, no progress has been made and symbolic tags are not in the list of the most wanted features.&lt;ref&gt;[http://subversion.apache.org/roadmap.html Subversion Roadmap]&lt;/ref&gt;

==Development and implementation==
{{Main|List of software that uses Subversion}}

[[CollabNet]] has continued its involvement with Subversion, but the project runs as an independent open source community. In November 2009, the project was accepted into the [[Apache Incubator]], aiming to become part of the [[Apache Software Foundation]]'s efforts.&lt;ref&gt;http://www.open.collab.net/news/press/2009/svn-asf.html Collabnet Press Release&lt;/ref&gt; Since March 2010, the project is formally known as Apache Subversion, being a part of the Apache Top-Level Projects.&lt;ref&gt;[http://www.open.collab.net/news/press/2010/apache.html Collabnet Press Release regarding Apache subversion]&lt;/ref&gt;

In October 2009, [[WANdisco]] announced the hiring of core Subversion committers as the company moved to become a major corporate sponsor of the project. This included Hyrum Wright, president of the Subversion Corporation and release manager for the Subversion project since early 2008, who joined the company to lead its open source team.&lt;ref&gt;{{Cite news |title= WANdisco Names Hyrum Wright to Lead Subversion Open Source Efforts |date= January 7, 2010 |work= News release |author= WANdisco |publisher= Open Source magazine |url= http://opensource.sys-con.com/node/1239202 |accessdate= October 29, 2011 }}&lt;/ref&gt;

The Subversion open-source community does not provide binaries, but potential users can download binaries from volunteers.&lt;ref&gt;{{Cite web |title= Apache Subversion Binary Packages |work= Official project website  |url= http://subversion.apache.org/packages.html  |accessdate= October 29, 2011 }}&lt;/ref&gt; While the Subversion project does not include an official [[graphical user interface]] (GUI) for use with Subversion, third parties have developed a number of different GUIs, along with a wide variety of additional ancillary software.

Work announced in 2009 included SubversionJ (a [[Java API]]) and implementation of the [http://svn.apache.org/repos/asf/subversion/trunk/notes/obliterate/obliterate-functional-spec.txt Obliterate] command, similar to that provided by [[Perforce]]. Both of these enhancements were sponsored by WANdisco.&lt;ref&gt;{{Cite news |title= WANdisco Presents New Initiatives for the Subversion Open Source Project |author= WANdisco |publisher= CM Crossroads |work= News release |date= October 28, 2009 |url= http://www.cmcrossroads.com/index.php?Itemid=100152&amp;catid=101:news-and-announcements&amp;id=13065:wandisco-presents-new-initiatives-for-the-subversion-open-source-project-&amp;option=com_content&amp;view=article WANdisco |accessdate= October 29, 2011 }}&lt;/ref&gt;

The Subversion committers normally have at least one or two new features under active development at any one time. The 1.7 release of Subversion in October 2011 included a streamlined HTTP transport to improve performance and a rewritten working-copy library.&lt;ref&gt;{{Cite web |title= Apache Subversion Roadmap |work= Official project website |url= http://subversion.apache.org/roadmap.html  |accessdate= October 29, 2011 }}&lt;/ref&gt;

==See also==
{{Portal|Free software}}

* [[List of revision control software]]
* [[Comparison of revision control software]]
* [[Comparison of Subversion clients]]
* [[TortoiseSVN]]
* [[UberSVN]]
* [[VisualSVN]]

==Notes==
{{Reflist|colwidth=30em}}

==References==
{{Refbegin}}
* C. Michael Pilato, Ben Collins-Sussman, Brian W. Fitzpatrick; &lt;cite&gt;Version Control with Subversion&lt;/cite&gt;; O'Reilly; ISBN 0-596-00448-6 (1st edition, paperback, 2004, [http://svnbook.red-bean.com/ full book online], [http://mentalpointer.com/Subversion/svn-book.html mirror])
* Garrett Rooney; &lt;cite&gt;Practical Subversion&lt;/cite&gt;; Apress; ISBN 1-59059-290-5 (1st edition, paperback, 2005)
* Mike Mason; &lt;cite&gt;Pragmatic Version Control Using Subversion&lt;/cite&gt;; Pragmatic Bookshelf; ISBN 0-9745140-6-3 (1st edition, paperback, 2005)
* William Nagel; &lt;cite&gt;Subversion Version Control: Using the Subversion Version Control System in Development Projects&lt;/cite&gt;; Prentice Hall; ISBN 0-13-185518-2 (1st edition, paperback, 2005)
{{Refend}}

==Further reading==
* [http://www.red-bean.com/sussman/svn-anti-fud.html Dispelling Subversion FUD] by Ben Collins-Sussman (Subversion developer), link broken as of 2013-03-07 (Internet Archive.org [[Wayback machine|Wayback Machine]] [https://web.archive.org/web/20110718233416/http://www.red-bean.com/sussman/svn-anti-fud.html 2011-07-18 captured version], &quot;last updated&quot; 2004-12-21)

==External links==
* {{Official website|http://subversion.apache.org/}}
** [http://subversion.tigris.org/ Previous official site] &lt;small&gt;Not all content has yet been migrated to the new official site.&lt;/small&gt;
* [http://svnbook.red-bean.com/ Version Control with Subversion], an [[O'Reilly Media|O'Reilly]] book available for free online
* {{dmoz|Computers/Software/Configuration_Management/Tools/Subversion/|Subversion}}

{{Revision control software}}
{{Apache}}

{{DEFAULTSORT:Subversion (Software)}}
[[Category:Apache Software Foundation]]
[[Category:Subversion]]
[[Category:Collaborative software]]
[[Category:Free revision control software]]
[[Category:Free software programmed in C]]
[[Category:Cross-platform free software]]
[[Category:Concurrent Versions System]]
[[Category:Unix archivers and compression-related utilities]]
[[Category:Software using the Apache license]]

{{Link GA|ru}}</text>
      <sha1>7kblyvqon3k9kqw2kltc7518q76x6l5</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Lucene</title>
    <ns>0</ns>
    <id>522923</id>
    <revision>
      <id>601780258</id>
      <parentid>598545069</parentid>
      <timestamp>2014-03-29T09:03:20Z</timestamp>
      <contributor>
        <username>Yamavu</username>
        <id>88881</id>
      </contributor>
      <comment>/* Lucene-based projects */ +ferret</comment>
      <text xml:space="preserve" bytes="7059">{{refimprove|date=February 2012}}
{{Merge from|Ferret search library|KinoSearch|date=May 2011}}
{{Infobox software
| name = Lucene
| logo = [[Image:Lucene logo green 300.png|160px|Lucene logo]]
| screenshot =
| caption =
| developer = [[Apache Software Foundation]]
| status = Active
| latest release version = 4.7.0
| latest release date = {{release date|2014|02|26}}
| operating system = [[Cross-platform]]
| programming language = [[Java (programming language)|Java]]
| genre = [[Search algorithm|Search]] and [[index (search engine)|index]]
| license = [[Apache License]] 2.0
| website = {{URL|http://lucene.apache.org}}
}}

'''Apache Lucene''' is a [[free software|free]]/[[open source software|open source]] [[information retrieval]] [[Library (computing)|software library]], originally created in [[Java (programming language)|Java]] by [[Doug Cutting]]. It is supported by the [[Apache Software Foundation]] and is released under the [[Apache Software License]].

Lucene has been ported to other programming languages including [[Object Pascal|Delphi]], [[Perl]], [[C Sharp (programming language)|C#]], [[C++]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]], and [[PHP]].&lt;ref name=&quot;port&quot;&gt;[http://wiki.apache.org/lucene-java/LuceneImplementations Lucene implementations]&lt;/ref&gt;

==History==
[[Doug Cutting]] originally wrote Lucene in 1999.&lt;ref&gt;
{{cite web|url=http://trijug.org/downloads/TriJug-11-07.pdf |title=Better Search with Apache Lucene and Solr |date=19 November 2007}}&lt;/ref&gt; It was initially available for download from its home at the [[SourceForge]] web site. It joined the Apache Software Foundation's [[Jakarta Project|Jakarta]] family of open-source Java products in September 2001 and became its own top-level Apache project in February 2005. Until recently,{{When|date=August 2012}} it included a number of sub-projects, such as [[Lucene.net|Lucene.NET]], [[Apache Mahout|Mahout]], [[Solr]] and [[Nutch]]. Solr has merged into the Lucene project itself and Mahout, Nutch, and Tika have moved to become independent top-level projects.

Version 4.0 was released on 12 October 2012.&lt;ref&gt;http://lucene.apache.org/&lt;/ref&gt;

==Features and common use==
While suitable for any application which requires full text [[Index (search engine)|indexing]] and searching capability, Lucene has been widely recognized&lt;ref&gt;{{cite book |title=Lucene in Action, Second Edition |last1= McCandless |first1=Michael |last2=Hatcher |first2=Erik |last3=Gospodnetić |first3=Otis |authorlink= |coauthors= |year=2010 |publisher=Manning  |location= |isbn=1933988177 |page=8 |pages= |url= |accessdate=}}&lt;/ref&gt;&lt;ref&gt;[http://www.glscube.org/downloads/glscube_design.pdf GNU/Linux Semantic Storage System]&lt;/ref&gt; for its utility in the implementation of [[Internet search engine]]s and local, single-site searching.

At the core of Lucene's logical architecture is the idea of a '''document''' containing '''fields''' of text. This flexibility allows Lucene's API to be independent of the [[file format]]. Text from [[Portable Document Format|PDFs]], [[HTML]], [[Microsoft Word]], and [[OpenDocument]] documents, as well as many others (except images), can all be indexed as long as their textual information can be extracted.&lt;ref&gt;{{cite book |title=Machine Learning and Data Mining in Pattern Recognition: 5th International Conference |last=Perner |first=Petra |authorlink= |coauthors= |year=2007 |publisher=Springer |location= |isbn=978-3-540-73498-7 |page=387 |pages= |url= |accessdate=}}&lt;/ref&gt;

==Lucene-based projects==
Lucene itself is just an indexing and search library and does not contain [[web spider|crawling]] and HTML [[parsers|parsing]] functionality. However, several projects extend Lucene's capability:
* Apache [[Nutch]] &amp;mdash; provides web crawling and HTML parsing
* Apache [[Solr]] &amp;mdash; an enterprise search server
* [[Elasticsearch]] &amp;mdash; an enterprise search server
* [[Compass Project|Compass]] &amp;mdash; a [[Java Search Engine Framework]]
* [[DocFetcher]] &amp;mdash; a [[multiplatform]] desktop search application

===Lucene.NET===

Lucene.NET is a port of Lucene written in [[C Sharp (programming language)|C#]] and targeted at [[.NET Framework]] users. There are currently two variations of the software, differing in Generics support and a few bug fixes.  The version that supports Generics has a lowercase &quot;g&quot; appended to the version name to distinguish between the two.{{Citation needed|date=October 2012}}

Regardless of generics support, all versions of Lucene.NET (with or without the &quot;g&quot;) should be able to read and write index files from the corresponding version of Java Lucene with no issues.{{Citation needed|date=October 2012}}

===Ferret===
'''Ferret'''&lt;ref&gt;https://github.com/jkraemer/ferret Ferret-Github repository&lt;/ref&gt; is a search library for [[Ruby (programming language)]] inspired by [[Lucene]].

There is a [[Ruby on Rails]] plugin called acts_as_ferret&lt;ref&gt;http://www.jkraemer.net/projects/acts_as_ferret&lt;/ref&gt;.

Ferret utilizes [[Poshlib]].

==Users==

For a list of companies that use Lucene (rather than extend), see Lucene's &quot;Powered By&quot; page.&lt;ref&gt;[http://wiki.apache.org/lucene-java/PoweredBy PoweredBy]&lt;/ref&gt; As an example, [[Twitter]] is using Lucene for its real time search.&lt;ref name=&quot;twitter&quot;&gt;[http://techcrunch.com/2010/10/06/new-twitter-search/ Twitter uses Lucene]&lt;/ref&gt;

==See also==
{{Portal|Free software}}
* [[Hadoop]]
* [[Hibernate search]]
* [[Xapian]]
* [[Sphinx (search engine)]]
* [[List of information retrieval libraries]]
* [[LGTE]]
* [[Information extraction]]
* [[Text mining]]
* [[eGranary Digital Library]]

==References==
{{Reflist}}

==Bibliography==
{{Refbegin}}
* {{cite book |last=Gospodnetic |first=Otis |coauthors=Erik Hatcher, Michael McCandless |pages=475
  |title=Lucene in Action |edition=2nd |date=28 June 2009 |publisher=[[Manning Publications]] |location=
  |isbn=1-9339-8817-7 |url=
}}
* {{cite book |last=Gospodnetic |first=Otis |coauthors=Erik Hatcher |pages=456
  |title=Lucene in Action |edition=1st |date=1 December 2004 |publisher=[[Manning Publications]] |location=
  |isbn=978-1-9323-9428-3 |url=
}}
{{Refend}}

==External links==
* {{Official website|http://lucene.apache.org/}}
* [http://lucenenet.apache.org/ Lucene.NET]
* [http://wiki.apache.org/lucene-java/LuceneImplementations List of Lucene Ports (or Implementations) in Other Languages on the Apache wiki]
* [http://web.archive.org/web/20060715234923/schmidt.devlib.org/software/lucene-wikipedia.html Introductory article with Java code for search] (This page is on [[Internet Archive]]) on [http://download.wikimedia.org/wikipedia/ Wikipedia data]
* [http://www.codeminima.com/CSharp:_A_simple_Lucene.Net_indexing_and_search_class An article with explanation and example implementation of Lucene.Net]

{{Apache}}

[[Category:Apache Software Foundation]]
[[Category:Free search engine software]]
[[Category:Java libraries]]
[[Category:C Sharp libraries]]
[[Category:Cross-platform software]]
[[Category:Software using the Apache license]]
[[Category:Search_engine_software]]</text>
      <sha1>72n9hntvoria9ulnrtyskehgb01qq11</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Marmotta</title>
    <ns>0</ns>
    <id>41457077</id>
    <revision>
      <id>587907478</id>
      <parentid>587827392</parentid>
      <timestamp>2013-12-27T12:54:50Z</timestamp>
      <contributor>
        <username>Devbug</username>
        <id>19355811</id>
      </contributor>
      <minor/>
      <comment>/* Apache Marmotta */</comment>
      <text xml:space="preserve" bytes="10835">{{notability|Products|date=August 2013}}
{{Infobox software
| name                   = Apache Marmotta
| logo                   = [[File:Apache Marmotta logo.jpg|284px|The logo of Apache Marmotta]]
| screenshot             =
| caption                =
| author                 = 
| developer              = [[Apache Software Foundation]]
| released               = 2013-10-03
| frequently updated     = yes
| status                 = Active
| programming language   = [[Java (programming language)|Java]]
| operating system       = [[Cross-platform]]
| language               = English
| genre                  = [[Triplestore]], [[Rule Reasoner]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://marmotta.apache.org}}
}}

'''Apache Marmotta''' is a [[Linked Data]] platform that comprises several [[#Components|components]]. In its most basic configuration it is a [[Linked Data]] server.&lt;ref&gt;{{cite web|url=http://marmotta.apache.org|publisher=Apache Marmotta|title=Apache Marmotta|accessdate=2013-08-20}}&lt;/ref&gt; Marmotta is one of the reference projects early implementing the new [[Linked Data Platform]] &lt;ref&gt;{{cite book| author = Steve Speicher, John Arwe, Ashok Malhotra| date = July 30, 2013 | title = Linked Data Platform 1.0 | edition = Last Call Working Draft | publisher = W3C | url = http://www.w3.org/TR/2013/WD-ldp-20130730/ | accessdate=2013-11-24}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.w3.org/wiki/LDP_Implementations#Apache_Marmotta_.28Client_and_Server.29|publisher=W3C LDP Working Group|title=LDP Implementations|accessdate=2013-11-24}}&lt;/ref&gt; recommendation that is being developed by [[W3C]].

It has been contributed by [[Salzburg Research]] from the [http://lmf.googlecode.com Linked Media Framework],&lt;ref&gt;{{cite web|url=http://wiki.apache.org/incubator/MarmottaProposal#Background|publisher=Apache|title= Apache Marmotta incubation proposal |accessdate=2013-08-20}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://marmotta.apache.org/acknowledgements.html|publisher=Apache Marmotta|title=Apache Marmotta - Acknowledgements|accessdate=2013-08-20}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://redlink.co/apache-marmotta/|publisher=Redlink GmbH|title=Apache Marmotta is now a Top Level Apache Project|accessdate=2013-11-25}}&lt;/ref&gt; and continues its versioning, hence starting at version 3.0.0.

Since April 2013, it is listed among the [[Semantic Web]] tools by the [[World Wide Web Consortium|W3C]].&lt;ref&gt;[https://www.w3.org/2001/sw/wiki/Marmotta W3C - Semantic Web - Apache Marmotta]&lt;/ref&gt;

In December 2013, it has been nominated as &quot;one of the [[Apache Software Foundation|ASF]]'s most active projects&quot;.&lt;ref&gt;{{cite web|url=http://globenewswire.com/news-release/2013/12/10/595835/10060945/en/The-ASF-asks-Have-you-met-Apache-tm-Marmotta-tm.html|publisher=The Apache Software Foundation |title=The ASF asks: Have you met Apache(tm) Marmotta(tm)?|accessdate=2013-12-25}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://blogs.apache.org/foundation/entry/the_asf_asks_have_you2|publisher=The Apache Software Foundation |title=The ASF asks: Have you met Apache(tm) Marmotta(tm)?|accessdate=2013-12-25}}&lt;/ref&gt;

== Components ==

The project is split in several parts: the platform itself, which includes full Read Write [[Linked Data]], [[SPARQL]], Reasoning, and basic security. In addition to the platform, the project develops some libraries can also be used separately:

* KiWi, a [[Triplestore]] built on top of a [[relational database]].
* LDPath, a path language to navigate across [[Linked Data]] resources.  
* LDClient, a Linked Data client that allows retrieval of remote resources via different protocols by making use of pluggable adapters (data providers) that wrap other data sources (such as [[YouTube]] and [[Facebook]]).&lt;ref&gt;{{cite web|url=http://www.w3.org/wiki/ConverterToRdf#Apache_Marmotta_LDClient|publisher=W3C|title=Converter to RDF|accessdate=2013-08-20}}&lt;/ref&gt;
* LDCache, a [[Web cache|cache]] system that automatically retrieves resources by internally using LDClient.

== History ==

=== Linked Media Framework ===

Apache Marmotta is the continuation of the open source Linked Media Framework published in early 2012.&lt;ref&gt;{{cite web|url=https://www.w3.org/2001/sw/wiki/Marmotta|publisher=W3C|title= W3C - Semantic Web - Apache Marmotta|accessdate=2013-12-25}}&lt;/ref&gt;&lt;ref&gt;{{cite mailinglist|url=http://mail-archives.us.apache.org/mod_mbox/www-announce/201304.mbox/%3CB157D25D-A49B-46CA-A4F0-71C6566CE924@apache.org%3E|mailinglist=announce@apache.org|date=26 April 2013|accessdate=2013-12-25}}&lt;/ref&gt;

=== Apache Marmotta ===
On November 16, 2012 it is proposed to the [[Apache Software Foundation]] under the name of Apache Linda, later changed to Apache Marmotta in order to avoid confusion with the [[Linda (coordination language)|Linda language]].&lt;ref&gt;{{cite mailinglist|url=http://markmail.org/thread/qesb3rbb2tyt3eti|title= Apache Marmotta (was: Apache Linda)|mailinglist= org.apache.incubator.general|date=16 November 2012|accessdate=2013-12-25}}&lt;/ref&gt;

On 3 December 2012 Marmotta enters incubation.&lt;ref&gt;{{cite web|url=http://incubator.apache.org/projects/marmotta.html|publisher=Apache|title= Marmotta Project Incubation Status page|accessdate=2013-08-20}}&lt;/ref&gt;

On April 26, 2013 Marmotta 3.0.0-incubating is released.&lt;ref&gt;{{cite mailinglist|url=http://lists.w3.org/Archives/Public/semantic-web/2013Apr/0209.html|title= Apache Marmotta 3.0.0-incubating released!|last=Schaffert|first=Sebastian|authorlink=Sebastian Schaffert|mailinglist= semantic-web AT w3.org|date= 26 April 2013|accessdate=2013-11-21}}&lt;/ref&gt;

On October 3, 2013 Marmotta 3.1.0-incubating is released.&lt;ref&gt;{{cite mailinglist|url=http://lists.w3.org/Archives/Public/semantic-web/2013Oct/0040.html|title= Apache Marmotta 3.1.0-incubating released!|last=Schaffert|first=Sebastian|authorlink=Sebastian Schaffert|mailinglist= semantic-web AT w3.org|date= 3 October 2013|accessdate=2013-11-21}}&lt;/ref&gt;

In November 2013, it graduated as [[Apache Software Foundation#Projects|top-level project]].&lt;ref&gt;{{cite web|url=http://mail-archives.apache.org/mod_mbox/incubator-marmotta-dev/201310.mbox/%3C1382947773.2712.12.camel%40kis07%3E|last=Frank|first=Jakob|authorlink=Jakob Frank|title=org.apache.incubator.general |date=28 October 2013 |accessdate=2013-12-25}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.salzburgresearch.at/en/2013/apache-marmotta-graduated-top-level-project/|publisher=Salzburg Research|title= Apache Marmotta graduated to top-level project |accessdate=2013-12-25}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://markmail.org/message/tc5vrsmady3mdo42|publisher=ASF|title=Resolved to establish the Apache Marmotta Project}}&lt;/ref&gt;

== Prominent users ==

* The backend of [[Salzburger Nachrichten]]'s [http://search.salzburg.com search and archive] is powered by Marmotta.&lt;ref&gt;{{cite book| author = Georg Güntner, [[Sebastian Schaffert]]| date = June 30, 2013 | title = A Marmot against the Flood of Data | publisher = Salzburg NewMediaLab | url = http://www.newmedialab.at/wp-content/uploads/SNML-TNG_SuccessStory_Marmotta_EN.pdf | accessdate=2013-11-24}}&lt;/ref&gt;
* [[Enel]] uses Marmotta for its [http://data.enel.com Open Data portal].&lt;ref&gt;{{cite web|url=http://cirullo.it/2013/07/liberate-dataset-fare-business-servono-linked-data/?lang=en|title=Free dataset! Linked data are necessary to do business|first=Raffaele|last=Cirullo|accessdate=2013-08-20}}&lt;/ref&gt;
* The [[Cloud computing|cloud]] infrastructure of [http://redlink.co Redlink] is powered by Marmotta.
* It is being used by some [[Framework Programmes for Research and Technological Development#Framework Programme 7|European research projects]] such as Fusepool and MICO (''Media in Context'').&lt;ref&gt;[http://challenge.semanticweb.org/2013/submissions/swc2013_submission_6.pdf Fusepool Linked Datapool for Technology Intelligence]&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.mico-project.eu/mico-kick-off-meeting-salzburg/|title=MICO Kick-off Meeting in Salzburg|accessdate=2013-12-25}}&lt;/ref&gt;

== Bibliography ==

{{Refbegin}}
*{{Cite book
| first1    =  Reto
| last1     = Bachmann-Gmur
| date      = July 26, 2013
| title     = Instant Apache Stanbol
| edition   = 1st
| publisher = [[Packt Publishing]]
| isbn      = 1783281235
| url       = http://www.packtpub.com/apache-stanbol/book
}}
*{{Cite book
| first1 = Michael
| last1 = Kaschesky
| first2 = Luigi
| last2 = Selmi
| date = June 2013
| title     = Fusepool R5 linked data framework: concepts, methodologies, and tools for linked data
| publisher = ACM New York
| pages     = 156–165
| isbn      = 978-1-4503-2057-3
| url       = http://dl.acm.org/citation.cfm?id=2479748
}}
*{{Cite book
| date     = 14 May 2013
| title     = Zukunft Von Linked Media: Trends, Entwicklungen Und Visionen.
| trans_title = The future of Linked Media: trends, discoveries and visions.
| language = German
| edition   = 1st
| publisher = [[Salzburg Research|Salzburg Research Forschungsgesellschaft]]
| isbn      = 3902448369
| url       = http://www.slideshare.net/snml/zukunft-von-linked-media-trends-entwicklungen-und-visionen
}}
*{{Cite book
| date     = 2 Jan 2013
| title     = Qualitätssicherung Bei Annotationen
| trans_title = Quality assurance in annotations
| language = German
| publisher = [[Salzburg Research|Salzburg Research Forschungsgesellschaft]]
| isbn      = 3902448326
}}
* {{Cite book
| editor1-last = Hassnaa
| editor1-first = Moustafa
| editor2-last = Zeadally
| editor2-first = Sherali
| title = Media Networks: Architectures, Applications, and Standards
| publisher = CRC Press
| isbn = 9781439877289
| date = 14 May 2012
| quote = an ongoing open source development used to demonstrate the concepts of semantic lifting and interlinking of media resources 
}}
* {{Cite book
| title = Linked Media Interfaces
| publisher = [[Salzburg Research|Salzburg Research Forschungsgesellschaft]]
| isbn = 3902448296
| date = 5 October 2011
}}
* {{Cite book
| title = Linked Media. Ein White-Paper zu den Potentialen von Linked People, Linked Content und Linked Data.
| trans_title = A white paper on the potential of Linked People, Linked Content and Linked Data.
| language = German
| page = 8
| publisher = [[Salzburg Research|Salzburg Research Forschungsgesellschaft]]
| isbn = 390244827X
| date = 26 April 2011
}}
{{Refend}}

== References ==

{{reflist}}

== External links ==
* [https://marmotta.apache.org Official website]
* [http://wiki.apache.org/incubator/MarmottaProposal Marmotta Incubation Proposal]
* [http://www.slideshare.net/Wikier/apache-marmotta Apache Marmotta presentation]
* [https://www.w3.org/2001/sw/wiki/Marmotta Apache Marmotta on the W3C]

{{Apache}}

{{DEFAULTSORT:Marmotta}}
[[Category:Apache Software Foundation]]
[[Category:Cross-platform software]]
[[Category:Free software programmed in Java]]
[[Category:Java programming language]]
[[Category:Semantic Web]]
[[Category:Triplestores]]</text>
      <sha1>lmb4savoiyuornhx2xzogv5fpjopf7b</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Giraph</title>
    <ns>0</ns>
    <id>37752641</id>
    <revision>
      <id>594506998</id>
      <parentid>594506274</parentid>
      <timestamp>2014-02-08T12:10:16Z</timestamp>
      <contributor>
        <username>Diaa abdelmoneim</username>
        <id>5782900</id>
      </contributor>
      <comment>updated stable release date</comment>
      <text xml:space="preserve" bytes="2363">{{ Infobox Software
| name                   = Apache Giraph
| logo                   = 
| screenshot             = 
| caption                = 
| collapsible            = yes
| developer              = [[Apache Software Foundation]] 
| status                 = Active
| latest release version = 1.0.0
| latest release date    = {{release date|2013|05|6|df=yes}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Cross-platform]]
| size                   = 
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Graph (computer science)|graph processing]]
| license                = [[Apache License|Apache 2.0 Licence]] 
| website                = {{url|http://giraph.apache.org}}
}}
'''Apache Giraph''' is an [[Apache Software Foundation|Apache]] project to perform [[Graph (computer science)|graph processing]] on big data. Giraph utilizes [[Apache Hadoop]]'s MapReduce implementation to process graphs. [[Facebook]] used Giraph with some performance improvements to analyze one trillion edges using 200 in 4 minutes.&lt;ref&gt;{{cite web|last=Ching|first=Avery|title=Scaling Apache Giraph to a trillion edges|url=http://www.facebook.com/notes/facebook-engineering/scaling-apache-giraph-to-a-trillion-edges/10151617006153920|publisher=Facebook|accessdate=8 February 2014|date=August 14, 2013}}&lt;/ref&gt; Giraph is based on a paper published by Google about its own graph processing system called Pregel.&lt;ref&gt;{{cite news|last=Jackson|first=Joab|title=Facebook's Graph Search puts Apache Giraph on the map|url=http://www.pcworld.com/article/2046680/facebooks-graph-search-puts-apache-giraph-on-the-map.html|accessdate=8 February 2014|newspaper=[[PC World]]|date=Aug 14, 2013}}&lt;/ref&gt; It can be compared to other Big Graph processing libraries such as Cassovary.&lt;ref&gt;{{cite web|last=Harris|first=Derrick|title=Facebook’s trillion-edge, Hadoop-based and open source graph-processing engine|url=http://gigaom.com/2013/08/14/facebooks-trillion-edge-hadoop-based-graph-processing-engine/|publisher=[[Gigaom]]|accessdate=8 February 2014|date=AUG. 14, 2013}}&lt;/ref&gt; 

==References==
{{Reflist}}

==External links==
* {{Official website|http://giraph.apache.org/}}

{{Apache}}

[[Category:Apache Software Foundation|Giraph]]
[[Category:Hadoop|Giraph]]
[[Category:Data mining and machine learning software]]</text>
      <sha1>2njjz8turiq1ztshz2nr9hry8crll1e</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Chemistry</title>
    <ns>0</ns>
    <id>35624581</id>
    <revision>
      <id>596867446</id>
      <parentid>596867188</parentid>
      <timestamp>2014-02-24T04:16:55Z</timestamp>
      <contributor>
        <username>JEH</username>
        <id>1661447</id>
      </contributor>
      <minor/>
      <comment>Added Apache box</comment>
      <text xml:space="preserve" bytes="1281">{{refimprove|date=January 2013}}
'''Apache Chemistry''' is a project of the [[Apache Software Foundation]] ('''ASF''') which provides  open source [[Content Management Interoperability Services]] (CMIS) for Python, Java, PHP and .NET.&lt;ref&gt;[http://chemistry.apache.org chemistry.apache.org]&lt;/ref&gt;

Apache Chemistry becoming the top-level project(TLP) of the [[Apache Software Foundation]] ('''ASF''').
Before becoming a TLP, Chemistry was just an incubating project, guided in its growth by the Incubator, like all Apache projects when they begin life in the [[Apache Software Foundation|ASF]].

== Sub-projects ==

* [[OpenCMIS]] - CMIS client and server libraries for Java
* cmislib - CMIS client library for Python
* phpclient - CMIS client library for PHP
* DotCMIS - CMIS client library for .NET

== References ==
{{Reflist}}

== External links ==
* [http://chemistry.apache.org/ Apache Chemistry Home Page]
* [http://chemistry.apache.org/java/opencmis.html Apache Chemistry - OpenCMIS]
* [http://chemistry.apache.org/python/cmislib.html Apache Chemistry - cmislib]
* [http://chemistry.apache.org/php/phpclient.html Apache Chemistry - phpclient]
* [http://chemistry.apache.org/dotnet/dotcmis.html Apache Chemistry - DotCMIS]

{{Apache}}

[[Category:Apache Software Foundation]]</text>
      <sha1>50riu6nlpgr773vetc5tuihrek231h8</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Struts 2</title>
    <ns>0</ns>
    <id>41721636</id>
    <revision>
      <id>599341933</id>
      <parentid>599264587</parentid>
      <timestamp>2014-03-12T21:06:33Z</timestamp>
      <contributor>
        <username>Jogep</username>
        <id>11358401</id>
      </contributor>
      <text xml:space="preserve" bytes="3403">{{Infobox software
| name                   = Apache Struts 2
| logo                   = [[File:Struts logo.gif|frameless|Apache Struts Logo]]
| developer              = [[Apache Software Foundation]]
| released               = {{Start date|2006|10|10}}
| latest release version = 2.3.16.1 GA
| latest release date    = {{release date|2014|03|02}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| platform               = [[Cross-platform]] ([[Java Virtual Machine|JVM]])
| license                = [[Apache License]] 2.0
| website                = {{URL|http://struts.apache.org/}}
}}
'''Apache Struts 2''' is an [[open-source]] [[web application framework]] for developing [[Java EE]] [[web application]]s. It uses and extends the [[Java Servlet]] [[application programming interface|API]] to encourage developers to adopt a [[model–view–controller]] (MVC) architecture. The [[WebWork]] framework spun off from [[Apache Struts]] aiming to offer enhancements and refinements while retaining the same general architecture of the original Struts framework. In December 2005, it was announced that WebWork 2.2 was adopted as Apache Struts 2, which reached its first full release in February 2007.&lt;ref&gt;[http://struts.apache.org/release/2.2.x/ About Apache Struts 2]&lt;/ref&gt;

== Features ==
* Simple [[Plain Old Java Object|POJO]]-based actions
* Simplified testability
* Thread safe
* [[Ajax (programming)|AJAX]] support
** [[jQuery]] plugin
** [[Dojo Toolkit]] plugin (deprecated)
** Ajax client-side validation
* Template support
* Support for different result types
* Easy to extend with plugins
** [[Representational state transfer|REST]] plugin (REST-based actions, extension-less URLs)
** Convention plugin (action configuration via Conventions and Annotations)
** Spring plugin ([[dependency injection]])
** [[Hibernate (Java)|Hibernate]] plugin
** Support in design
** JFreechart plugin (charts)
** [[jQuery]] plugin (Ajax support, UI widgets, dynamic table, charts)
** Rome plugin
**plugin

== See also ==
[[Comparison of web application frameworks]]

== References ==
{{Reflist}}

== External links ==
* {{Official website|http://struts.apache.org/}}
* [http://cwiki.apache.org/S2PLUGINS/home.html Struts 2 Plugin Registry]
* [http://code.google.com/p/struts2-jquery/ Struts2 jQuery Plugin]
* [http://struts2tutorial.sourceforge.net/ Struts2 Tutorial]
* [http://www.mastertheboss.com/web-interfaces/190-jboss-struts-tutorial.html Struts tutorial on JBoss]
* [http://www.digitalsanctum.com/2010/01/25/how-to-test-struts-2-actions-without-a-container/ How to Test Struts 2 Actions Without a Container]
* [http://www.javatips.net/blog/2013/07/struts-2-example/ Struts 2 Example]
* [http://www.javatips.net/blog/2013/07/struts-2-validation-example/ Struts 2 Validation Example]
* [http://www.cvedetails.com/vulnerability-list/vendor_id-45/product_id-6117/Apache-Struts.html Apache Struts Vulnerabilities]
* [http://www.javabeat.net/2007/05/struts-2-0-introduction-and-validations-using-annotations/ Struts 2.0 Validations]
{{Application frameworks}}
{{apache}}

[[Category:Apache Software Foundation|Struts]]
[[Category:Cross-platform free software]]
[[Category:Free software programmed in Java]]
[[Category:Java enterprise platform|Struts]]
[[Category:Web application frameworks|Struts]]
[[Category:Software using the Apache license]]</text>
      <sha1>1iqiommbwzvag4gp8cm4qamj5iems88</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Cloudera</title>
    <ns>0</ns>
    <id>27059655</id>
    <revision>
      <id>600688340</id>
      <parentid>598926602</parentid>
      <timestamp>2014-03-22T03:33:50Z</timestamp>
      <contributor>
        <username>Artyllect</username>
        <id>21028570</id>
      </contributor>
      <comment>Updated from 400 employees to 579 employees</comment>
      <text xml:space="preserve" bytes="8601">{{Infobox company
| name              = Cloudera
| logo              = Cloudera logo tag rgb.png
| logo_size         = 220
| logo_caption      = Cloudera, Inc. logo with tagline
|industry= Software Development
|services= [[Apache Hadoop]] distribution and support
| location          = [[Palo Alto, California]]
| num_employees     = 579&lt;ref&gt;http://www.crunchbase.com/company/cloudera&lt;/ref&gt;
| homepage          = {{URL|http://www.cloudera.com/}}
}}

'''Cloudera Inc.''' is an American-based software company that provides [[Apache Hadoop]]-based software, support and services, and training to business customers.

Cloudera's open-source Apache Hadoop distribution, CDH (Cloudera Distribution Including Apache Hadoop), targets enterprise-class deployments of that technology. Cloudera says that more than 50% of its engineering output is donated upstream to the various Apache-licensed open source projects (Apache Hive, Apache Avro, Apache HBase, and so on) that combine to form the Hadoop platform. Cloudera is also a sponsor of the [[Apache Software Foundation]].&lt;ref name=&quot;Apache Software Foundation Sponsorship&quot;&gt;{{cite web|url=http://www.apache.org/foundation/sponsorship.html|title=Apache Software Foundation Sponsorship|accessdate=28 August 2012}}&lt;/ref&gt;

==History==
Cloudera was covered by a blog in ''[[The New York Times]]'' in March 2009.&lt;ref&gt;{{cite news |url= http://bits.blogs.nytimes.com/2009/03/16/bottling-the-magic-behind-google-and-facebook/ |title= Bottling the Magic Behind Google and Facebook |last= Vance |first= Ashlee |date= 16 March 2009 |accessdate= 20 January 2014 |work=The New York Times}}&lt;/ref&gt; 
Three engineers from [[Google]], [[Yahoo]] and [[Facebook Inc.|Facebook]] ([[Christophe Bisciglia]], Amr Awadallah and Jeff Hammerbacher, respectively) joined with a former [[Oracle Corporation|Oracle]] executive (Mike Olson) in the company.
Olson was the CEO of [[Sleepycat Software]], the creator of the open-source embedded database engine [[Berkeley DB]] (acquired by Oracle in 2006). 
Awadallah was from [[Yahoo!|Yahoo]], where he ran one of the first business units using Hadoop for data analysis.&lt;ref&gt;{{cite web|url=http://articles.businessinsider.com/2012-01-31/news/31008410_1_yahoo-servers-and-storage-systems-oracle|title=This Former Yahoo-er's Startup Is So Hot, Even the CIA Invested In It|accessdate=28 August 2012}}&lt;/ref&gt;
At Facebook Hammerbacher used Hadoop for building analytic applications involving massive volumes of user data.&lt;ref&gt;{{cite web|url=http://www.businessweek.com/magazine/content/11_17/b4225060960537.htm|title=This Tech Bubble Is Different|accessdate=28 August 2012}}&lt;/ref&gt;

Architect [[Doug Cutting]], also chairman of the [[Apache Software Foundation]], authored the open-source [[Lucene]] and [[Nutch]] search technologies before he wrote the initial Hadoop software in 2004. He designed and managed a Hadoop storage and analysis cluster at Yahoo! before joining Cloudera in 2009.
Chief operating officer was Kirk Dunn.&lt;ref&gt;{{cite web
|url=http://investing.businessweek.com/research/stocks/private/person.asp?personId=1032567&amp;privcapId=5648440&amp;previousCapId=5648440&amp;previousTitle=PowerFile,%20Inc.
|title=Bloomberg Business Week, Executive Profile Kirk Dunn 
|accessdate=30 September 2012}}&lt;/ref&gt;

In March 2009, Cloudera announced the availability of Cloudera Distribution Including [[Apache Software Foundation|Apache]] [[Hadoop]] in conjunction with a $5-million investment led by [[Accel Partners]].&lt;ref&gt;{{cite web|url=http://techcrunch.com/2009/03/16/cloudera-raises-5-million-series-a-round-for-hadoop-commercialization/|title=Cloudera Raises $5 Million Series A Round For Hadoop Commercialization|last=Wauters |first=Robin|date=16 March 2009|publisher=TechCrunch|accessdate=22 April 2010}}&lt;/ref&gt; In 2011, the company raised a further $40 million from [[Ignition Partners]], [[Accel Partners]], [[Greylock Partners]], [[Meritech Capital Partners]], and [[In-Q-Tel]], a [[venture capital]] firm with open connections to the [[CIA]].&lt;ref&gt;{{cite web|url=http://venturebeat.com/2011/11/07/hadoop-cloudera-funding-ignition-accel-greylock/|title=Hadoop-based startup Cloudera raises $40M from Ignition Partners, Accel, Greylock|accessdate=28 August 2012}}&lt;/ref&gt;

In June 2013 Tom Reilly became chief executive, although Olson remained as chairman of the board and chief strategist. Reilly was chief executive at [[ArcSight]] when it was acquired by [[Hewlett-Packard]] in 2010.&lt;ref&gt;{{Cite news |title= Cloudera taps new CEO for inevitable IPO push or acquisition: Former CEO becomes chairman and chief strategist |author= Timothy Prickett Morgan |work= The Register |date= 20 June 2013 |url= http://www.theregister.co.uk/2013/06/20/cloudera_taps_new_ceo_for_inevitable_ipo_push_or_acquisition/ |accessdate= 20 January 2014 }}&lt;/ref&gt;

The preferred [[demonym]] for an employee of Cloudera is &quot;Clouderan.&quot;&lt;ref&gt;{{cite web|url=http://blog.cloudera.com/blog/2012/12/how-to-contribute-to-apache-hadoop-projects-in-24-minutes/|title=Use of the term Clouderan to refer to an employee of Cloudera |accessdate=12 December 2012}}&lt;/ref&gt;
It is headquartered in [[Palo Alto, California]].

==Products and services==
Cloudera offers software, services and support in three different bundles:
* Cloudera Enterprise includes CDH and an annual subscription license (per node) to Cloudera Manager and technical support. It comes in three editions: Basic, Flex, and Data Hub.
* Cloudera Express includes CDH and a version of Cloudera Manager lacking enterprise features such as rolling upgrades and backup/disaster recovery.
* CDH may be downloaded from Cloudera's website at no charge, but with no technical support nor Cloudera Manager.

CDH contains the main, core elements of Hadoop that provide reliable, scalable distributed data processing of large data sets (chiefly MapReduce and HDFS), as well as other enterprise-oriented components that provide security, high availability, and integration with hardware and other software.&lt;ref&gt;{{cite web|url=http://www.informationweek.com/software/information-management/cloudera-releases-next-generation-hadoop/240001574/|title=Cloudera Releases Next-Generation Hadoop Platform|last=Henschen |first=Doug|date=6 June 2012|publisher=InformationWeek|accessdate=22 April 2010}}&lt;/ref&gt;

In October 2012, Cloudera announced the [[Cloudera Impala]] project, an open-source distributed query engine for Apache Hadoop.&lt;ref&gt;{{cite web |url= http://www.zdnet.com/clouderas-impala-brings-hadoop-to-sql-and-bi-7000006413/|title=Cloudera’s Impala brings Hadoop to SQL and BI |last= Brust |first= Andrew |date= 25 October 2012 |work= ZDNet |accessdate= 20 January 2014 }}&lt;/ref&gt;

==Ecosystem==
Technology, software, and services providers for the Cloudera distribution of Apache Hadoop include:
*[[10gen]]
*[[Actian]]
*[[Alteryx]]
*[[Birst]]
*[[Cisco]]
*[[Continuuity]]
*[[Couchbase]]
*[[Databricks]]
*[[Datameer]]
*[[Dell]]
*EngineRoom.io
*Gazzang
*GoGrid
*[[Hitachi]]
*[[Hewlett-Packard]]
*[[IBM]]
*[[Intel]]
*[[Jaspersoft]]
*[[Joyent]]
*[[Lenovo]]
*[[Mellanox]]
*[[MicroStrategy]]
*[[New Relic]]
*[[Oracle]]
*[[ParAccel]]
*[[Pentaho]]
*Platfora
*[[Puppet Labs]]
*[[Qlikview]]
*[[SAP ERP|SAP]]
*SiSense
*[[Splunk]]
*[[Tableau Software]]
*[[Teradata]]
*[[VoltDB]]
*[[VMware]]
*[[Ubuntu (operating system)|Ubuntu]]
*[[Xebia]]
*Yarc Data
*Zaponet
*[[Zettaset]]
*ZoomData
*Zuna Infotech

==Awards==
* In April 2010, Chief Scientist Jeff Hammerbacher was named a &quot;Best Young Tech Entrepreneur&quot; by Bloomberg BusinessWeek.&lt;ref&gt;{{cite web|url=http://www.businessweek.com/technology/special_reports/20100420best_young_tech_entrepreneurs.htm|title=Best Young Technology Entrepreneurs 2010|accessdate=28 August 2012}}&lt;/ref&gt;
* In June 2012, received [[Morgan Stanley]]'s &quot;CTO Award for Innovation&quot;.&lt;ref&gt;{{cite web|url=http://www.marketwatch.com/story/cloudera-honored-by-morgan-stanley-with-prestigious-cto-award-for-innovation-2012-06-28|title=Cloudera Honored by Morgan Stanley With Prestigious 'CTO Award for Innovation'|accessdate=28 August 2012}}&lt;/ref&gt;
* In August 2012, CRN named Cloudera among the &quot;The 25 Coolest Emerging Vendors For 2012&quot;.&lt;ref&gt;{{cite web|url=http://www.crn.com/slide-shows/storage/240005642/the-25-coolest-emerging-vendors-for-2012.htm|title=The 25 Coolest Emerging Vendors For 2012|accessdate=28 August 2012}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
* [http://www.cloudera.com Official web site]

[[Category:Cloud computing providers]]
[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Cloud infrastructure]]
[[Category:Distributed file systems]]
[[Category:Hadoop]]</text>
      <sha1>brnocx3v0301g2brn9hfhlk1an6qsvm</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Cascading (software)</title>
    <ns>0</ns>
    <id>1915249</id>
    <revision>
      <id>602380012</id>
      <parentid>602379947</parentid>
      <timestamp>2014-04-02T06:34:45Z</timestamp>
      <contributor>
        <username>Dsimic</username>
        <id>6479630</id>
      </contributor>
      <minor/>
      <comment>/* References */ Layout compaction</comment>
      <text xml:space="preserve" bytes="7311">{{Multiple issues|
{{Unreliable sources|date=October 2013}}
{{Primary sources|date=October 2013}}
}}

{{Infobox software
| name                   = Cascading
| status                 = Active
| latest release version = 2.1
| programming language   = [[Java (programming language)|Java]]
| license                = [[Apache License]]
| website                = http://www.cascading.org/
}}
'''Cascading''' is a software abstraction layer for [[Apache Hadoop]].  Cascading is used to create and execute complex data processing workflows on a Hadoop cluster using any [[JVM]]-based language ([[Java (programming language)|Java]], [[JRuby]], [[Clojure]], etc.), hiding the underlying complexity of [[MapReduce]] jobs.  It is open source and available under the [[Apache License]]. Commercial support is available from Concurrent, Inc.&lt;ref&gt;[http://www.cascading.org/support Cascading support page]&lt;/ref&gt; 

Cascading was originally authored by Chris Wensel, who later founded Concurrent, Inc.&lt;ref&gt;[http://www.concurrentinc.com Concurrent, Inc.]&lt;/ref&gt; Cascading is being actively developed by the community{{fact|date=October 2013}} and a number of add-on modules are available.&lt;ref&gt;[http://www.cascading.org/modules.html Cascading modules]&lt;/ref&gt;

==Architecture==
To use Cascading, Apache Hadoop must also be installed, and the Hadoop job .jar must contain the Cascading .jars. Cascading consists of a data processing API, integration API, process planner and process scheduler.  

Cascading leverages the scalability of Hadoop but abstracts standard data processing operations away from underlying map and reduce tasks.&lt;ref&gt;[http://codeascraft.etsy.com/2010/02/24/analyzing-etsys-data-with-hadoop-and-cascading/  Blog post by Etsy describing their use of Cascading with Hadoop]&lt;/ref&gt;{{Better source|date=October 2013}}  Developers use Cascading to create a .jar file that describes the required processes. It follows a ‘source-pipe-sink’ paradigm, where data is captured from sources, follows reusable ‘pipes’ that perform data analysis processes, where the results are stored in output files or ‘sinks’. Pipes are created independent from the data they will process. Once tied to data sources and sinks, it is called a ‘flow’. These flows can be grouped into a ‘cascade’, and the process scheduler will ensure a given flow does not execute until all its dependencies are satisfied. Pipes and flows can be reused and reordered to support different business needs.&lt;ref&gt;[http://www.cascading.org/1.2/userguide/pdf/userguide.pdf Cascading User Guide]&lt;/ref&gt;  

Developers write the code in a JVM-based language and do not need to learn MapReduce. The resulting program can be regression tested and integrated with external applications like any other Java application.&lt;ref&gt;[http://www.concurrentinc.com/products/ Concurrent product page]&lt;/ref&gt;

Cascading is most often used for ad targeting, log file analysis, bioinformatics, machine learning, predictive analytics, web content mining, and extract, transform and load (ETL) applications.&lt;ref&gt;[http://www.concurrentinc.com/ Concurrent home page]&lt;/ref&gt;

==Uses of Cascading==
Cascading is cited as one of the top five most powerful Hadoop projects by SD Times in 2011,&lt;ref name=sdtimes1&gt;{{cite news
| last = Handy
| first = Alex
| date = 1 June 2011
| title = The top five most powerful Hadoop projects
| url =http://www.sdtimes.com/content/article.aspx?ArticleID=35596&amp;page=1
| newspaper = [[SD Times]]
| location = 
| publisher = 
| accessdate = 26 October 2013
}}&lt;/ref&gt;{{Verify credibility|date=October 2013}} as a major open source project relevant to bioinformatics&lt;ref name=biomedcent1&gt;{{cite news
| last = Taylor
| first = Ronald
| date = 21 December 2010
| title = An overview of the Hadoop/MapReduce/HBase framework and its current applications in bioinformatics
| url = http://www.biomedcentral.com/1471-2105/11/S12/S1
| newspaper = [[BioMed Central]]
| location = 
| publisher = [[Springer Science+Business Media]]
| accessdate = 26 October 2013
}}&lt;/ref&gt;{{Verify credibility|date=October 2013}} and is included in Hadoop: A Definitive Guide, by Tom White.&lt;ref&gt;[http://books.google.com/books?id=Nff49D7vnJcC&amp;lpg=PA539&amp;dq=cascading%20hadoop&amp;pg=PA548#v=onepage&amp;q=cascading%20hadoop&amp;f=false White, Tom, “Hadoop: The Definitive Guide,” O’Reilly Media, Inc., 2010, pp. 539 – 549.]&lt;/ref&gt; The project is also widely cited in presentations, conference proceedings and Hadoop user group meetings as a useful tool for working with Hadoop.&lt;ref&gt;[http://www.slideshare.net/pacoid/getting-started-on-hadoop Nathan, Paco (Wikipedia: [[Paco Nathan]]), “Getting Started on Hadoop” presentation for the SV Cloud Computing Meetup, 7/19/2010.]&lt;/ref&gt;&lt;ref&gt;[http://www.smartfrog.org/wiki/download/attachments/6193590/hadoop_and_beyond.pdf?version=1&amp;modificationDate=1238073739000 Julio Guijarro, Steve Loughran and Paolo Castagna, “Hadoop and beyond,” HP Labs, Bristol UK, 2008.]&lt;/ref&gt;&lt;ref&gt;[http://www.slideshare.net/hadoopusergroup/flightcaster-presentation-hadoop Cross, Bradford, “Flightcaster_HUG,” Presentation at the Bay Area Hadoop Users’ Group, March 26, 2010]&lt;/ref&gt;&lt;ref&gt;[http://www.slideshare.net/chriscurtin/nosql-hadoop-cascading-june-2010?from=ss_embed Curtin, Christopher, “NoSQL, Hadoop and Cascading,” June 2010.]&lt;/ref&gt;

* MultiTool on [[Amazon Web Services]] was developed using Cascading.&lt;ref&gt;[http://aws.amazon.com/articles/2293?_encoding=UTF8&amp;jiveRedirect=1 Cascading{{Not a typo|.}}Multitool on AWS]&lt;/ref&gt;
* LogAnalyzer for [[Amazon CloudFront]] was developed using Cascading.&lt;ref&gt;[http://aws.amazon.com/articles/2440?_encoding=UTF8&amp;jiveRedirect=1 LogAnalyzer for Amazon CloudFront]&lt;/ref&gt; 
* BackType&lt;ref&gt;[http://tech.backtype.com/ BackType blog]&lt;/ref&gt; - social analytics platform
* Etsy&lt;ref&gt;[http://codeascraft.etsy.com/2010/02/24/analyzing-etsys-data-with-hadoop-and-cascading/  Blog post by Etsy describing their use of Cascading with Hadoop]&lt;/ref&gt; - marketplace
* FlightCaster&lt;ref&gt;[http://www.informationweek.com/news/software/infrastructure/224000240 FlightCaster]&lt;/ref&gt; - predicting flight delays
* Ion Flux&lt;ref&gt;[http://www.concurrentinc.com/casestudies/ion_flux Ion Flux]&lt;/ref&gt;  - analyzing DNA sequence data
* RapLeaf&lt;ref&gt;[http://blog.rapleaf.com/dev/2008/09/05/goodbye-mapreduce-hello-cascading/ RapLeaf Blog]&lt;/ref&gt;  - personalization and recommendation systems
* Razorfish&lt;ref&gt;[http://aws.amazon.com/solutions/case-studies/razorfish/ Razorfish]&lt;/ref&gt;  - digital advertising

Other users are listed on the [http://www.cascading.org cascading.org site].

==Domain-Specific Languages Built on Cascading==
* PyCascading&lt;ref&gt;[https://github.com/twitter/pycascading]&lt;/ref&gt; - by Twitter, available on GitHub
* Cascading.jruby&lt;ref&gt;[https://github.com/gmarabout/cascading.jruby Cascading.jruby]&lt;/ref&gt; - developed by Gregoire Marabout, available on GitHub
* [[Cascalog]]&lt;ref&gt;[https://github.com/nathanmarz/cascalog Cascalog]&lt;/ref&gt; - authored by [[Nathan Marz]], available on GitHub
* Scalding&lt;ref&gt;[https://github.com/twitter/scalding Scalding]&lt;/ref&gt; - by Twitter, available on GitHub

==References==
{{Reflist|30em}}

==External links==
* [http://www.cascading.org/ Official website]

[[Category:Free software programmed in Java]]
[[Category:Free system software]]
[[Category:Cloud infrastructure]]
[[Category:Hadoop]]</text>
      <sha1>l5njuhosesd6y25dhvoou9p0o1aqid7</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache HBase</title>
    <ns>0</ns>
    <id>16266878</id>
    <revision>
      <id>601434880</id>
      <parentid>601434808</parentid>
      <timestamp>2014-03-27T00:29:23Z</timestamp>
      <contributor>
        <username>Emocat</username>
        <id>1212618</id>
      </contributor>
      <minor/>
      <comment>/* External links */</comment>
      <text xml:space="preserve" bytes="5302">{{Use dmy dates|date=October 2013}}
{{Infobox software
| name                   = Apache HBase
| logo                   = [[File:HBase Logo.png]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 0.94.17
| latest release date    = {{release date|df=yes|2014|2|25}}
| latest preview version = 
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Column oriented database]]
| license                = [[Apache License]] 2.0
| website                = {{URL|http://hbase.apache.org/}}
}}

'''HBase''' is an [[open source]], [[Non-relational database|non-relational]], [[distributed database]] modeled after [[Google|Google's]] [[BigTable]] and written in [[Java (programming language)|Java]]. It is developed as part of [[Apache Software Foundation]]'s [[Hadoop|Apache Hadoop]] project and runs on top of [[Hadoop Distributed Filesystem|HDFS (Hadoop Distributed Filesystem)]], providing BigTable-like capabilities for Hadoop. That is, it provides a [[fault-tolerant]] way of storing large quantities of [[sparse file|sparse]] data (small amounts of information caught within a large collection of empty or unimportant data, such as finding the 50 largest items in a group of 2 billion records, or finding the non-zero items representing less than 0.1% of a huge collection). 

HBase features compression, in-memory operation, and [[Bloom filter]]s on a per-column basis as outlined in the original BigTable paper.&lt;ref&gt;[http://db.usenix.org//events/osdi06/tech/chang/chang_html/ Chang, et al. (2006). Bigtable: A Distributed Storage System for Structured Data]&lt;/ref&gt; Tables in HBase can serve as the input and output for [[Mapreduce|MapReduce]] jobs run in Hadoop, and may be accessed through the [http://hbase.apache.org/apidocs/index.html Java API] but also through [[REST]], [[Avro_(serialization_system)|Avro]] or [[Thrift (protocol)|Thrift]] gateway APIs.

HBase is not a direct replacement for a classic [[SQL]] [[database]], although recently its performance has improved, and it is now serving several data-driven websites,&lt;ref&gt;[http://wiki.apache.org/hadoop/Hbase/PoweredBy Powered By HBase]&lt;/ref&gt;&lt;ref&gt;[http://www.docstoc.com/docs/9912857/HBase-nosql-presentation StumbleUpon HBase Presentation]&lt;/ref&gt; including [[Facebook]]'s Messaging Platform.&lt;ref name=&quot;the-underlying-technology-of-messages&quot;&gt;[http://www.facebook.com/notes/facebook-engineering/the-underlying-technology-of-messages/454991608919 The Underlying Technology of Messages]&lt;/ref&gt;&lt;ref name=&quot;theregister&quot;&gt;[http://www.theregister.co.uk/2010/12/17/facebook_messages_tech/ Facebook: Why our 'next-gen' comms ditched MySQL] Retrieved: 17 December 2010&lt;/ref&gt;

In the parlance of Eric Brewer’s [[CAP theorem]], HBase is a CP type system.

==History==
Apache HBase began as a project by the company [[Powerset (company)|Powerset]] out of a need to process massive amounts of data for the purposes of [[natural language search]]. It is now a top-level Apache project and has generated considerable interest.

Facebook elected to implement its new messaging platform using HBase in November 2010.&lt;ref name=&quot;the-underlying-technology-of-messages&quot;&gt;&lt;/ref&gt;

==See also==
{{Portal|Free software|Java}}
*[[NoSQL]]
*[[Cassandra (database)|Apache Cassandra]]
*[[Hypertable]]
*[[Apache Accumulo]]
*[[MongoDB]]
*[[Project Voldemort]]
*[[Riak]]
*[[Sqoop]]
*[[ElasticSearch]]

==References==
{{Reflist}}

==Bibliography==
{{Refbegin}}
* {{cite book
| first1       = Nick 
| last1        = Dimiduk
| first2      = Amandeep 
| last2       = Khurana 
| date        = November 28, 2012
| title       = HBase in Action
| publisher   = [[Manning Publications]]
| edition     = 1st
| page        = 350 
| isbn        = 978-1617290527
| url         = &lt;!-- http://www.manning.com/dimidukkhurana/ --&gt;
}}
* {{cite book
| first1      = Lars 
| last1       = George
| date        = September 20, 2011
| title       = HBase: The Definitive Guide
| publisher   = [[O'Reilly Media]]
| edition     = 1st
| page        = 556
| isbn        = 978-1449396107
| url         = http://shop.oreilly.com/product/0636920014348.do
}}
* {{cite book
| first       = Yifeng 
| last        = Jiang 
| date        = August 16, 2012
| title       = HBase Administration Cookbook 
| publisher   = [[Packt Publishing]]
| edition     = 1st
| page        = 332
| isbn        = 978-1849517140
| url         = http://www.packtpub.com/hbase-administration-for-optimum-database-performance-cookbook/book
}}
{{Refend}}

==External links==
*[http://hbase.apache.org/ Official Apache HBase homepage]
*[http://hadoop.apache.org/ Official Apache Hadoop homepage]
*[http://hbasecon.com/ HBaseCon: Official community conference]
*[http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable Understanding HBase]
*[http://www.networkworld.com/news/tech/2012/102212-nosql-263595.html A vendor-independent comparison of NoSQL databases: Cassandra, HBase, MongoDB, Riak] (NetworkWorld)


{{apache}}

{{DEFAULTSORT:Hbase}}
[[Category:BigTable implementations]]
[[Category:Hadoop]]
[[Category:Free database management systems]]
[[Category:Structured storage]]</text>
      <sha1>o8xdvj7g6pla0zp83g8f8w3nur55fwj</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Hive</title>
    <ns>0</ns>
    <id>30248516</id>
    <revision>
      <id>601482644</id>
      <parentid>592828607</parentid>
      <timestamp>2014-03-27T09:31:09Z</timestamp>
      <contributor>
        <ip>83.36.223.239</ip>
      </contributor>
      <comment>/* External links */</comment>
      <text xml:space="preserve" bytes="5753">{{About|a data warehouse infrastructure|the Java application framework|Apache Beehive}}
{{Infobox Software
| name = Hive
| logo =
| screenshot =
| caption =
| developer =
| status = Active
| latest release version = 0.12.0
| latest release date = {{release date|2013|10|15}}
| latest preview version =
| latest preview date =
| operating system = [[Cross-platform]]
| programming language = [[Java (programming language)|Java]]
| genre =
| license = [[Apache License]] 2.0
| website = {{URL|http://hive.apache.org/}}
}}

'''Apache Hive''' is a [[data warehouse]] infrastructure built on top of [[Hadoop]] for providing data summarization, query, and analysis.&lt;ref&gt;{{cite book |last=Venner |first=Jason |title=Pro Hadoop |publisher=[[Apress]] |year=2009 |isbn=978-1-4302-1942-2}}&lt;/ref&gt; While initially developed by [[Facebook]], Apache Hive is now used and developed by other companies such as [[Netflix]].&lt;ref&gt;[http://www.slideshare.net/evamtse/hive-user-group-presentation-from-netflix-3182010-3483386 Use Case Study of Hive/Hadoop ]&lt;/ref&gt;&lt;ref&gt;{{YouTube|id=Idu9OKnAOis|title=OSCON Data 2011, Adrian Cockcroft, &quot;Data Flow at Netflix&quot;}}&lt;/ref&gt; Amazon maintains a software fork of Apache Hive that is included in ''Amazon Elastic MapReduce'' on [[Amazon Web Services]].&lt;ref&gt;[http://s3.amazonaws.com/awsdocs/ElasticMapReduce/latest/emr-dg.pdf Amazon Elastic MapReduce Developer Guide]&lt;/ref&gt;

==Features==
Apache Hive supports analysis of large datasets stored in Hadoop's HDFS and compatible file systems such as [[Amazon S3]] filesystem. It provides an [[SQL]]-like language called HiveQL while maintaining full support for [[MapReduce|map/reduce]]. To accelerate queries, it provides indexes, including [[bitmap index]]es.&lt;ref&gt;[http://www.facebook.com/notes/facebook-engineering/working-with-students-to-improve-indexing-in-apache-hive/10150168427733920 Working with Students to Improve Indexing in Apache Hive]&lt;/ref&gt;

By default, Hive stores metadata in an embedded [[Apache Derby]] database, and other client/server databases like [[MySQL]] can optionally be used.&lt;ref&gt;{{cite book |last=Lam |first=Chuck |title=Hadoop in Action |publisher=[[Manning Publications]] |year=2010 |isbn=1-935182-19-6}}&lt;/ref&gt;

Currently, there are four file formats supported in Hive, which are TEXTFILE, SEQUENCEFILE, ORC and [[RCFile|RCFILE]].&lt;ref&gt;[http://www.sfbayacm.org/wp/wp-content/uploads/2010/01/sig_2010_v21.pdf Facebook's Petabyte Scale Data Warehouse using Hive and Hadoop]&lt;/ref&gt;&lt;ref&gt;{{cite web| url=http://www.cse.ohio-state.edu/hpcs/WWW/HTML/publications/papers/TR-11-4.pdf| title=RCFile: A Fast and Space-efﬁcient Data Placement Structure in MapReduce-based Warehouse Systems| author=Yongqiang He, Rubao Lee, Yin Huai, Zheng Shao, Namit Jain, Xiaodong Zhang and Zhiwei Xu|format=PDF}}&lt;/ref&gt;

Other features of Hive include:
* Indexing to provide acceleration, index type including compaction and [[Bitmap index]] as of 0.10, more index types are planned.
* Different storage types such as plain text, [[RCFile]], [[HBase]], ORC, and others.
* Metadata storage in an [[Relational database management system|RDBMS]], significantly reducing the time to perform semantic checks during query execution.
* Operating on compressed data stored into Hadoop ecosystem, algorithm including [[gzip]], [[bzip2]], [[snappy (software)|snappy]], etc.
* Built-in user defined functions (UDFs) to manipulate dates, strings, and other data-mining tools. Hive supports extending the UDF set to handle use-cases not supported by built-in functions.
* SQL-like queries (Hive QL), which are implicitly converted into map-reduce jobs.

==HiveQL==
While based on SQL, HiveQL does not strictly follow the full [[SQL-92]] standard. HiveQL offers extensions not in SQL, including ''multitable inserts'' and ''create table as select'', but only offers basic support for [[index (database)|indexes]]. Also, HiveQL lacks support for [[database transaction|transactions]] and [[materialized view]]s, and only limited subquery support.&lt;ref&gt;{{cite book |last=White |first=Tom |title=Hadoop: The Definitive Guide |publisher=[[O'Reilly Media]] |year=2010 |isbn=978-1-4493-8973-4}}&lt;/ref&gt;&lt;ref&gt;[https://cwiki.apache.org/confluence/display/Hive/LanguageManual Hive Language Manual]&lt;/ref&gt; There are plans for adding support for insert, update, and delete with full ACID functionality.&lt;ref&gt;[https://issues.apache.org/jira/browse/HIVE-5317 Implement insert, update, and delete in Hive with full [[ACID]] support]&lt;/ref&gt;

Internally, a [[compiler]] translates HiveQL statements into a [[directed acyclic graph]] of [[MapReduce]] jobs, which are submitted to Hadoop for execution.&lt;ref&gt;[http://www.vldb.org/pvldb/2/vldb09-938.pdf Hive A Warehousing Solution Over a MapReduce Framework]&lt;/ref&gt;

==See also==
* [[Pig (programming language)|Apache Pig]]
* [[Sqoop]]
* [[Cloudera Impala]]
* [[Apache Drill]]

==References==
&lt;references /&gt;

==External links==
*{{Official website|http://hive.apache.org/}}
*[http://www.semantikoz.com/blog/the-free-apache-hive-book/ The Free Hive Book] (CC by-nc licensed)
*[http://www.vldb.org/pvldb/2/vldb09-938.pdf Hive A Warehousing Solution Over a MapReduce Framework] - Original paper presented by Facebook at [[VLDB]] 2009
*[http://www.youtube.com/watch?v=Y3UXDtDR9bg Using Apache Hive With Amazon Elastic MapReduce (Part 1)] and {{Youtube|id=1hDhpVmeSGI|title=Part 2}}, presented by an AWS Engineer
*[https://github.com/2013Commons/hive-cassandra Using hive + cassandra + shark. A hive cassandra cql storage handler.]

{{apache}}
{{Facebook navbox}}

{{DEFAULTSORT:Hive}}
[[Category:Apache Software Foundation projects]]
[[Category:Facebook]]
[[Category:Free software programmed in Java]]
[[Category:Free system software]]
[[Category:Cloud computing]]
[[Category:Hadoop]]</text>
      <sha1>l56qmso2y2fhb61ehrxnga0z3b9mjey</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>MapR</title>
    <ns>0</ns>
    <id>31958369</id>
    <revision>
      <id>598486722</id>
      <parentid>598486689</parentid>
      <timestamp>2014-03-07T01:26:58Z</timestamp>
      <contributor>
        <username>Funandtrvl</username>
        <id>2966869</id>
      </contributor>
      <comment>added [[Category:Companies based in San Jose, California]] using [[WP:HC|HotCat]]</comment>
      <text xml:space="preserve" bytes="6176">{{Infobox company
| name             = MapR
| logo             = MapR_Company_Logo.png
| logo_caption     = MapR Company Logo
| industry         = Data Services
| location_city    = San Jose, CA
| location_country = United States of America
| locations        = 10
| area_served      = United States, United Kingdom, France, Germany, Sweden, Japan, Singapore, Korea, Australia, Singapore
| key_people       = 
| products         = Hadoop technology
| production       = 
| services         = 
| revenue          = 
| operating_income = 
| net_income       = 
| homepage         = http://www.mapr.com/
| footnotes        = 
| intl             = 
| bodystyle        = 
}}

'''MapR''' is a [[San Jose, California]]-based enterprise software company that develops and sells [[Apache Hadoop]]-derived software.  The company contributes to [[Apache Hadoop]] projects like [[HBase]], [[Pig (programming language)]], [[Apache Hive]], and [[Apache ZooKeeper]].&lt;ref name=&quot;contrib&quot;&gt;{{cite web|url=http://gigaom.com/cloud/why-mapr-is-right-to-give-back-to-apache-hadoop/|title=Why MapR is Right to Give Back to Apache Hadoop|accessdate=1 June 2011}}&lt;/ref&gt;  MapR's [[Apache Hadoop]] distribution claims to provide full data protection, no single points of failure, improved performance, and dramatic ease of use advantages. MapR entered a technology licensing agreement with [[EMC Corporation]] on 25 May 2011, supporting an EMC-specific distribution of [[Apache Hadoop]].&lt;ref name=&quot;emchadoop&quot;&gt;{{cite web|url=http://msoftnews.com/google/startup-mapr-underpins-emc%E2%80%99s-hadoop-effort/|title=Startup MapR Underpins EMC’s Hadoop Effort|accessdate=1 June 2011}}&lt;/ref&gt;  MapR was selected by Amazon to provide an upgraded version of Amazon's Elastic Map Reduce (EMR) service&lt;ref name=&quot;emr-mapr&quot;&gt;{{cite web|url=http://aws.amazon.com/elasticmapreduce/mapr/|title=Amazon EMR with the MapR Distribution for Hadoop|accessdate=25 June 2011}}&lt;/ref&gt;&lt;ref name=&quot;emr-announce&quot;&gt;{{cite web|url=http://gigaom.com/cloud/amazon-taps-mapr-for-high-powered-elastic-mapreduce/|title=Amazon Taps MapR for High Powered Elastic Map Reduce|accessdate=25 June 2011}}&lt;/ref&gt;  MapR has also been selected by Google as a technology partner.&lt;ref&gt;{{cite web|title=MapR Technologies Joins Google Cloud Platform Partner Program|url=http://www.reuters.com/article/2012/07/24/idUS189111+24-Jul-2012+BW20120724|publisher=Reuters|accessdate=9 May 2013}}&lt;/ref&gt;  MapR was able to break the minute sort speed record on Google's compute platform.&lt;ref&gt;{{cite web|last=Metz|first=Cade|title=Google Teams With Prodigal Son to Bust Data Sort Record|url=http://www.wired.com/wiredenterprise/2013/02/google-mapr-data-sort-record/|publisher=Wired|accessdate=9 May 2013}}&lt;/ref&gt;

MapR provides three versions of their product known as M3,  M5 and M7.  M3 is a free version of the M5 product with degraded availability features.  M7 is like M5, but adds a purpose built rewrite of HBase that implements the HBase API directly in the file-system layer.

MapR is privately held with original funding of $9 million from [[Lightspeed Venture Partners]] and [[New Enterprise Associates]] since 2009. Key MapR executives come from [[Google]], [[Lightspeed Venture Partners]], [[Informatica]], [[EMC Corporation]] and [[Veoh]].  MapR had an additional round of funding led by [[Redpoint]] in August, 2011.&lt;ref name=&quot;redpoint&quot;&gt;{{cite web|url=http://www.cnbc.com/id/44325254/MapR_Technologies_Secures_20_Million_in_Funding_Investment_to_Fuel_Rapid_Adoption_for_Big_Data_Analytics_and_Hadoop|title=MapR Technologies Secures 20 million in Funding|accessdate=19 Sep 2011}}&lt;/ref&gt;&lt;ref name=&quot;nyt-redpoint&quot;&gt;{{cite web|url=http://www.nytimes.com/external/venturebeat/2011/08/30/30venturebeat-mapr-makes-friends-of-hadoop-and-the-enterpr-34892.html|title=MapR Makes Friends of Hadoop|accessdate=19 Sep 2011}}&lt;/ref&gt;  A C round was led by [[Mayfield Fund]] that also included Greenspring Associates as an investor.&lt;ref&gt;{{cite web|last=Hesseldahl|first=Arik|title=MapR Lands $30 Million Series C Led by Mayfield Fund.|url=http://allthingsd.com/20130318/mapr-lands-30-million-series-c-led-by-mayfield-fund/|publisher=All Things D|accessdate=9 May 2013}}&lt;/ref&gt;

==Partners==
Amazon offers MapR's M3 and M5 editions as a premium options on the Elastic MapReduce service next to a set of Apache Hadoop versions.&lt;ref name=emr-announce/&gt;

[http://www.bigdatapartnership.com Big Data Partnership] partners with MapR as first Certified Training &amp; SI partner in EMEA&lt;ref&gt;{{cite web|title=MapR Announces Certified Training with Big Data Partnership in EMEA |url=http://www.mapr.com/press-release/mapr-announces-certified-training-with-big-data-partnership-in-emea}}&lt;/ref&gt;

Google partnered with MapR in the launch of the [[Google Compute Engine]].&lt;ref name=google-mapr&gt;{{cite web|title=Google Compute Engine Leverages Third Party Support |url=http://www.informationweek.com/news/hardware/virtual/240003042}}&lt;/ref&gt;

[[Cisco Systems]] announced support of MapR software on the UCS platform.&lt;ref&gt;{{cite web|title=Cisco UCS with MapR:  Delivering Advanced Performance  for Hadoop Workloads|url=http://www.cisco.com/en/US/solutions/collateral/ns340/ns517/ns224/ns944/le_38203_sb_mapr_130208.pdf|publisher=Cisco|accessdate=9 May 2013}}&lt;/ref&gt;

[[Informatica Corporation]]

[http://www.impetus.com/partners Impetus Technologies]

[[Talend]]

[[Teradata Corporation]]

==See also==
* [[Apache Accumulo|Accumulo]]
* [[Apache Software Foundation]]
* [[Big data]]
* [[BigTable]]
* [[Cloud computing]]
* [[Cloud infrastructure]]
* [[Database-centric architecture]]
* [[Datastructure]]
* [[Hadoop]]
* [[MapReduce]]
* [[HBase]]

== References ==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

== External links ==
* [http://www.mapr.com MapR Homepage]

&lt;!--- Categories ---&gt;

{{DEFAULTSORT:Mapr}}
[[Category:Articles created via the Article Wizard]]
[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Cloud infrastructure]]
[[Category:Distributed file systems]]
[[Category:Hadoop]]
[[Category:Companies based in San Jose, California]]</text>
      <sha1>3g8iam7n3o7j28kx15pw3f0lk496p1f</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Accumulo</title>
    <ns>0</ns>
    <id>34571412</id>
    <revision>
      <id>591826948</id>
      <parentid>579357379</parentid>
      <timestamp>2014-01-22T05:32:03Z</timestamp>
      <contributor>
        <username>S4saurabh</username>
        <id>6640499</id>
      </contributor>
      <minor/>
      <comment>Added external link to Accumulo Mailing List Archives</comment>
      <text xml:space="preserve" bytes="5418">{{Infobox software
| name                   = Apache Accumulo
| logo                   = [[File:Accumulo logo.png|220px]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 1.4.3
| latest release date    = {{release date|2013|03|18}}
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Column oriented database]]
| license                = [[Apache License]] 2.0
| website                = {{URL|accumulo.apache.org}}
}}

'''Apache Accumulo''' is a computer software project that developed a sorted, distributed key/value store based on the [[BigTable]] technology from [[Google]].&lt;ref&gt;[http://accumulo.apache.org/ Apache Accumulo]. Accumulo.apache.org. Retrieved on 2013-09-18.&lt;/ref&gt;  It is a system built on top of [[Apache Hadoop]], [[Apache ZooKeeper]], and [[Apache Thrift]]. Written in [[Java (programming language)|Java]], Accumulo has cell-level [[Apache Accumulo#Cell-Level Security|access label]]s and [[server-side programming]] mechanisms. Accumulo is the 3rd most popular NoSQL Wide Column system according to DB-Engines ranking of Wide Column Stores.&lt;ref&gt;[http://db-engines.com/en/ranking/wide+column+store DB-Engines Ranking - popularity ranking of wide column stores]. Db-engines.com. Retrieved on 2013-09-18.&lt;/ref&gt;

==History==
Accumulo was created in 2008 by the US [[National Security Agency]] and contributed to the [[Apache Foundation]] as an incubator project in September 2011.&lt;ref name=&quot;informationweek1&quot;&gt;[http://www.informationweek.com/news/government/enterprise-apps/231600835 NSA Submits Open Source, Secure Database To Apache - Government]. Informationweek.com (2011-09-06). Retrieved on 2013-09-18.&lt;/ref&gt;

On March 21, 2012, Accumulo graduated from incubation at Apache, making it a top level project.&lt;ref&gt;[http://incubator.apache.org/projects/accumulo.html Accumulo Incubation Status - Apache Incubator]. Incubator.apache.org. Retrieved on 2013-09-18.&lt;/ref&gt;

===Controversy===

In June 2012 the US [[Senate Armed Services Committee]] (SASC) released the Draft 2012 Department of Defense (DoD) Authorization Bill, which included references to Apache Accumulo.  In the draft bill SASC required DoD to evaluate whether Apache Accumulo could achieve commercial viability before implementing it throughout DoD.&lt;ref&gt;Metz, Cade. (2012-12-19) [http://www.wired.com/wiredenterprise/2012/07/nsa-accumulo-google-bigtable/ NSA Mimics Google, Pisses Off Senate | Wired Enterprise]. Wired.com. Retrieved on 2013-09-18.&lt;/ref&gt; Specific criteria were not included in the draft language, but the establishment of commercial entities supporting Apache Accumulo could be considered a success factor.&lt;ref&gt;[http://www.fiercegovernmentit.com/story/sasc-accumulo-language-pro-open-source-say-proponents/2012-06-14 SASC Accumulo language pro-open source, say proponents]. FierceGovernmentIT (2012-06-14). Retrieved on 2013-09-18.&lt;/ref&gt;

==Main Features==
===Cell-Level Security===
Apache Accumulo extends the [[BigTable#Design|BigTable data model]], adding a new element to the key called [http://accumulo.apache.org/1.4/user_manual/Security.html Column Visibility]. This element stores a logical combination of security labels that must be satisfied at query time in order for the key and value to be returned as part of a user request. This allows data of varying security requirements to be stored in the same table, and allows users to see only those keys and values for which they are authorized.&lt;ref name=&quot;informationweek1&quot;/&gt;

===Server-side Programming===
In addition to Cell-Level Security, Apache Accumulo provides a server-side programming mechanism called Iterators that allows users to perform additional processing at the Tablet Server. The range of operations that can be applied is equivalent to those that can be implemented within a [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/mapreduce-osdi04.pdf MapReduce Combiner function], which produces an aggregate value for several key-value pairs.

==Papers==
* 2011 [http://www.pdl.cmu.edu/PDL-FTP/Storage/socc2011.pdf YCSB++: Benchmarking and Performance Debugging Advanced Features in Scalable Table Stores] by Carnegie Mellon University and the National Security Agency.
* 2012 [http://www.mit.edu/~kepner/pubs/ByunKepner_2012_BigData_Paper.pdf Driving Big Data With Big Compute] by MIT Lincoln Laboratory.
* 2013 [http://www.mit.edu/~kepner/pubs/D4Mschema_HPEC2013_Paper.pdf  D4M 2.0 Schema:A General Purpose High Performance Schema for the Accumulo Database] by MIT Lincoln Laboratory.

==See also==
{{Portal|Free software}}
{{Portal|Java}}
* [[BigTable]]
* [[Cassandra (database)|Apache Cassandra]]
* [[Column-oriented DBMS]]
* [[Hypertable]]
* [[HBase]]
* [[Hadoop]]
* [[sqrrl]]

==References==
{{Reflist}}

==External links==
*[http://accumulo.apache.org/ Apache Accumulo homepage]
*[http://www.reddit.com/r/accumulo Accumulo topic on reddit]
*[http://qnalist.com/g/accumulo Accumulo mailing list archives]

{{apache}}

[[Category:Apache Software Foundation projects]]
[[Category:BigTable implementations]]
[[Category:Hadoop]]
[[Category:Free database management systems]]
[[Category:NoSQL]]
[[Category:Distributed data stores]]
[[Category:Distributed computing architecture]]</text>
      <sha1>4huxrkbd2no27walol9mx5tx8y9h7l2</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Pig (programming tool)</title>
    <ns>0</ns>
    <id>29417433</id>
    <revision>
      <id>600012787</id>
      <parentid>597881170</parentid>
      <timestamp>2014-03-17T14:26:28Z</timestamp>
      <contributor>
        <ip>193.252.213.54</ip>
      </contributor>
      <comment>/* Pig vs SQL */</comment>
      <text xml:space="preserve" bytes="5624">'''Pig'''
&lt;ref name=&quot;mainpage&quot;&gt;{{cite web |url=http://pig.apache.org/|title=Hadoop: Apache Pig|accessdate=Sep 2, 2011}}&lt;/ref&gt;
is a high-level platform for creating [[MapReduce]] programs used with [[Hadoop]]. The language for this platform is called '''Pig Latin'''.&lt;ref name=&quot;mainpage&quot;/&gt;  Pig Latin abstracts the programming from the [[Java (programming language)|Java]] MapReduce idiom into a notation which makes MapReduce programming high level, similar to that of [[SQL]] for [[RDBMS]] systems. Pig Latin can be extended using UDF (User Defined Functions) which the user can write in Java, [[Python (programming language)|Python]], [[JavaScript]], [[Ruby (programming language)|Ruby]] or [[Groovy (programming language)|Groovy]] &lt;ref&gt;{{cite web|url= http://pig.apache.org/docs/r0.11.1/udf.html|title=Pig user defined functions|accessdate=May 3,2013}}&lt;/ref&gt; and then call directly from the language.

Pig was originally &lt;ref&gt;{{cite web |url=http://developer.yahoo.com/blogs/hadoop/posts/2008/10/pig_-_the_road_to_an_efficient_high-level_language_for_hadoop/|title=Yahoo Blog:Pig – The Road to an Efficient High-level language for Hadoop|accessdate=Nov 1, 2010}}&lt;/ref&gt; developed at [[Yahoo]] Research around 2006 for researchers to have an ad-hoc way of creating and executing map-reduce jobs on very large data sets. In 2007,&lt;ref&gt;{{cite web |url=http://developer.yahoo.com/blogs/hadoop/posts/2007/11/pig_into_incubation/|title=Pig into Incubation at the Apache Software Foundation|accessdate=Nov 1, 2010}}&lt;/ref&gt; it was moved into the [[Apache Software Foundation]].&lt;ref&gt;{{cite web |url=http://apache.org/|title=The Apache Software Foundation|accessdate=Nov 1, 2010}}&lt;/ref&gt;

==Example==
Below is an example of a &quot;[[word count|Word Count]]&quot; program in Pig Latin:

 input_lines = LOAD '/tmp/my-copy-of-all-pages-on-internet' AS (line:chararray);
 
 ''-- Extract words from each line and put them into a pig bag''
 ''-- datatype, then flatten the bag to get one word on each row''
 words = FOREACH input_lines GENERATE FLATTEN(TOKENIZE(line)) AS word;
 
 ''-- filter out any words that are just white spaces''
 filtered_words = FILTER words BY word MATCHES '\\w+';
 
 ''-- create a group for each word''
 word_groups = GROUP filtered_words BY word;
 
 ''-- count the entries in each group''
 word_count = FOREACH word_groups GENERATE COUNT(filtered_words) AS count, group AS word;
 
 ''-- order the records by count''
 ordered_word_count = ORDER word_count BY count DESC;
 STORE ordered_word_count INTO '/tmp/number-of-words-on-internet';
The above program will generate parallel executable tasks which can be distributed across multiple machines in a Hadoop cluster to count the number of words in a dataset such as all the webpages on the internet.

==Pig vs SQL==
In comparison to SQL, Pig
# uses [[lazy evaluation]], 
# uses [[Extract, transform, load|ETL]], 
# is able to store data at any point during a [[Pipeline (software)|pipeline]], 
# declares execution plans, 
# supports pipeline splits. 
On the other hand, it has been argued [[DBMS]]s are substantially faster than the MapReduce system once the data is loaded, but that loading the data takes considerably longer in the database systems. It has also been argued [[Relational database management system|RDBMS]]s offer out of the box support for column-storage, working with compressed data, indexes for efficient random data access, and transaction-
level fault tolerance.&lt;ref&gt;[http://database.cs.brown.edu/papers/stonebraker-cacm2010.pdf Communications of the ACM: MapReduce and Parallel DBMSs: Friends or Foes?]&lt;/ref&gt;

Pig Latin is [[Procedural programming|procedural]] and fits very naturally in the pipeline paradigm while SQL is instead [[Declarative programming|declarative]]. In SQL users can specify that data from two tables must be joined, but not what join implementation to use (You can specify the implementation of JOIN in SQL, thus &quot;... for many SQL applications the query writer may not have enough knowledge of the data or enough expertise to specify an appropriate join algorithm.&quot;). Pig Latin allows users to specify an implementation or aspects of an implementation to be used in executing a script in several ways.&lt;ref name = ypgd /&gt; In effect, Pig Latin programming is similar to specifying a query execution plan, making it easier for programmers to explicitly control the flow of their data processing task.&lt;ref&gt;[http://infolab.stanford.edu/~olston/publications/sigmod08.pdf ACM SigMod 08: Pig Latin: A Not-So-Foreign Language for Data Processing]&lt;/ref&gt;

SQL is oriented around queries that produce a single result. SQL handles trees naturally, but has no built in mechanism for splitting a data processing stream and applying different operators to each sub-stream. Pig Latin script describes a [[directed acyclic graph]] (DAG) rather than a pipeline.&lt;ref name = ypgd /&gt;

Pig Latin's ability to include user code at any point in the pipeline is useful for pipeline development. If SQL is used, data must first be imported into the database, and then the cleansing and transformation process can begin.&lt;ref name = ypgd&gt;[http://developer.yahoo.com/blogs/hadoop/posts/2010/01/comparing_pig_latin_and_sql_fo/ Yahoo Pig Development Team: Comparing Pig Latin and SQL for Constructing Data Processing Pipelines]&lt;/ref&gt;

==See also==
* [[Apache Hive]]
* [[Sawzall (programming language)|Sawzall]] — similar tool from Google

==References==
{{reflist}}

==External links==
*[http://pig.apache.org/ Official site]

{{Apache}}

[[Category:Cloud computing]]
[[Category:Query languages]]
[[Category:Data modeling languages]]
[[Category:Hadoop]]</text>
      <sha1>q1lid6vw2p4mwllnwhxro9isfqbtmxk</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Sqoop</title>
    <ns>0</ns>
    <id>36956866</id>
    <revision>
      <id>602118001</id>
      <parentid>591859184</parentid>
      <timestamp>2014-03-31T14:57:03Z</timestamp>
      <contributor>
        <username>Trh178</username>
        <id>19309860</id>
      </contributor>
      <minor/>
      <comment>/* External links */ -- Removed duplicate link to 'Official Site'.</comment>
      <text xml:space="preserve" bytes="2851">{{Infobox software
| name                   = Apache Sqoop
| logo                   = 
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 1.4.4
| latest release date    = {{release date|2013|07|31}}
| latest preview version = 
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Data management]]
| license                = [[Apache License]] 2.0
| website                = {{URL|https://sqoop.apache.org}}
}}

'''Sqoop''' is a [[command-line interface]] application for transferring data between [[relational database]]s and [[Hadoop]].&lt;ref name=&quot;mainpage&quot;&gt;{{cite web |url=https://sqoop.apache.org|title=Hadoop: Apache Sqoop|accessdate=Sep 8, 2012}}&lt;/ref&gt; It supports incremental loads of a single table or a free form [[SQL query]] as well as saved jobs which can be run multiple times to import updates made to a database since the last import. Imports can also be used to populate tables in [[Apache Hive|Hive]] or [[HBase]].&lt;ref&gt;{{cite web |url=https://blogs.apache.org/sqoop/entry/apache_sqoop_overview|title=Apache Sqoop - Overview|accessdate=Sep 8, 2012}}&lt;/ref&gt; Exports can be used to put data from Hadoop into a relational database.
Sqoop became a top-level [[Apache Software Foundation|Apache]] project in March 2012.&lt;ref&gt;{{cite web |url=https://blogs.apache.org/sqoop/entry/apache_sqoop_graduates_from_incubator|title=Apache Sqoop Graduates from Incubator|accessdate=Sep 8, 2012}}&lt;/ref&gt;

[[Microsoft]] uses a Sqoop-based connector to help transfer data from [[Microsoft SQL Server]] databases to Hadoop.&lt;ref&gt;{{cite web |url=https://www.microsoft.com/en-us/download/details.aspx?id=27584|title=Microsoft SQL Server Connector for Apache Hadoop|accessdate=Sep 8, 2012}}&lt;/ref&gt;
[[Couchbase, Inc.]] also provides a [[Couchbase Server]]-Hadoop connector by means of Sqoop.&lt;ref&gt;{{cite web |url=http://www.couchbase.com/develop/connectors/hadoop|title=Couchbase Hadoop Connector|accessdate=Sep 8, 2012}}&lt;/ref&gt;

==See also==
* [[Apache Hive|Hive]]
* [[Apache Accumulo|Accumulo]]
* [[HBase]]

==References==
{{reflist}}

==Bibliography==
{{Refbegin}}
*{{Cite book
| first1    = Tom
| last1     = White
| title     = Hadoop: The Definitive Guide
| edition   = 2nd
| chapter   = Chapter 15: Sqoop
| publisher = [[O'Reilly Media]]
| pages      = 477–495
| isbn      = 978-1-449-38973-4
| url       = http://oreilly.com/catalog/9780596521974
}}
{{Refend}}

==External links==
*[https://sqoop.apache.org Official site]
*[https://cwiki.apache.org/confluence/display/SQOOP/Home Sqoop Wiki]
*[http://qnalist.com/q/sqoop-user Sqoop Users Mailing List Archives]

{{Apache}}

[[Category:Cloud computing]]
[[Category:Hadoop]]</text>
      <sha1>ce9wcwe1cvk6c1cfy97bzn6453ktal6</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Hadoop</title>
    <ns>0</ns>
    <id>5919308</id>
    <revision>
      <id>602132456</id>
      <parentid>602132284</parentid>
      <timestamp>2014-03-31T16:49:42Z</timestamp>
      <contributor>
        <username>SteveLoughran</username>
        <id>293997</id>
      </contributor>
      <comment>/* Running Hadoop in compute farm environments */ gridengine is dead.</comment>
      <text xml:space="preserve" bytes="36345">{{multiple issues|
{{advert|date=October 2013}}
{{buzzword|date=October 2013}}
}}
{{Infobox software
| name                   = Apache Hadoop
| logo                   = [[File:Hadoop logo.svg|frameless|Hadoop Logo]]
| screenshot             =
| caption                =
| developer              = [[Apache Software Foundation]]
| status                 = Active
| latest release version = 2.2
| latest release date    = {{release date|2013|10|15}}&lt;ref name=&quot;Hadoop Releases&quot;&gt;{{cite web|url=http://hadoop.apache.org/releases.html |title=Hadoop Releases |publisher=Hadoop.apache.org |date= |accessdate=2013-04-08}}&lt;/ref&gt;
| latest preview version = 2.1.0-beta
| latest preview date    = {{release date|2013|08|25}}&lt;ref name=&quot;Hadoop Releases&quot;/&gt;
| operating system       = [[Cross-platform]]
| programming language   = [[Java (programming language)|Java]]
| genre                  = [[Distributed file system|Distributed File System]]
| posix compliant        = Not [[POSIX]]-Compliant
| license                = [[Apache License]] 2.0
| website                = {{URL|http://hadoop.apache.org/}}
}}

'''Apache Hadoop''' is an [[open source|open-source]] [[software framework]] for storage and large-scale processing of data-sets on clusters of [[commodity hardware]]. Hadoop is an [[List of Apache Software Foundation projects|Apache top-level project]] being built and used by a global community of contributors and users.&lt;ref name=&quot;poweredby&quot;&gt;{{cite web|url=http://wiki.apache.org/hadoop/PoweredBy |title=Applications and organizations using Hadoop |publisher=Wiki.apache.org |date=2013-06-19 |accessdate=2013-10-17}}&lt;/ref&gt; It is licensed under the [[Apache License]] 2.0.

The Apache Hadoop framework is composed of the following modules:
* Hadoop Common – contains libraries and utilities needed by other Hadoop modules
* Hadoop Distributed File System (HDFS) – a distributed file-system that stores data on commodity machines, providing very high aggregate bandwidth across the cluster.
* Hadoop YARN – a resource-management platform responsible for managing compute resources in clusters and using them for scheduling of users' applications.
* Hadoop MapReduce – a programming model for large scale data processing.

All the modules in Hadoop are designed with a fundamental assumption that hardware failures (of individual machines, or racks of machines) are common and thus should be automatically handled in software by the framework. Apache Hadoop's MapReduce and HDFS components originally derived respectively from [[Google]]'s [[MapReduce]] and [[Google File System]] (GFS) papers.

Beyond HDFS, YARN and MapReduce, the entire Apache Hadoop “platform” is now commonly considered to consist of a number of related projects as well&amp;nbsp;– [[Pig (programming tool)|Apache Pig]], [[Apache Hive]], [[HBase|Apache HBase]], [[Apache Spark]], and others.&lt;ref&gt;{{cite web|url=http://hadoop.apache.org/ |title=Hadoop-related projects at |publisher=Hadoop.apache.org |date= |accessdate=2013-10-17}}&lt;/ref&gt;

For the end-users, though MapReduce Java code is common, any programming language can be used with &quot;Hadoop Streaming&quot; to implement the &quot;map&quot; and &quot;reduce&quot; parts of the user's program.&lt;ref&gt;{{cite web|url=http://www.mail-archive.com/nlpatumd@yahoogroups.com/msg00570.html |title=[nlpatumd&amp;#93; Adventures with Hadoop and Perl |publisher=Mail-archive.com |date=2010-05-02 |accessdate=2013-04-05}}&lt;/ref&gt; [[Pig (programming tool)|Apache Pig]], [[Apache Hive]], [[Apache Spark]] among other related projects expose higher level user interfaces like Pig latin and a SQL variant respectively. The Hadoop framework itself is mostly written in the [[Java (programming language)|Java]] programming language, with some native code in [[C (programming language)|C]] and command line utilities written as shell-scripts.

Apache Hadoop is a registered trademark of the [[Apache Software Foundation]].

==History==
Hadoop was created by [[Doug Cutting]] and [[Mike Cafarella]]&lt;ref&gt;{{cite web|url=http://web.eecs.umich.edu/~michjc/bio.html |title=Michael J. Cafarella |publisher=Web.eecs.umich.edu |date= |accessdate=2013-04-05}}&lt;/ref&gt; in 2005. Cutting, who was working at [[Yahoo!]] at the time,&lt;ref&gt;[http://www.sdtimes.com/blog/post/2009/08/10/Hadoop-creator-goes-to-Cloudera.aspx Hadoop creator goes to Cloudera]{{dead link|date=May 2013}}&lt;/ref&gt; named it after his son's toy elephant.&lt;ref&gt;{{cite news |title=Hadoop, a Free Software Program, Finds Uses Beyond Search  |author=Ashlee Vance |newspaper=New York Times |date=2009-03-17 |url=http://www.nytimes.com/2009/03/17/technology/business-computing/17cloud.html |accessdate=2010-01-20 | archiveurl= http://web.archive.org/web/20100211022503/http://www.nytimes.com/2009/03/17/technology/business-computing/17cloud.html?| archivedate= 11 February 2010 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;  It was originally developed to support distribution for the [[Nutch]] search engine project.&lt;ref&gt;&quot;Hadoop contains the distributed computing platform that was formerly a part of Nutch. This includes the Hadoop Distributed Filesystem (HDFS) and an implementation of MapReduce.&quot; [http://hadoop.apache.org/core/ About Hadoop]{{dead link|date=May 2013}}&lt;/ref&gt;

==Architecture==
{{See also|#Hadoop_distributed_file_system|Apache HBase|MapReduce|l1=Hadoop Distributed File System}}

Hadoop consists of the ''Hadoop Common'' package, which provides filesystem and OS level abstractions, a MapReduce engine (either MapReduce/MR1 or YARN/MR2)&lt;ref&gt;{{cite web |url=http://blog.cloudera.com/blog/2012/10/mr2-and-yarn-briefly-explained/ |title=MR2 and YARN Briefly Explained |author=Harsh Chouraria |date=21 October 2012 |website=cloudera.com |publisher=[[Cloudera]] |accessdate=23 October 2013}}&lt;/ref&gt; and the [[#Hadoop distributed file system|Hadoop Distributed File System]] (HDFS). The Hadoop Common package contains the necessary [[JAR (file format)|Java ARchive (JAR)]] files and scripts needed to start Hadoop. The package also provides source code, documentation and a contribution section that includes projects from the Hadoop Community.{{Citation needed|date=October 2012}}

For effective scheduling of work, every Hadoop-compatible file system should provide location awareness: the name of the rack (more precisely, of the network switch) where a worker node is. Hadoop applications can use this information to run work on the node where the data is, and, failing that, on the same rack/switch, reducing backbone traffic. HDFS uses this method when replicating data to try to keep different copies of the data on different racks. The goal is to reduce the impact of a rack power outage or switch failure, so that even if these events occur, the data may still be readable.&lt;ref&gt;{{cite web|url=http://hadoop.apache.org/common/docs/r0.20.2/hdfs_user_guide.html#Rack+Awareness |title=HDFS User Guide |publisher=Hadoop.apache.org |date= |accessdate=2012-05-23}}{{dead link|date=May 2013}}&lt;/ref&gt;
[[File:Hadoop 1.png|thumb|upright=1.2|right|alt=Hadoop cluster|A multi-node Hadoop cluster]]

A small Hadoop cluster includes a single master and multiple worker nodes. The master node consists of a JobTracker, TaskTracker, NameNode and DataNode. A slave or ''worker node'' acts as both a DataNode and TaskTracker, though it is possible to have data-only worker nodes and compute-only worker nodes. These are normally used only in nonstandard applications.&lt;ref name=michael-noll.com_2&gt;{{cite web|title=Running Hadoop on Ubuntu Linux (Multi-Node Cluster)|url=http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/}}&lt;/ref&gt;
Hadoop requires [[JRE|Java Runtime Environment (JRE)]] 1.6 or higher. The standard start-up and shutdown scripts require [[Secure Shell]] (ssh) to be set up between nodes in the cluster.&lt;ref name=michael-noll.com_1&gt;{{cite web|title=Running Hadoop on Ubuntu Linux (Single-Node Cluster)|url=http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/#prerequisites|accessdate=6 June 2013}}&lt;/ref&gt;

In a larger cluster, the HDFS is managed through a dedicated NameNode server to host the file system index, and a secondary NameNode that can generate snapshots of the namenode's memory structures, thus preventing file-system corruption and reducing loss of data. Similarly, a standalone JobTracker server can manage job scheduling. In clusters where the Hadoop MapReduce engine is deployed against an alternate file system, the NameNode, secondary NameNode and DataNode architecture of HDFS is replaced by the file-system-specific equivalent.

===File system===

===={{Anchor|HDFS}}Hadoop distributed file system====
The '''Hadoop distributed file system''' ('''HDFS''') is a distributed, scalable, and portable file-system written in [[Java (software platform)|Java]] for the Hadoop framework. Each node in a Hadoop instance typically has a single namenode; a cluster of datanodes form the HDFS cluster. The situation is typical because each node does not require a datanode to be present. Each datanode serves up blocks of data over the network using a block protocol specific to HDFS. The file system uses the [[TCP/IP]] layer for communication. Clients use [[Remote procedure call]] (RPC) to communicate between each other.

HDFS stores large files (typically in the range of gigabytes to terabytes&lt;ref&gt;
{{cite web|title=HDFS Architecture|url=http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Large_Data_Sets|accessdate=1 September 2013}}
&lt;/ref&gt;) across multiple machines.
It achieves reliability by [[Replication (computer science)|replicating]] the data across multiple hosts, and hence theoretically does not require [[RAID]] storage on hosts (but to increase I/O performance some RAID configurations are still useful). With the default replication value, 3, data is stored on three nodes: two on the same rack, and one on a different rack. Data nodes can talk to each other to rebalance data, to move copies around, and to keep the replication of data high. HDFS is not fully [[POSIX]]-compliant, because the requirements for a POSIX file-system differ from the target goals for a Hadoop application. The tradeoff of  not having a fully POSIX-compliant file-system is increased performance for data [[throughput]] and support for non-POSIX operations such as Append.&lt;ref name=&quot;openlibrary1&quot;&gt;
{{Cite journal
|publisher = Amazon
|author = Yaniv Pessach
|url = http://openlibrary.org/books/OL25423189M/Distributed_Storage_Concepts_Algorithms_and_Implementations
|title = Distributed Storage
|edition = Distributed Storage: Concepts, Algorithms, and Implementations
|publication-date = 2013
|postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to &quot;.&quot; for the cite to end in a &quot;.&quot;, as necessary. --&gt;{{inconsistent citations}}
}}&lt;/ref&gt;

HDFS added the high-availability capabilities, as announced for release 2.0 in May 2012,&lt;ref name=failover&gt;{{cite web |title=Version 2.0 provides for manual failover and they are working on automatic failover: |url=https://hadoop.apache.org/releases.html#23+May%2C+2012%3A+Release+2.0.0-alpha+available  |accessdate= 30 July 2013 |publisher=Hadoop.apache.org}}&lt;/ref&gt; allowing the main metadata server (the NameNode) to be failed over manually to a backup in the event of failure.  The project has also started developing automatic [[fail-over]].

The HDFS file system includes a so-called ''secondary namenode,'' which {{citation needed span|text=misleads some people into thinking|date=September 2013}} that when the primary namenode goes offline, the secondary namenode takes over. In fact, the secondary namenode regularly connects with the primary namenode and builds snapshots of the primary namenode's directory information, which the system then saves to local or remote directories. These checkpointed images can be used to restart a failed primary namenode without having to replay the entire journal of file-system actions, then to edit the log to create an up-to-date directory structure. Because the namenode is the single point for storage and management of metadata, it can become a bottleneck for supporting a huge number of files, especially a large number of small files. HDFS Federation, a new addition, aims to tackle this problem to a certain extent by allowing multiple name-spaces served by separate namenodes.

An advantage of using HDFS is data awareness between the job tracker and task tracker. The job tracker schedules map or reduce jobs to task trackers with an awareness of the data location. For example: if node A contains data (x,y,z) and node B contains data (a,b,c), the job tracker schedules node B to perform map or reduce tasks on (a,b,c) and node A would be scheduled to perform map or reduce tasks on (x,y,z). This reduces the amount of traffic that goes over the network and prevents unnecessary data transfer. When Hadoop is used with other file systems this advantage is not always available. This can have a significant impact on job-completion times, which has been demonstrated when running data-intensive jobs.&lt;ref&gt;{{cite web |url=
http://www.eng.auburn.edu/~xqin/pubs/hcw10.pdf |format=PDF |title= Improving MapReduce performance through data placement in heterogeneous Hadoop Clusters |date=April 2010 |publisher=Eng.auburn.ed}}&lt;/ref&gt;

HDFS was designed{{by whom|date=September 2013}} for mostly immutable files&lt;ref name=&quot;openlibrary1&quot;/&gt; and may not be suitable for systems requiring concurrent write-operations.

Another limitation of HDFS is that it cannot be [[Mount (computing)|mounted]] directly by an existing operating system. Getting data into and out of the HDFS file system, an action that often needs to be performed before and after executing a job, can be inconvenient. A [[Filesystem in Userspace]] (FUSE) [[virtual file system]] has been developed to address this problem, at least for [[Linux]] and some other [[Unix]] systems.

File access can be achieved through the native Java [[API]], the [[Thrift (protocol)|Thrift]] API to generate a client in the language of the users' choosing (C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, Smalltalk, and OCaml), the [[command-line interface]], or browsed through the HDFS-UI [[Web application|webapp]] over [[HTTP]].

====Other file systems====

Hadoop works directly with any [[distributed file system]] that can be mounted by the underlying operating system simply by using a file:// URL; however, this comes at a price: the loss of locality. To reduce network traffic, Hadoop needs to know which servers are closest to the data; this is information that Hadoop-specific file system bridges can provide.

In May 2011, the list of supported file systems bundled with Apache Hadoop were:

* HDFS: Hadoop's own rack-aware file system.&lt;ref&gt;{{cite web|url=http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Rack_Awareness |title=HDFS Users Guide – Rack Awareness |publisher=Hadoop.apache.org |date= |accessdate=2013-10-17}}&lt;/ref&gt; This is designed to scale to tens of petabytes of storage and runs on top of the file systems of the underlying [[operating system]]s.
* [[FTP]] File system: this stores all its data on remotely accessible FTP servers.
* [[Amazon Simple Storage Service|Amazon S3]] file system. This is targeted at clusters hosted on the [[Amazon Elastic Compute Cloud]] server-on-demand infrastructure. There is no rack-awareness in this file system, as it is all remote.

A number of third-party file system bridges have also been written, none of which are currently in Hadoop distributions. However, some commercial distributions of Hadoop ship with an alternative filesystem as the default, -specifically IBM and MapR.

* In 2009 [[IBM]] discussed running Hadoop over the [[IBM General Parallel File System]].&lt;ref&gt;{{cite web|url=http://www.usenix.org/events/hotcloud09/tech/full_papers/ananthanarayanan.pdf|title= Cloud analytics: Do we really need to reinvent the storage stack?|date=June 2009|publisher=IBM}}&lt;/ref&gt; The source code was published in October 2009.&lt;ref&gt;{{cite web|url=https://issues.apache.org/jira/browse/HADOOP-6330|title=HADOOP-6330: Integrating IBM General Parallel File System implementation of Hadoop Filesystem interface|date=2009-10-23|publisher=IBM}}&lt;/ref&gt;
* In April 2010, [[Parascale]] published the source code to run Hadoop against the Parascale file system.&lt;ref&gt;{{cite web|url=https://issues.apache.org/jira/browse/HADOOP-6704|title=HADOOP-6704: add support for Parascale filesystem|date=2010-04-14|publisher=Parascale}}&lt;/ref&gt;
* In April 2010, [[Appistry]] released a Hadoop file system driver for use with its own CloudIQ Storage product.&lt;ref&gt;{{cite web|url=http://resources.appistry.com/news-and-events/press/06072010-appistry-cloudiq-storage-now-generally-available|title=HDFS with CloudIQ Storage|date=2010-07-06|publisher=Appistry,Inc}}&lt;/ref&gt;
* In June 2010, [[HP]] discussed a location-aware [[IBRIX Fusion]] file system driver.&lt;ref&gt;{{cite web|url=http://www.slideshare.net/steve_l/high-availability-hadoop|title=High Availability Hadoop|date=2010-06-09|publisher=HP}}&lt;/ref&gt;
* In May 2011, [[MapR]] Technologies, Inc. announced the availability of an alternative file system for Hadoop, which replaced the HDFS file system with a full random-access read/write file system.

===JobTracker and TaskTracker: the MapReduce engine===
{{main|MapReduce}}

Above the file systems comes the [[MapReduce]] engine, which consists of one ''JobTracker'', to which client applications submit MapReduce jobs. The JobTracker pushes work out to available ''TaskTracker'' nodes in the cluster, striving to keep the work as close to the data as possible. With a rack-aware file system, the JobTracker knows which node contains the data, and which other machines are nearby. If the work cannot be hosted on the actual node where the data resides, priority is given to nodes in the same rack. This reduces network traffic on the main backbone network. If a TaskTracker fails or times out, that part of the job is rescheduled. The TaskTracker on each node spawns off a separate [[Java Virtual Machine]] process to prevent the TaskTracker itself from failing if the running job crashes the JVM. A heartbeat is sent from the TaskTracker to the JobTracker every few minutes to check its status. The Job Tracker and TaskTracker status and information is exposed by [[Jetty (web server)|Jetty]] and can be viewed from a web browser.

If the JobTracker failed on Hadoop 0.20 or earlier, all ongoing work was lost. Hadoop version 0.21 added some checkpointing to this process; the JobTracker records what it is up to in the file system. When a JobTracker starts up, it looks for any such data, so that it can restart work from where it left off.

Known limitations of this approach are:
* The allocation of work to TaskTrackers is very simple. Every TaskTracker has a number of available ''slots'' (such as &quot;4 slots&quot;). Every active map or reduce task takes up one slot. The Job Tracker allocates work to the tracker nearest to the data with an available slot. There is no consideration of the current [[load (computing)|system load]] of the allocated machine, and hence its actual availability.
* If one TaskTracker is very slow, it can delay the entire MapReduce job – especially towards the end of a job, where everything can end up waiting for the slowest task.  With speculative execution enabled, however, a single task can be executed on multiple slave nodes.

====Scheduling====
By default Hadoop uses [[FIFO]], and optional 5 scheduling priorities to schedule jobs from a work queue.&lt;ref&gt;[http://hadoop.apache.org/common/docs/current/commands_manual.html#job job]{{dead link|date=May 2013}}&lt;/ref&gt; In version 0.19 the job scheduler was refactored out of the JobTracker, and added the ability to use an alternate scheduler (such as the ''Fair scheduler'' or the ''Capacity scheduler'').&lt;ref&gt;{{cite web|title=Refactor the scheduler out of the JobTracker|url=https://issues.apache.org/jira/browse/HADOOP-3412|work=Hadoop Common|publisher=Apache Software Foundation|accessdate=9 June 2012}}&lt;/ref&gt;

=====Fair scheduler=====
The fair scheduler was developed by [[Facebook]].&lt;ref&gt;{{cite web |url=http://www.ibm.com/developerworks/library/os-hadoop-scheduling/ |title=Scheduling in Hadoop |author=M. Tim Jones |date=6 December 2011 |website=ibm.com |publisher=[[IBM]] |accessdate=20 November 2013}}&lt;/ref&gt; The goal of the fair scheduler is to provide fast response times for small jobs and [[Quality of service|QoS]] for production jobs. The fair scheduler has three basic concepts.&lt;ref&gt;[http://svn.apache.org/repos/asf/hadoop/mapreduce/trunk/src/contrib/fairscheduler/designdoc/fair_scheduler_design_doc.pdf]{{dead link|date=May 2013}} Hadoop Fair Scheduler Design Document&lt;/ref&gt;
# Jobs are grouped into [[Pool (computer science)|Pools]].
# Each pool is assigned a guaranteed minimum share.
# Excess capacity is split between jobs.

By default, jobs that are uncategorized go into a default pool. Pools have to specify the minimum number of map slots, reduce slots, and a limit on the number of running jobs.

=====Capacity scheduler=====
The capacity scheduler was developed by [[Yahoo]]. The capacity scheduler supports several features that are similar to the fair scheduler.&lt;ref&gt;[http://hadoop.apache.org/common/docs/r0.20.2/capacity_scheduler.html]{{dead link|date=May 2013}} Capacity Scheduler Guide&lt;/ref&gt;

* Jobs are submitted into queues.
* Queues are allocated a fraction of the total resource capacity.
* Free resources are allocated to queues beyond their total capacity.
* Within a queue a job with a high level of priority has access to the queue's resources.

There is no [[preemption (computing)|preemption]] once a job is running.

===Other applications===
The HDFS file system is not restricted to MapReduce jobs. It can be used for other applications, many of which are under development at Apache. The list includes the [[HBase]] database, the [[Apache Mahout]] [[machine learning]] system, and the [[Apache Hive]] [[Data Warehouse]] system. Hadoop can in theory be used for any sort of work that is batch-oriented rather than real-time, that is very data-intensive, and able to work on pieces of the data in parallel. As of October 2009, commercial applications of Hadoop&lt;ref&gt;{{cite web|author=October 10, 2009 |url=http://www.dbms2.com/2009/10/10/enterprises-using-hadoo/ |title=&quot;How 30+ enterprises are using Hadoop&quot;, in DBMS2 |publisher=Dbms2.com |date=2009-10-10 |accessdate=2013-10-17}}&lt;/ref&gt; included:
* Log and/or clickstream analysis of various kinds
* Marketing analytics
* Machine learning and/or sophisticated data mining
* Image processing
* Processing of XML messages
* Web crawling and/or text processing
* General archiving, including of relational/tabular data, e.g. for compliance

==Prominent users==

===Yahoo!===
On February 19, 2008, [[Yahoo! Inc.]] launched what it claimed was the world's largest Hadoop production application. The Yahoo! Search Webmap is a Hadoop application that runs on a more than 10,000 [[Multi-core|core]] [[Linux]] [[Cluster (computing)|cluster]] and produces data that is used in every Yahoo! Web search query.&lt;ref&gt;[http://developer.yahoo.com/blogs/hadoop/2008/02/yahoo-worlds-largest-production-hadoop.html Yahoo! Launches World's Largest Hadoop Production Application (Hadoop and Distributed Computing at Yahoo!)]{{dead link|date=May 2013}}&lt;/ref&gt;

There are multiple Hadoop clusters at Yahoo! and no HDFS file systems or MapReduce jobs are split across multiple datacenters. Every Hadoop cluster node bootstraps the Linux image, including the Hadoop distribution. Work that the clusters perform is known to include the index calculations for the Yahoo! search engine.

On June 10, 2009, Yahoo! made the source code of the version of Hadoop it runs in production available to the public.&lt;ref&gt;{{cite web|url=http://developer.yahoo.com/hadoop/ |title=Hadoop and Distributed Computing at Yahoo! |publisher=Developer.yahoo.com |date=2011-04-20 |accessdate=2013-10-17}}&lt;/ref&gt; Yahoo! contributes all the work it does on Hadoop to the open-source community. The company's developers also fix bugs, provide stability improvements internally and release this patched source code so that other users may benefit from their effort.

===Facebook===
In 2010 [[Facebook]] claimed that they had the largest Hadoop cluster in the world with 21 [[Petabyte|PB]] of storage.&lt;ref&gt;{{cite web|url=http://hadoopblog.blogspot.com/2010/05/facebook-has-worlds-largest-hadoop.html |title=HDFS: Facebook has the world's largest Hadoop cluster! |publisher=Hadoopblog.blogspot.com |date=2010-05-09 |accessdate=2012-05-23}}&lt;/ref&gt; On June 13, 2012 they announced the data had grown to 100 PB.&lt;ref&gt;{{cite web|url=http://www.facebook.com/notes/facebook-engineering/under-the-hood-hadoop-distributed-filesystem-reliability-with-namenode-and-avata/10150888759153920 |title=Under the Hood: Hadoop Distributed File system reliability with Namenode and Avatarnode |publisher=Facebook |date= |accessdate=2012-09-13}}&lt;/ref&gt; On November 8, 2012 they announced the data gathered in the warehouse grows by roughly half a PB per day.&lt;ref&gt;{{cite web|url=https://www.facebook.com/notes/facebook-engineering/under-the-hood-scheduling-mapreduce-jobs-more-efficiently-with-corona/10151142560538920 |title=Under the Hood: Scheduling MapReduce jobs more efficiently with Corona |publisher=Facebook |date= |accessdate=2012-11-09}}&lt;/ref&gt;

===Other users===
As of 2013, Hadoop adoption is widespread. For example, more than half of the Fortune 50 uses Hadoop.&lt;ref&gt;{{cite press release |author=&lt;!--Staff writer(s); no by-line.--&gt; |title=Altior's AltraSTAR – Hadoop Storage Accelerator and Optimizer Now Certified on CDH4 (Cloudera's Distribution Including Apache Hadoop Version 4) |url=http://www.prnewswire.com/news-releases/altiors-altrastar---hadoop-storage-accelerator-and-optimizer-now-certified-on-cdh4-clouderas-distribution-including-apache-hadoop-version-4-183906141.html |location=Eatontown, New Jersey |publisher=Altior Inc. |date=2012-12-18 |accessdate=2013-10-30}}&lt;/ref&gt;

==Hadoop on Amazon EC2/S3 services==
It is possible to run Hadoop on [[Amazon Elastic Compute Cloud]] (EC2) and [[Amazon Simple Storage Service]] (S3).&lt;ref&gt;{{cite web|last=Varia|first=Jinesh (@jinman)|title=Taking Massive Distributed Computing to the Common Man – Hadoop on Amazon EC2/S3|url=http://aws.typepad.com/aws/2008/02/taking-massive.html|work=Amazon Web Services Blog|publisher=Amazon|accessdate=9 June 2012}}&lt;/ref&gt; As an example [[The New York Times]] used 100 Amazon EC2 instances and a Hadoop application to process 4&amp;nbsp;TB of raw image [[TIFF]] data (stored in S3) into 11 million finished [[PDF]]s in the space of 24 hours at a computation cost of about $240 (not including bandwidth).&lt;ref&gt;{{cite news| url=http://open.blogs.nytimes.com/2007/11/01/self-service-prorated-super-computing-fun/?scp=1&amp;sq=self%20service%20prorated&amp;st=cse | work=The New York Times | title=Self-service, Prorated Super Computing Fun! | first=Derek | last=Gottfrid | date=November 1, 2007 | accessdate=May 4, 2010}}&lt;/ref&gt;

There is support for the S3 file system in Hadoop distributions, and the Hadoop team generates EC2 machine images after every release. From a pure performance perspective, Hadoop on S3/EC2 is inefficient, as the S3 file system is remote and delays returning from every write operation until the data is guaranteed not to be lost. This removes the locality advantages of Hadoop, which schedules work near data to save on network load.

===Amazon Elastic MapReduce===
Elastic MapReduce (EMR)&lt;ref&gt;http://aws.amazon.com/elasticmapreduce/&lt;/ref&gt; was introduced by [[Amazon.com|Amazon]] in April 2009. Provisioning of the Hadoop cluster, running and terminating jobs, and handling data transfer between EC2 and S3 are automated by Elastic MapReduce. [[Apache Hive]], which is built on top of Hadoop for providing data warehouse services, is also offered in Elastic MapReduce.&lt;ref&gt;{{cite web|url=http://s3.amazonaws.com/awsdocs/ElasticMapReduce/latest/emr-dg.pdf |title=Amazon Elastic MapReduce Developer Guide |format=PDF |date= |accessdate=2013-10-17}}&lt;/ref&gt;

Support for using Spot Instances&lt;ref&gt;http://aws.amazon.com/ec2/spot-instances/&lt;/ref&gt; was later added in August 2011.&lt;ref&gt;{{cite web|url=http://aws.amazon.com/about-aws/whats-new/2011/08/18/amazon-elastic-mapreduce-now-supports-spot-instances/ |title=Amazon Elastic MapReduce Now Supports Spot Instances |publisher=Aws.amazon.com |date=2011-08-18 |accessdate=2013-10-17}}&lt;/ref&gt; Elastic MapReduce is fault tolerant for slave failures,&lt;ref&gt;{{cite web|url=http://aws.amazon.com/elasticmapreduce/faqs/#cluster-10 |title=Amazon Elastic MapReduce FAQs |publisher=Aws.amazon.com |date= |accessdate=2013-10-17}}&lt;/ref&gt; and it is recommended to only run the Task Instance Group on spot instances to take advantage of the lower cost while maintaining availability.&lt;ref&gt;{{Youtube|id=66rfnFA0jpM|title=Using Spot Instances with EMR}}&lt;/ref&gt;

==Industry support of academic clusters==
[[IBM]] and [[Google]] announced an initiative in 2007 to use Hadoop to support university courses in distributed computer programming.&lt;ref&gt;{{cite web|url=http://www.google.com/intl/en/press/pressrel/20071008_ibm_univ.html |title=Google Press Center: Google and IBM Announce University Initiative to Address Internet-Scale Computing Challenges |publisher=Google.com |date=2007-10-08 |accessdate=2013-10-17}}&lt;/ref&gt;

In 2008 this collaboration, the Academic Cloud Computing Initiative (ACCI), partnered with the [[National Science Foundation]] to provide grant funding to academic researchers interested in exploring large-data applications.  This resulted in the creation of the Cluster Exploratory (CLuE) program.&lt;ref&gt;{{cite web|author=Name (required) |url=http://hadoopcommunity.wordpress.com/2009/09/29/nsf-google-ibm-clue-pi-meeting-october-5-2009/ |title=NSF, Google, IBM form CLuE |publisher=Hadoopcommunity.wordpress.com |date= |accessdate=2013-10-17}}&lt;/ref&gt;

==Running Hadoop in compute farm environments==
Hadoop can also be used in compute farms and [[high-performance computing]] environments. Instead of setting up a dedicated Hadoop cluster, an existing compute farm can be used if the resource manager of the cluster is aware of the Hadoop jobs, and thus Hadoop jobs can be scheduled like other jobs in the cluster.

===Condor integration===
The [[Condor High-Throughput Computing System]] integration was presented at the ''Condor Week'' conference in 2010.&lt;ref&gt;{{cite web|url=http://www.cs.wisc.edu/condor/CondorWeek2010/condor-presentations/thain-condor-hadoop.pdf|title=Condor integrated with Hadoop's Map Reduce|date=2010-04-15|publisher=[[University of Wisconsin–Madison]]}}&lt;/ref&gt;

==Commercial support==
&lt;!--
Please don't go overboard in marketing here, as it will only be edited out. Use external citations rather than press releases, and be aware of wikipedia's rules regarding conflict of interest and external links, WP:COI and WP:EL specifically
--&gt;
A number of companies offer commercial implementations or support for Hadoop.&lt;ref&gt;{{cite web|url=http://gigaom.com/cloud/why-we-need-more-hadoop-innovation/ |title=Why the Pace of Hadoop Innovation Has to Pick Up |publisher=Gigaom.com |date=2011-04-25 |accessdate=2013-10-17}}&lt;/ref&gt;

===ASF's view on the use of &quot;Hadoop&quot; in product names===
The Apache Software Foundation has stated that only software officially released by the Apache Hadoop Project can be called ''Apache Hadoop'' or ''Distributions of Apache Hadoop''.&lt;ref&gt;{{cite web|url=http://wiki.apache.org/hadoop/Defining%20Hadoop |title=Defining Hadoop |publisher=Wiki.apache.org |date=2013-03-30 |accessdate=2013-10-17}}&lt;/ref&gt; The naming of products and derivative works from other vendors and the term &quot;compatible&quot; are somewhat controversial within the Hadoop developer community.&lt;ref&gt;{{cite web|url=http://mail-archives.apache.org/mod_mbox/hadoop-general/201105.mbox/%3C4DC91392.2010308@apache.org%3E |title=Defining Hadoop Compatibility: revisited |publisher=Mail-archives.apache.org |date=2011-05-10 |accessdate=2013-10-17}}&lt;/ref&gt;

==Papers==
Some papers influenced the birth and growth of Hadoop and big data processing. Here is a partial list:
* 2004 [https://www.usenix.org/legacy/publications/library/proceedings/osdi04/tech/full_papers/dean/dean_html/index.html MapReduce: Simplified Data Processing on Large Clusters] by Jeffrey Dean and Sanjay Ghemawat from Google Lab. This paper inspired [[Doug Cutting]] to develop an open-source implementation of the Map-Reduce framework. He named it [http://hadoop.apache.org/ Hadoop], after his son's toy elephant.
* 2005 [http://www.eecs.berkeley.edu/~franklin/Papers/dataspaceSR.pdf From Databases to Dataspaces: A New Abstraction for Information Management], the authors highlight the need for storage systems to accept all data formats and to provide APIs for data access that evolve based on the storage system’s understanding of the data.
* 2006 [http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/bigtable-osdi06.pdf Bigtable: A Distributed Storage System for Structured Data] from Google Lab.
* 2008 [http://www.vldb.org/pvldb/1/1454211.pdf H-store: a high-performance, distributed main memory transaction processing system]
* 2009 [http://db.cs.berkeley.edu/jmh/papers/madskills-032009.pdf MAD Skills: New Analysis Practices for Big Data]
* 2011 [http://borthakur.com/ftp/RealtimeHadoopSigmod2011.pdf Apache Hadoop Goes Realtime at Facebook]

==See also==
{{Portal|Free software}}
* [[Apache Accumulo]] – Secure Big Table
* [[Apache Cassandra]] – A column-oriented database that supports access from Hadoop
* [[Apache CouchDB]] is a database that uses JSON for documents, JavaScript for MapReduce queries, and regular HTTP for an API
* [[Apache Mahout]] – Machine Learning algorithms implemented on Hadoop
* [[Big data]]
* [[Cloud computing]]
* [[Data Intensive Computing]]
* [[Datameer]] Analytics Solution (DAS) – data source integration, storage, analytics engine and visualization
* [[HBase]] – [[BigTable]]-model database
* [[HPCC]] – [[LexisNexis]] Risk Solutions High Performance Computing Cluster
* [[Hypertable]] – HBase alternative
* [[MapReduce]] – Hadoop's fundamental data filtering algorithm
* [[Nutch]] – An effort to build an open source search engine based on [[Lucene]] and Hadoop, also created by Doug Cutting
* [[Pentaho]] – Open source data integration (Kettle), analytics, reporting, visualization and predictive analytics directly from Hadoop nodes
* [[Sector/Sphere]] – Open source distributed storage and processing
* [[Simple Linux Utility for Resource Management]]
* [[Talend]] – An open source integration software

==References==
{{Reflist|2}}

==Bibliography==
{{Refbegin}}
*{{Cite book
| first1    = Chuck
| last1     = Lam
| date      = July 28, 2010
| title     = Hadoop in Action
| edition   = 1st
| publisher = [[Manning Publications]]
| page     = 325
| isbn      = 1-935182-19-6
| url       =
}}
*{{Cite book
| first1    = Jason
| last1     = Venner
| date      = June 22, 2009
| title     = Pro Hadoop
| edition   = 1st
| publisher = [[Apress]]
| page     = 440
| isbn      = 1-4302-1942-4
| url       = http://www.apress.com/book/view/1430219424
}}
*{{Cite book
| first1    = Tom
| last1     = White
| date      = June 16, 2009
| title     = Hadoop: The Definitive Guide
| edition   = 1st
| publisher = [[O'Reilly Media]]
| page     = 524
| isbn      = 0-596-52197-9
| url       = http://oreilly.com/catalog/9780596521974
}}
{{Refend}}

==External links==
* {{Official website|http://hadoop.apache.org/|name=Official Hadoop Homepage}}
* {{Official website|http://wiki.apache.org/hadoop/|name=Official Hadoop Wiki}}
* [http://www.stanford.edu/class/ee380/Abstracts/111116.html Introducing Apache Hadoop: The Modern Data Operating System] — lecture given at [[Stanford University]] by Co-Founder and CTO of Cloudera, Amr Awadallah ([http://ee380.stanford.edu/cgi-bin/videologger.php?target=111116-ee380-300.asx video archive]).
{{apache}}

[[Category:Hadoop| ]]
[[Category:Free software programmed in Java]]
[[Category:Free system software]]
[[Category:Distributed file systems]]
[[Category:Cloud computing]]
[[Category:Cloud infrastructure]]
[[Category:Free software for cloud computing]]</text>
      <sha1>o7zlbyc8emgefbxz7gyd8lj8pi6972q</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Oozie</title>
    <ns>0</ns>
    <id>6728387</id>
    <revision>
      <id>573117467</id>
      <parentid>571552920</parentid>
      <timestamp>2013-09-16T06:28:00Z</timestamp>
      <contributor>
        <ip>61.246.57.3</ip>
      </contributor>
      <comment>Grammar correction</comment>
      <text xml:space="preserve" bytes="1976">{{multiple issues|
{{advert|date=January 2013}}
{{notability|date=January 2013}}
{{primary sources|date=January 2013}}
{{underlinked|date=January 2013}}
}}
'''Oozie''' is a workflow scheduler system to manage '''[[Apache Hadoop|Hadoop]]''' jobs. It is a server-based Workflow Engine specialized in running workflow jobs with actions that run Hadoop MapReduce and Pig jobs. Oozie is implemented as a Java Web-Application that runs in a Java Servlet-Container.

For the purposes of Oozie, a workflow is a collection of actions (e.g. Hadoop Map/Reduce jobs, Pig jobs) arranged in a control dependency DAG (Direct Acyclic Graph). A &quot;control dependency&quot; from one action to another means that the second action can't run until the first action has completed.
The workflow actions start jobs in remote systems (Hadoop or Pig). Upon action completion, the remote systems call back Oozie to notify the action completion; at this point Oozie proceeds to the next action in the workflow. 

Oozie workflows contain control flow nodes and action nodes. 
Control flow nodes define the beginning and the end of a workflow (start, end and fail nodes) and provide a mechanism to control the workflow execution path (decision, fork and join nodes). 
Action nodes are the mechanism by which a workflow triggers the execution of a computation/processing task. Oozie provides support for different types of actions: Hadoop MapReduce, Hadoop file system, Pig, SSH, HTTP, eMail and Oozie sub-workflow. Oozie can be extended to support additional types of actions.

Oozie workflows can be parameterized (using variables like ${inputDir} within the workflow definition). When submitting a workflow job, values for the parameters must be provided. If properly parameterized (using different output directories), several identical workflow jobs can run concurrently.

Oozie is distributed under the Apache License 2.0.

==References==
* {{Official website|http://oozie.apache.org/}}

[[Category:Hadoop]]</text>
      <sha1>fx1tvvxc4m2q155yd3o5rlcceswh978</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Cloudera Impala</title>
    <ns>0</ns>
    <id>40147148</id>
    <revision>
      <id>599550803</id>
      <parentid>592332364</parentid>
      <timestamp>2014-03-14T07:27:28Z</timestamp>
      <contributor>
        <ip>67.148.60.68</ip>
      </contributor>
      <text xml:space="preserve" bytes="3831">{{Infobox Software
| name = Cloudera Impala
| logo =
| screenshot =
| caption =
| developer =
| status = Active
| latest release version = 1.2.4
| latest release date = 
| latest preview version =
| latest preview date =
| operating system = [[Cross-platform]]
| programming language = 
| genre =
| license = 
| website = {{URL|http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html|Cloudera Impala Homepage}}
}}
'''Cloudera Impala''' is [[Cloudera]]'s [[open source software|open source]] [[massively parallel processing]] (MPP) SQL query engine for data stored in a [[computer cluster]] running [[Apache Hadoop]].&lt;ref name=&quot;Cloudera Impala&quot;&gt;{{cite web|title=Cloudera Impala|url=http://www.cloudera.com/content/cloudera/en/products-and-services/cdh/impala.html|accessdate=14 March 2014}}&lt;/ref&gt; 

==Description==
Cloudera Impala is a query engine that runs on Apache Hadoop. 
The project was announced in October 2012 with a public [[beta test]] distribution.&lt;ref&gt;{{cite web |url= http://www.zdnet.com/cloudera-aims-to-bring-real-time-queries-to-hadoop-big-data-7000005951/ |title=Cloudera aims to bring real-time queries to Hadoop, big data |author= Larry Digna |date= October 24, 2012 |work= Between the lines blog |publisher= ZDNet |accessdate= January 20, 2014 }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://www.zdnet.com/clouderas-impala-brings-hadoop-to-sql-and-bi-7000006413/ |title=Cloudera’s Impala brings Hadoop to SQL and BI |author= Andrew Brust |date= October 25, 2012 |work= ZDNet |accessdate= January 20, 2014 }}&lt;/ref&gt;
The Apache-licensed Impala project brings scalable parallel database technology to Hadoop, enabling users to issue low-latency SQL queries to data stored in HDFS and Apache HBase without requiring data movement or transformation. Impala is integrated with Hadoop to use the same file and data formats, metadata, security and resource management frameworks used by MapReduce, Apache Hive, Apache Pig and other Hadoop software.

Impala is promoted for analysts and data scientists to perform analytics on data stored in Hadoop via SQL or [[business intelligence]]  tools. The result is that large-scale data processing (via MapReduce) and interactive queries can be done on the same system using the same data and metadata – removing the need to migrate data sets into specialized systems and/or proprietary formats simply to perform analysis.

Features include:
* Supports [[HDFS#Hadoop_distributed_file_system|HDFS]] and [[Apache HBase]] storage
* Reads Hadoop date formats, including text, LZO, SequenceFile, Avro and RCFile
* Supports Hadoop security (Kerberos authentication)
* Fine-grained, role-based authorization with Sentry&lt;ref&gt;[http://www.cloudera.com/content/cloudera/en/products/cdh/sentry.html Sentry]&lt;/ref&gt;
* Uses metadata, ODBC driver, and SQL syntax from [[Apache Hive]]

In early 2013, a [[column-oriented DBMS|column-oriented data format]] called Parquet was announced for architectures including Impala.&lt;ref&gt;{{Cite web |title= Parquet: Columnar Storage for Hadoop |work= Project web site |year= 2013 |url= http://parquet.io/ |accessdate= January 20, 2014 }}&lt;/ref&gt;
In December 2013, [[Amazon Web Services]] announced support for Impala.&lt;ref&gt;{{Cite web |title= Announcing Support for Impala with Amazon Elastic MapReduce |publisher= Amazon.com |date= December 12, 2013 |url= http://aws.amazon.com/about-aws/whats-new/2013/12/12/announcing-support-for-impala-with-amazon-elastic-mapreduce/ |accessdate= January 20, 2014 }}&lt;/ref&gt;

==References==
{{reflist}}

==External links==

*[http://www.cloudera.com/content/cloudera/en/products/cdh/impala.html Cloudera Impala] commercial web site
*[https://github.com/cloudera/impala Impala GitHub] project web site

{{DEFAULTSORT:Impala}}
[[Category:Free system software]]
[[Category:Cloud computing]]
[[Category:Hadoop]]</text>
      <sha1>gm1zgdqv7rqw9o18zurpl34a07o3uq7</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hue (Hadoop)</title>
    <ns>0</ns>
    <id>40564013</id>
    <revision>
      <id>598472864</id>
      <parentid>598401741</parentid>
      <timestamp>2014-03-06T23:22:17Z</timestamp>
      <contributor>
        <username>Romainrr</username>
        <id>19747014</id>
      </contributor>
      <comment>Added latest screenshot</comment>
      <text xml:space="preserve" bytes="1965">{{multiple issues|
{{notability|Products|date=September 2013}}
{{refimprove|date=September 2013}}
}}

{{Infobox software
| name                   = Cloudera Hue
| status                 = Active
| programming language   = [[Python (programming language)|Python]]
| genre                  = Web platform
| license                = [[Apache License]] 2.0
| website                = {{URL|http://gethue.com/}}
| logo                   = [[File:Hue official logo.png|frameless|Hue Logo]]
| latest release version = 3.5.0
| latest release date    = {{Start date and age|2013|12|04}}
}}

'''Hue''' is an [[open source|open-source]] Web interface that supports [[Apache Hadoop]] and its ecosystem, licensed under the Apache v2 license.&lt;ref name=&quot;Hue license&quot;&gt;{{cite web|url=https://github.com/cloudera/hue#license|title=Apache v2 License}}&lt;/ref&gt;

== Features ==

Hue aggreates the most common [[Apache Hadoop]] components into a single interface and targets the user experience. Its main goal is to have the users &quot;just use&quot; Hadoop without worrying about the underlying complexity or using a command line.
Hue provides a series of Hadoop Tutorials on its [http://gethue.tumblr.com blog].

== Applications ==

Hadoop
* File Browser for [[HDFS]]
* Job Browser for [[MapReduce]]/YARN
* [[Apache HBase]] Browser
* Query editors for [[Apache Hive]], [[Apache Pig]], Impala
* Apache Sqoop2 editor
* Apache Oozie editor and dashboard
* [[Apache ZooKeeper]] Browser

Generic
* Spark Editor
* SQL Editor for traditional databases

== Distribution ==

Hue works with upstream [[Apache Hadoop]] and provides &lt;ref name=&quot;Hue releases&quot;&gt;{{cite web|url=http://gethue.tumblr.com/tagged/release|title=releases}}&lt;/ref&gt; on its website. Hue is also present in some major Hadoop distributions (CDH, HDP, BigTop) and demo VM.

=== Interface ===
[[File:Hue_3.6_interface.png|thumb|thumb|center|upright=4|Hue 3.6 interface]]

==References==
{{Reflist|2}}

[[Category:Hadoop]]
[[Category:Big data]]</text>
      <sha1>jxbkml2rt6n6rglc0xslf6f8twlpptg</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Quantcast File System</title>
    <ns>0</ns>
    <id>37854704</id>
    <revision>
      <id>585503987</id>
      <parentid>574438541</parentid>
      <timestamp>2013-12-10T22:48:22Z</timestamp>
      <contributor>
        <username>Jonsafari</username>
        <id>110634</id>
      </contributor>
      <comment>not main article in category</comment>
      <text xml:space="preserve" bytes="3745">{{Infobox software
| name                   = Quantcast File System (QFS)
| logo                   = 
| screenshot             =
| caption                =
| developers              = Sriram Rao, [[Quantcast]]
| status                 = Active
| latest release version = 1.0
| latest release date    = {{release date|2012|09|27}}
| latest preview version = 
| latest preview date    = 
| operating system       = 
| programming language   = [[C++ (programming language)|C++]]
| genre                  = [[Distributed file system|Distributed File System]]
| posix compliant        = 
| license                = [[Apache License]] 2.0
| website                = {{URL|http://quantcast.github.com/qfs/}}
}}
{{Portal|Free software}}
'''Quantcast File System''' (QFS) is an open-source [[distributed file system]] software package for large-scale [[MapReduce]] or other batch-processing workloads. It was designed as an alternative to [[Apache Hadoop]]’s [[HDFS]], intended to deliver better performance and cost-efficiency for large-scale processing clusters.

==Design==
QFS is software that runs on a cluster of hundreds or thousands of commodity [[Linux]] servers and allows other software layers to interact with them as if they were one giant hard drive. It has three components:

* A chunk server runs on each machine that will host data, manages I/O to its hard drives, and monitors its activity and capacity.
* A central process called the metaserver keeps the directory structure and maps of files to physical storage. It coordinates activities of all the chunk servers and monitors the overall health of the file system.  For high performance it holds all its data in memory, writing checkpoints and transaction logs to disk for recovery.
* A client component is the interface point that presents a file system [[Application programming interface|API]] to other layers of the software. It makes requests of the metaserver to identify which chunk servers hold (or will hold) its data, then interacts with the chunk servers directly to read and write.

In a cluster of hundreds or thousands of machines, the odds are low that all will be running and reachable at any given moment, so fault tolerance is the central design challenge. QFS meets it with [[Reed–Solomon error correction]]. The form of Reed–Solomon encoding used in QFS stores redundant data in nine places and is able to reconstruct the file from any six of these stripes.&lt;ref&gt;[http://strata.oreilly.com/2012/09/qfs-improves-performance-of-hadoop-file-system.html QFS improves performance of Hadoop file system - Strata]&lt;/ref&gt; When it writes a file, it stripes it across by default nine physically different machines—six holding the data, three holding parity information. Any three of those can become unavailable; if any six remain readable, QFS can reconstruct the original data. The result is fault tolerance at a cost of a 50% expansion of data. 

QFS is written in [[C++ (programming language)|C++]], operates within a fixed memory footprint, and uses direct I/O.

==History==
QFS evolved from the Kosmos File System (KFS), an open source project started by [[Kosmix]] in 2005.  [[Quantcast]] adopted KFS in 2007, built its own improvements on top of it over the next several years, and released QFS 1.0 as an open source project in September, 2012.&lt;ref&gt;[http://gigaom.com/data/quantcast-releases-bigger-faster-stronger-hadoop-file-system/ Quantcast releases bigger, faster, stronger Hadoop file system — Tech News and Analysis]&lt;/ref&gt;

==References==
&lt;references/&gt;

==External links==
* {{Official website|http://quantcast.github.com/qfs/|name=QFS Open Source Project}}

[[Category:Hadoop]]
[[Category:Free system software]]
[[Category:Distributed file systems]]</text>
      <sha1>btc4jvaz5l1limh2jzk9hcl1my4e0op</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Apache Spark</title>
    <ns>0</ns>
    <id>42164234</id>
    <revision>
      <id>598987489</id>
      <parentid>598987389</parentid>
      <timestamp>2014-03-10T14:22:27Z</timestamp>
      <contributor>
        <username>Anthony Appleyard</username>
        <id>119438</id>
      </contributor>
      <comment>rv histmerge junk</comment>
      <text xml:space="preserve" bytes="7522">{{Infobox Software
| name                   = Apache Spark
| logo                   = [[File:Spark-logo-192x100px.png|frameless|Spark Logo]]
| caption                =
| developer              = [[Apache Software Foundation]], [[UC Berkeley]], Databricks
| status                 = Active
| latest release version = v0.9.0
| latest release date    = {{release date|2014|02|02}}
| latest preview version = 
| latest preview date    = 
| operating system       = [[Linux]], [[MAC OS]], [[Windows]]
| size                   = 
| programming language   = [[Scala (programming language)|Scala]], [[Java (programming language)|Java]], [[Python (programming language)|Python]]
| genre                  = data analytics, [[machine learning]] algorithms
| license                = [[Apache License]] 2.0 
| website                = http://spark.apache.org/
}}

'''Apache Spark''' is an open-source&lt;ref name=&quot;faq&quot;/&gt; data analytics cluster computing framework originally developed in the AMPLab at [[UC Berkeley]]. Spark fits into the Hadoop open-source community, building on top of the [[Apache Hadoop#Hadoop distributed file system|Hadoop Distributed File System]] (HDFS).&lt;ref&gt;[https://amplab.cs.berkeley.edu/software/ Figure showing Spark in relation to other open-source Software projects including Hadoop]&lt;/ref&gt; However, Spark is not tied to the two-stage [[MapReduce]] paradigm, and promises performance up to 100 times faster than Hadoop MapReduce, for certain applications.&lt;ref&gt;{{cite paper|first1=Reynold| last1=Xin| first2=Josh |last2=Rosen| first3=Matei| last3=Zaharia| first4=Michael| last4=Franklin| first5=Scott| last5=Shenker| first6=Ion| last6=Stoica|title=Shark: SQL and Rich Analytics at Scale| conference=SIGMOD 2013|date=June 2013| url=https://amplab.cs.berkeley.edu/wp-content/uploads/2013/02/shark_sigmod2013.pdf}}&lt;/ref&gt; Spark provides primitives for in-memory cluster computing that allows user programs to load data into a cluster's memory and query it repeatedly, making it well suited to machine learning algorithms.&lt;ref&gt;{{cite AV media| url=http://www.youtube.com/watch?v=qLvLg-sqxKc| location=Invited Talk at NIPS 2011 Big Learning Workshop: Algorithms, Systems, and Tools for Learning at Scale
|people=Matei Zaharia|title=Spark: In-Memory Cluster Computing for Iterative and Interactive Applications}}&lt;/ref&gt;

Spark became an Apache Top-Level Project in February 2014,&lt;ref&gt;{{cite web |url=https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces50 |title=The Apache Software Foundation Announces Apache&amp;#8482 Spark&amp;#8482 as a Top-Level Project |author=&lt;!--Staff writer(s); no by-line.--&gt; |date=27 February 2014 |website=apache.org |publisher=Apache Software Foundation |accessdate=4 March 2014}}&lt;/ref&gt; and was previously an Apache Incubator project since June 2013.&lt;ref name=&quot;mail-archives.apache.org&quot;&gt;[http://mail-archives.apache.org/mod_mbox/incubator-general/201306.mbox/%3CCDE7B773.E9A48%25chris.a.mattmann%40jpl.nasa.gov%3E  Email thread archive: &amp;#91;RESULT&amp;#93; &amp;#91;VOTE&amp;#93; Apache Spark for the Incubator]&lt;/ref&gt; It has received code contributions from large companies that use Spark, including Yahoo! and Intel&lt;ref&gt;{{cite news| work=wired.com| title=Spark: Open Source Superstar Rewrites Future of Big Data| author= Cade Metz| date=June 19, 2013| url=http://www.wired.com/wiredenterprise/2013/06/yahoo-amazon-amplab-spark/all/}}&lt;/ref&gt; as well as small companies and startups such as Conviva,&lt;ref&gt;{{cite web| author=Dilip Joseph| date=December 27, 2011| title=Using Spark and Hive to process BigData at Conviva|url=http://www.conviva.com/using-spark-and-hive-to-process-bigdata-at-conviva/}}&lt;/ref&gt; Quantifind,&lt;ref&gt;{{cite AV media| url=http://www.youtube.com/watch?v=DI81yppHI6w| people=Erich Nachbar| title=Running Spark In Production| location=Spark use-cases session at AMP Camp One Aug 2012, UC Berkeley}}&lt;/ref&gt; ClearStory Data,&lt;ref&gt;[http://strataconf.com/strata2013/public/schedule/detail/26743 Beyond Hadoop MapReduce: Interactive Analytic Insights Using Spark] - Abstract of talk given by ClearStory Data CEO Sharmila Shahani-Mulligan about using Spark&lt;/ref&gt; Ooyala&lt;ref&gt;{{cite web|title=Fast Spark Queries on In-Memory Datasets|author=Evan Chan|date=June 2013|url=http://engineering.ooyala.com/blog/fast-spark-queries-memory-datasets}}&lt;/ref&gt; and many more.&lt;ref&gt;[http://andykonwinski.com/spark-and-shark-in-the-news/ Spark, Shark, and BDAS In the News]&lt;/ref&gt; By Mar 2014, over 150 individual developers had contributed code to Spark, representing over 30 different companies.&lt;ref&gt;[http://databricks.com/blog/2013/10/27/the-growing-spark-community.html The Growing Spark Community]&lt;/ref&gt; Prior to joining Apache Incubator, versions 0.7 and earlier were licensed under the [[BSD License]].&lt;ref name=&quot;faq&quot;&gt;{{cite web |url=http://spark.incubator.apache.org/faq.html |title=Spark FAQ |website=apache.org |publisher=[[Apache Software Foundation]] |accessdate=10 October 2013}}&lt;/ref&gt;

== Features==
* Java, Scala, and Python APIs.
* Proven scalability to 100 nodes in the research lab&lt;ref&gt;{{cite conference |url=https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf |title=Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing |last1=Zaharia |first1=Matei |authorlink1=http://www.cs.berkeley.edu/~matei/ |last2=Chowdhury |first2=Mosharaf |authorlink2=http://www.mosharaf.com/ |date=25 April 2012 |publisher=[[USENIX]]}}&lt;/ref&gt; and 80 nodes in production at [[Yahoo!]].&lt;ref&gt;{{cite web |url=http://ampcamp.berkeley.edu/wp-content/uploads/2013/07/andy-feng-ampcamp-3-presentation-Spark_on_YARN.pdf |title=Spark and Hadoop at Yahoo: Brought to you by YARN |last1=Feng |first1=Andy |date=23 July 2013 |website=[[University of California, Berkeley]]|accessdate=11 October 2013}}&lt;/ref&gt;
* Ability to cache datasets in memory for interactive data analysis: extract a working set, cache it, query it repeatedly.
* Interactive command line interface (in Scala or Python) for low-latency data exploration at scale.
* Higher level library for stream processing, through [http://spark.apache.org/streaming/ Spark Streaming].
* Higher level libraries for machine learning and graph processing that because of the distributed memory-based Spark architecture are ten times as fast as Hadoop disk-based [[Apache Mahout]] and even scale better than [[Vowpal Wabbit]].&lt;ref&gt;{{cite web |url=http://www.slideshare.net/chaochen5496/mlllib-sparkmeetup8613finalreduced |title=Spark Meetup: MLbase, Distributed Machine Learning with Spark |last1=Sparks |first1=Evan |last2=Talwalkar |first2=Ameet |date=2013 August 6 |website=slideshare.net |publisher=Spark User Meetup, San Francisco, California |accessdate=10 February 2014}}&lt;/ref&gt;

==External links==
* [http://spark.apache.org Spark Homepage]
* [http://shark.cs.berkeley.edu Shark] - A large-scale data warehouse system for Spark designed to be compatible with [[Apache Hive]]. It can execute Hive QL queries up to 100 times faster than Hive without any modification to the existing data or queries.
* [http://spark.apache.org/docs/latest/streaming-programming-guide.html Spark Streaming] - A component of Spark that extends core Spark functionality to allow for real-time analysis of streaming data.
* [http://strata.oreilly.com/2013/11/how-companies-are-using-spark.html How companies are using Spark]

==References==
{{Reflist}}

[[Category:Cluster computing]]
[[Category:Data mining and machine learning software]]
[[Category:Hadoop]]
[[Category:University of California, Berkeley]]</text>
      <sha1>cde9742tzz1r4c05m626wnzl6wloiw2</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Big data</title>
    <ns>0</ns>
    <id>27051151</id>
    <revision>
      <id>602496718</id>
      <parentid>602401733</parentid>
      <timestamp>2014-04-03T00:17:36Z</timestamp>
      <contributor>
        <ip>50.193.204.14</ip>
      </contributor>
      <comment>/* Further reading */</comment>
      <text xml:space="preserve" bytes="54991">{{About|large collections of data|the band|Big Data (band)}}

'''Big data'''&lt;ref&gt;{{cite book |first=Tom |last=White |title=Hadoop: The Definitive Guide |url=http://books.google.com/books?id=Wu_xeGdU4G8C&amp;pg=PA3 |date=10 May 2012 |publisher=O'Reilly Media |isbn=978-1-4493-3877-0 |page=3}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=MIKE2.0, Big Data Definition |url=http://mike2.openmethodology.org/wiki/Big_Data_Definition}}&lt;/ref&gt; is the term for a collection of [[data set]]s so large and complex that it becomes difficult to process using on-hand database management tools or traditional data processing applications. The challenges include capture, curation, storage,&lt;ref&gt;{{cite web |author=Kusnetzky, Dan |title=What is &quot;Big Data?&quot; |publisher=ZDNet |url=http://blogs.zdnet.com/virtualization/?p=1708}}&lt;/ref&gt; search, sharing, transfer, analysis&lt;ref&gt;{{cite web |author=Vance, Ashley |title=Start-Up Goes After Big Data With Hadoop Helper |date=22 April 2010 |work=New York Times Blog |url=http://bits.blogs.nytimes.com/2010/04/22/start-up-goes-after-big-data-with-hadoop-helper/?dbk}}&lt;/ref&gt; and visualization. The trend to larger data sets is due to the additional information derivable from analysis of a single large set of related data, as compared to separate smaller sets with the same total amount of data, allowing correlations to be found to &quot;spot business trends, determine quality of research, prevent diseases, [[legal citation analysis|link legal citations]], combat crime, and determine real-time roadway traffic conditions.&quot;{{r|Economist}}&lt;ref name=&quot;BD-HB-R-01&quot;&gt;{{cite web|title=E-Discovery Special Report: The Rising Tide of Nonlinear Review|url=http://hudsonlegalblog.com/e-discovery/e-discovery-special-report-rising-tide-nonlinear-review.html|publisher=[[Hudson Global]]|accessdate=1 July 2012}} by Cat Casey and Alejandra Perez&lt;/ref&gt;&lt;ref name=&quot;BD-HB-R-02&quot;&gt;{{cite web|title=What Technology-Assisted Electronic Discovery Teaches Us About The Role Of Humans In Technology&amp;nbsp;— Re-Humanizing Technology-Assisted Review|url=http://www.forbes.com/sites/benkerschberg/2012/01/09/what-technology-assisted-electronic-discovery-teaches-us-about-the-role-of-humans-in-technology/|publisher=[[Forbes]]|accessdate=1 July 2012}}&lt;/ref&gt;

[[File:Viegas-UserActivityonWikipedia.gif|thumb|A visualization created by IBM of Wikipedia edits. At multiple [[terabyte]]s in size, the text and images of Wikipedia are a classic example of big data.]]
[[File:Hilbert InfoGrowth.png|thumb|Growth of and Digitization of Global Information Storage Capacity; source: http://www.martinhilbert.net/WorldInfoCapacity.html]]

{{As of|2012|}}, limits on the size of data sets that are feasible to process in a reasonable amount of time were on the order of [[exabytes]] of data.&lt;ref name=&quot;Ars Technica&quot;&gt;{{cite web|url=http://arstechnica.com/science/2012/04/future-telescope-array-drives-development-of-exabyte-processing/|title=Future telescope array drives development of exabyte processing|date=2012-04-02|accessdate=2012-10-24|author=Francis, Matthew}}&lt;/ref&gt; Scientists regularly encounter limitations due to large data sets in many areas, including [[meteorology]], [[genomics]],&lt;ref&gt;{{cite journal |title=Community cleverness required |journal=Nature |volume=455 |issue=7209 |page=1 |date=4 September 2008 |doi=10.1038/455001a |url=http://www.nature.com/nature/journal/v455/n7209/full/455001a.html}}&lt;/ref&gt; [[connectomics]], complex physics simulations,&lt;ref&gt;{{cite web |title=Sandia sees data management challenges spiral |date=4 August 2009 |work=HPC Projects |url=http://www.hpcprojects.com/news/news_story.php?news_id=922}}&lt;/ref&gt; and biological and environmental research.&lt;ref&gt;{{cite journal |last1=Reichman |first1=O.J. |last2=Jones |first2=M.B. |last3=Schildhauer |first3=M.P. |title=Challenges and Opportunities of Open Data in Ecology |journal=Science |volume=331 |issue=6018 |pages=703–5 |year=2011 |doi=10.1126/science.1197962 }}&lt;/ref&gt; The limitations also affect [[Web search engine|Internet search]], [[finance]] and [[business informatics]]. Data sets grow in size in part because they are increasingly being gathered by ubiquitous information-sensing mobile devices, aerial sensory technologies ([[remote sensing]]), software logs, cameras, microphones, [[radio-frequency identification]] readers, and [[wireless sensor networks]].&lt;ref&gt;{{cite web|title=Data Crush by Christopher Surdak|url=http://bookreviews.infoversant.com/data-crush-christopher-surdak/|accessdate=14 February 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web |author=Hellerstein, Joe |title=Parallel Programming in the Age of Big Data |date=9 November 2008 |work=Gigaom Blog |url=http://gigaom.com/2008/11/09/mapreduce-leads-the-way-for-parallel-programming/}}&lt;/ref&gt;&lt;ref&gt;{{cite book |first1=Toby |last1=Segaran |first2=Jeff |last2=Hammerbacher |title=Beautiful Data: The Stories Behind Elegant Data Solutions |url=http://books.google.com/books?id=zxNglqU1FKgC |year=2009 |publisher=O'Reilly Media |isbn=978-0-596-15711-1 |page=257}}&lt;/ref&gt; The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s;&lt;ref name=&quot;HilbertLopez2011&quot;&gt;{{harvnb|Hilbert|López|2011}}&lt;/ref&gt; {{As of|2012|lc=on}}, every day 2.5 [[exabytes]] (2.5×10&lt;sup&gt;18&lt;/sup&gt;) of data were created.&lt;ref&gt;{{cite web|url=http://www.ibm.com/big-data/us/en/ |title=IBM What is big data? — Bringing big data to the enterprise |publisher=www.ibm.com |accessdate=2013-08-26}}&lt;/ref&gt; The challenge for large enterprises is determining who should own big data initiatives that straddle the entire organization.&lt;ref&gt;Oracle and FSN, [http://www.fsn.co.uk/channel_bi_bpm_cpm/mastering_big_data_cfo_strategies_to_transform_insight_into_opportunity#.UO2Ac-TTuys &quot;Mastering Big Data: CFO Strategies to Transform Insight into Opportunity&quot;], December 2012&lt;/ref&gt;

Big data is difficult to work with using most [[relational database management system]]s and desktop statistics and visualization packages, requiring instead &quot;massively parallel software running on tens, hundreds, or even thousands of servers&quot;.&lt;ref&gt;{{cite web |author=Jacobs, A. |title=The Pathologies of Big Data |date=6 July 2009 |work=ACMQueue |url=http://queue.acm.org/detail.cfm?id=1563874}}&lt;/ref&gt; What is considered &quot;big data&quot; varies depending on the capabilities of the organization managing the set, and on the capabilities of the applications that are traditionally used to process and analyze the data set in its domain. &quot;For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.&quot;&lt;ref&gt;{{cite journal |last1=Magoulas |first1=Roger |last2=Lorica |first2=Ben |title=Introduction to Big Data |journal=Release 2.0 |issue=11 |date=February 2009 |url=http://radar.oreilly.com/r2/release2-0-11.html |publisher=O’Reilly Media |location=Sebastopol CA}}&lt;/ref&gt;

==Definition==
Big Data usually includes data sets with sizes beyond the ability of commonly used software tools to [[data acquisition|capture]], [[data curation|curate]], manage, and process the data within a tolerable elapsed time.&lt;ref name=&quot;Editorial&quot;&gt;Snijders, C., Matzat, U., &amp; Reips, U.-D. (2012). ‘Big Data’: Big gaps of knowledge in the field of Internet. ''International Journal of Internet Science, 7'', 1-5. http://www.ijis.net/ijis7_1/ijis7_1_editorial.html&lt;/ref&gt; Big data sizes are a constantly moving target, {{As of|2012|lc=on}} ranging from a few dozen terabytes to many [[petabyte]]s of data in a single data set.

In a 2001 research report&lt;ref&gt;{{cite web |first=Douglas |last=Laney |title=3D Data Management: Controlling Data Volume, Velocity and Variety |url=http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf |publisher=Gartner |accessdate = 6 February 2001}}&lt;/ref&gt; and related lectures, [[META Group]] (now [[Gartner]]) analyst Doug Laney defined data growth challenges and opportunities as being three-dimensional, i.e. increasing volume (amount of data), velocity (speed of data in and out), and variety (range of data types and sources). Gartner, and now much of the industry, continue to use this &quot;3Vs&quot; model for describing big data.&lt;ref&gt;{{cite web |last=Beyer |first=Mark |title=Gartner Says Solving 'Big Data' Challenge Involves More Than Just Managing Volumes of Data |url=http://www.gartner.com/it/page.jsp?id=1731916 |publisher=Gartner |accessdate = 13 July 2011| archiveurl= http://web.archive.org/web/20110710043533/http://www.gartner.com/it/page.jsp?id=1731916| archivedate= 10 July 2011 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt; In 2012, [[Gartner]] updated its definition as follows: &quot;Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization.&quot;&lt;ref&gt;{{cite web |first=Douglas |last=Laney |title=The Importance of 'Big Data': A Definition |url=http://www.gartner.com/resId=2057415 |publisher=Gartner |accessdate = 21 June 2012}}&lt;/ref&gt; Additionally, a new V &quot;Veracity&quot; is added by some organizations to describe it.&lt;ref&gt;{{cite web|title=What is Big Data?|url=http://www.villanovau.com/university-online-programs/what-is-big-data/|publisher=[[Villanova University]]}}&lt;/ref&gt;

If Gartner’s definition (the 3Vs) is still widely used, the growing maturity of the concept fosters a more sound difference between big data and [[Business Intelligence]], regarding data and their use:
* Business Intelligence uses [[descriptive statistics]] with data with high information density to measure things, detect trends etc.;
* Big data uses [[inductive statistics]] and concepts from  [[nonlinear system identification]] &lt;ref name=&quot;SAB1&quot;&gt;Billings S.A. &quot;Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains&quot;. Wiley, 2013&lt;/ref&gt;  to infer laws (regressions, nonlinear relationships, and causal effects) from large data sets &lt;ref&gt;Delort P., Big data Paris 2013 http://www.andsi.fr/tag/dsi-big-data/&lt;/ref&gt; to reveal relationships, dependencies, and to perform predictions of outcomes and behaviors.&lt;ref name=&quot;SAB1&quot; /&gt;&lt;ref&gt;Delort P., Big Data car Low-Density Data ? La faible densité en information comme facteur discriminant http://lecercle.lesechos.fr/entrepreneur/tendances-innovation/221169222/big-data-low-density-data-faible-densite-information-com&lt;/ref&gt;

===Big science===
The  [[Large Hadron Collider]] experiments represent about 150 million sensors delivering data 40 million times per second. There are nearly 600 million collisions per second. After filtering and refraining from recording more than 99.999% of these streams, there are 100 collisions of interest per second.&lt;ref&gt;{{cite web |title=LHC Brochure, English version. A presentation of the largest and the most powerful particle accelerator in the world, the Large Hadron Collider (LHC), which started up in 2008. Its role, characteristics, technologies, etc. are explained for the general public. |url=http://cds.cern.ch/record/1278169?ln=en |work=CERN-Brochure-2010-006-Eng. LHC Brochure, English version. |publisher=CERN |accessdate=20 January 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web |title=LHC Guide, English version. A collection of facts and figures about the Large Hadron Collider (LHC) in the form of questions and answers. |url=http://cds.cern.ch/record/1092437?ln=en |work=CERN-Brochure-2008-001-Eng. LHC Guide, English version. |publisher=CERN |accessdate=20 January 2013}}&lt;/ref&gt;&lt;ref name=&quot;nature&quot;&gt;{{Cite news |title=High-energy physics: Down the petabyte highway |work= Nature |date= 19 January 2011 |first=Geoff |last=Brumfiel |doi= 10.1038/469282a |volume= 469 |pages= 282–83 |url= http://www.nature.com/news/2011/110119/full/469282a.html }}&lt;/ref&gt;
* As a result, only working with less than 0.001% of the sensor stream data, the data flow from all four LHC experiments represents 25 petabytes annual rate before replication (as of 2012). This becomes nearly 200 petabytes after replication.
* If all sensor data were to be recorded in LHC, the data flow would be extremely hard to work with. The data flow would exceed 150 million petabytes annual rate, or nearly 500 [[exabytes]] per day, before replication. To put the number in perspective, this is equivalent to 500 [[quintillion]] (5×10&lt;sup&gt;20&lt;/sup&gt;) bytes per day, almost 200 times higher than all the other sources combined in the world.

===Science and research===
* When the [[Sloan Digital Sky Survey]] (SDSS) began collecting astronomical data in 2000, it amassed more in its first few weeks than all data collected in the history of astronomy. Continuing at a rate of about 200 GB per night, SDSS has amassed more than 140 terabytes of information. When the [[Large Synoptic Survey Telescope]], successor to SDSS, comes online in 2016 it is anticipated to acquire that amount of data every five days.{{r|Economist}}
* Decoding the [[Human Genome Project|human genome]] originally took 10 years to process, now it can be achieved in less than a week : the DNA sequencers have divided the sequencing cost by 10,000 in the last ten years, which is 100 times faster than the reduction in cost predicted by [[Moore's Law]].&lt;ref&gt;Delort P., OECD ICCP Technology Foresight Forum, 2012. http://www.oecd.org/sti/ieconomy/Session_3_Delort.pdf#page=6&lt;/ref&gt;
* Computational social science&amp;nbsp;— [[Tobias Preis]] ''et al.'' used [[Google Trends]] data to demonstrate that Internet users from countries with a higher per capita gross domestic product (GDP) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviour and real-world economic indicators.&lt;ref&gt;{{cite journal |first1=Tobias |last1=Preis |first2=Helen Susannah |last2=Moat, |first3=H. Eugene |last3=Stanley |first4=Steven R. |last4=Bishop |title=Quantifying the Advantage of Looking Forward |journal=Scientific Reports |volume= 2 |page=350 |year=2012 |doi=10.1038/srep00350 |pmid=22482034 |pmc=3320057}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.newscientist.com/article/dn21678-online-searches-for-future-linked-to-economic-success.html | title=Online searches for future linked to economic success |first=Paul |last=Marks |work=New Scientist | date=April 5, 2012 | accessdate=April 9, 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://arstechnica.com/gadgets/news/2012/04/google-trends-reveals-clues-about-the-mentality-of-richer-nations.ars | title=Google Trends reveals clues about the mentality of richer nations |first=Casey |last=Johnston |work=Ars Technica | date=April 6, 2012 | accessdate=April 9, 2012}}&lt;/ref&gt; The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year (‘2011’) to the volume of searches for the previous year (‘2009’), which they call the ‘[[future orientation index]]’.&lt;ref&gt;{{cite web | url = http://www.tobiaspreis.de/bigdata/future_orientation_index.pdf | title = Supplementary Information: The Future Orientation Index is available for download | author = Tobias Preis | date = 2012-05-24 | accessdate = 2012-05-24}}&lt;/ref&gt; They compared the future orientation index to the per capita GDP of each country and found a strong tendency for countries in which Google users enquire more about the future to exhibit a higher GDP. The results hint that there may potentially be a relationship between the economic success of a country and the information-seeking behavior of its citizens captured in big data.
* The [http://www.nasa.gov/centers/goddard/news/releases/2010/10-051.html [[Nasa|NASA]] Center for Climate Simulation (NCCS)] stores 32 petabytes of climate observations and simulations on the Discover supercomputing cluster.&lt;ref&gt;{{cite web|last=Webster|first=Phil|title=Supercomputing the Climate: NASA's Big Data Mission|url=http://www.csc.com/cscworld/publications/81769/81773-supercomputing_the_climate_nasa_s_big_data_mission|work=CSC World|publisher=Computer Sciences Corporation|accessdate=2013-01-18}}&lt;/ref&gt;
* [[Tobias Preis]] and his colleagues [[Helen Susannah Moat]] and [[H. Eugene Stanley]] introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends.&lt;ref&gt;{{cite web | url=http://www.nature.com/news/counting-google-searches-predicts-market-movements-1.12879 | title=Counting Google searches predicts market movements | author=[[Philip Ball]] | work=Nature | date=April 26, 2013 | accessdate=August 9, 2013}}&lt;/ref&gt; Their analysis of [[Google]] search volume for 98 terms of varying financial relevance, published in ''[[Scientific Reports]]'',&lt;ref&gt;{{cite journal | author=Tobias Preis, Helen Susannah Moat and H. Eugene Stanley | title=Quantifying Trading Behavior in Financial Markets Using Google Trends | journal=[[Scientific Reports]] | volume= 3 | pages=1684 | year=2013 | accessdate=August 9, 2013 | doi=10.1038/srep01684}}&lt;/ref&gt; suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.&lt;ref&gt;{{cite web | url=http://bits.blogs.nytimes.com/2013/04/26/google-search-terms-can-predict-stock-market-study-finds/ | title= Google Search Terms Can Predict Stock Market, Study Finds | author=Nick Bilton | work=[[New York Times]] | date=April 26, 2013 | accessdate=August 9, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://business.time.com/2013/04/26/trouble-with-your-investment-portfolio-google-it/ | title=Trouble With Your Investment Portfolio? Google It! | author=Christopher Matthews | work=[[TIME Magazine]] | date=April 26, 2013 | accessdate=August 9, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url= http://www.nature.com/news/counting-google-searches-predicts-market-movements-1.12879 | title=Counting Google searches predicts market movements | author=Philip Ball | work=[[Nature]] | date=April 26, 2013 | accessdate=August 9, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.businessweek.com/articles/2013-04-25/big-data-researchers-turn-to-google-to-beat-the-markets | title='Big Data' Researchers Turn to Google to Beat the Markets | author=Bernhard Warner | work=[[Bloomberg Businessweek]] | date=April 25, 2013 | accessdate=August 9, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.independent.co.uk/news/business/comment/hamish-mcrae/hamish-mcrae-need-a-valuable-handle-on-investor-sentiment-google-it-8590991.html | title=Hamish McRae: Need a valuable handle on investor sentiment? Google it | author=Hamish McRae | work=[[The Independent]] | date=April 28, 2013 | accessdate=August 9, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.ft.com/intl/cms/s/0/e5d959b8-acf2-11e2-b27f-00144feabdc0.html | title= Google search proves to be new word in stock market prediction | author=Richard Waters | work=[[Financial Times]] | date=April 25, 2013 | accessdate=August 9, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.forbes.com/sites/davidleinweber/2013/04/26/big-data-gets-bigger-now-google-trends-can-predict-the-market/ | title=Big Data Gets Bigger: Now Google Trends Can Predict The Market | author=David Leinweber | work=[[Forbes]] | date=April 26, 2013 | accessdate=August 9, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.bbc.co.uk/news/science-environment-22293693 | title=Google searches predict market moves | author=Jason Palmer | work=[[BBC]] | date=April 25, 2013 | accessdate=August 9, 2013}}&lt;/ref&gt;

===Government===
* In 2012, the [[Presidency of Barack Obama|Obama administration]] announced the Big Data Research and Development Initiative, which explored how big data could be used to address important problems faced by the government.&lt;ref name=WH_Big_Data&gt;{{cite web|last=Kalil|first=Tom|title=Big Data is a Big Deal|url=http://www.whitehouse.gov/blog/2012/03/29/big-data-big-deal|publisher=White House|accessdate=26 September 2012}}&lt;/ref&gt; The initiative was composed of 84 different big data programs spread across six departments.&lt;ref&gt;{{cite web|last=Executive Office of the President|title=Big Data Across the Federal Government|url=http://www.whitehouse.gov/sites/default/files/microsites/ostp/big_data_fact_sheet_final_1.pdf|publisher=White House|accessdate=26 September 2012 |date=March 2012}}&lt;/ref&gt;
* Big data analysis played a large role in [[Barack Obama]]'s successful [[Barack Obama presidential campaign, 2012|2012 re-election campaign]].&lt;ref&gt;{{cite web|url=http://bosmol.com/2013/02/how-big-data-analysis-helped-president-obama-defeat-romney-in-2012-elections.html#.USxGtjDm4cw|title=How big data analysis helped President Obama defeat Romney in 2012 Elections|date=8 February 2013|publisher=Bosmol Social Media News|accessdate=9 March 2013}}&lt;/ref&gt;
* The [[United States Federal Government]] owns six of the ten most powerful supercomputers in the world.&lt;ref&gt;{{cite web |last=Hoover |first=J. Nicholas |title=Government's 10 Most Powerful Supercomputers |url=http://www.informationweek.com/government/enterprise-applications/image-gallery-governments-10-most-powerf/224700271 |work=Information Week |publisher=UBM |accessdate=26 September 2012}}&lt;/ref&gt;
* The [[Utah Data Center]] is a data center currently being constructed by the [[United States]] [[National Security Agency]]. When finished, the facility will be able to handle a large amount of information collected by the NSA over the Internet. The exact amount of storage space is unknown, but more recent sources claim it will be on the order of a few [[Exabytes]].&lt;ref&gt;{{cite web | last=Bamford|first=James|title=The NSA Is Building the Country’s Biggest Spy Center (Watch What You Say)|url=http://www.wired.com/threatlevel/2012/03/ff_nsadatacenter/all/1|work=Wired Magazine|accessdate=2013-03-18}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.nsa.gov/public_info/press_room/2011/utah_groundbreaking_ceremony.shtml|title=Groundbreaking Ceremony Held for $1.2 Billion Utah Data Center|publisher=National Security Agency Central Security Service|accessdate=2013-03-18}}&lt;/ref&gt;&lt;ref&gt;{{cite web | last=Hill|first=Kashmir|title=TBlueprints Of NSA's Ridiculously Expensive Data Center In Utah Suggest It Holds Less Info Than Thought|url=http://www.forbes.com/sites/kashmirhill/2013/07/24/blueprints-of-nsa-data-center-in-utah-suggest-its-storage-capacity-is-less-impressive-than-thought/|work=Forbes|accessdate=2013-10-31}}&lt;/ref&gt;

===Private sector===
[[File:2013-09-11 Bus wrapped with SAP Big Data parked outside IDF13 (9730051783).jpg|thumb|Bus wrapped with [[SAP AG|SAP]] Big data parked outside [[Intel Developer Forum|IDF13]].]]
* [[eBay.com]] uses two data warehouses at 7.5 petabytes and 40PB as well as a 40PB Hadoop cluster for search, consumer recommendations, and merchandising.  [http://www.itnews.com.au/News/342615,inside-ebay8217s-90pb-data-warehouse.aspx Inside eBay’s 90PB data warehouse]
* [[Amazon.com]] handles millions of back-end operations every day, as well as queries from more than half a million third-party sellers. The core technology that keeps Amazon running is Linux-based and as of 2005 they had the world’s three largest Linux databases, with capacities of 7.8 TB, 18.5 TB, and 24.7 TB.&lt;ref&gt;{{cite web|last=Layton |first=Julia |url=http://money.howstuffworks.com/amazon1.htm |title=Amazon Technology |publisher=Money.howstuffworks.com |date= |accessdate=2013-03-05}}&lt;/ref&gt;
* [[Walmart]] handles more than 1 million customer transactions every hour, which is imported into databases estimated to contain more than 2.5 petabytes (2560 terabytes) of data&amp;nbsp;– the equivalent of 167 times the information contained in all the books in the US [[Library of Congress]].{{r|Economist}}
* Facebook handles 50 billion photos from its user base.&lt;ref&gt;{{cite web|url=https://www.facebook.com/notes/facebook-engineering/scaling-facebook-to-500-million-users-and-beyond/409881258919 |title=Scaling Facebook to 500 Million Users and Beyond |publisher=Facebook.com |date= |accessdate=2013-07-21}}&lt;/ref&gt;
* [[FICO]] Falcon Credit Card Fraud Detection System protects 2.1 billion active accounts world-wide.&lt;ref name=&quot;fico.com&quot;&gt;{{cite web|url=http://www.fico.com/en/Products/DMApps/Pages/FICO-Falcon-Fraud-Manager.aspx |title=FICO® Falcon® Fraud Manager |publisher=Fico.com |date= |accessdate=2013-07-21}}&lt;/ref&gt;
* The volume of business data worldwide, across all companies, doubles every 1.2 years, according to estimates.&lt;ref name=&quot;KnowWPCarey.com&quot;&gt;{{cite web|url=http://knowwpcarey.com/article.cfm?cid=25&amp;aid=1171 |title=eBay Study: How to Build Trust and Improve the Shopping Experience |publisher=Knowwpcarey.com |date=2012-05-08 |accessdate=2013-03-05}}&lt;/ref&gt;&lt;ref&gt;[http://www.statista.com/statistics/280444/global-leading-priorities-for-big-data-according-to-business-and-it-executives/ Leading Priorities for Big Data for Business and IT]. eMarketer. October 2013. Retrieved January 2014.&lt;/ref&gt;
* [[Windermere Real Estate]] uses anonymous GPS signals from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day.&lt;ref&gt;{{cite web|last=Wingfield |first=Nick |url=http://bits.blogs.nytimes.com/2013/03/12/predicting-commutes-more-accurately-for-would-be-home-buyers/ |title=Predicting Commutes More Accurately for Would-Be Home Buyers - NYTimes.com |publisher=Bits.blogs.nytimes.com |date=2013-03-12 |accessdate=2013-07-21}}&lt;/ref&gt;

===International development===
Research on the effective usage of [[information and communication technologies for development]] (also known as [[ICT4D]]) suggests that big data technology can make important contributions but also present unique challenges to [[International development]].&lt;ref&gt;UN GLobal Pulse (2012). Big Data for Development: Opportunities and Challenges (White p. by Letouzé, E.). New York: United Nations. Retrieved from http://www.unglobalpulse.org/projects/BigDataforDevelopment&lt;/ref&gt;&lt;ref&gt;WEF (World Economic Forum), &amp; Vital Wave Consulting. (2012). Big Data, Big Impact: New Possibilities for International Development. World Economic Forum. Retrieved August 24, 2012, from http://www.weforum.org/reports/big-data-big-impact-new-possibilities-international-development&lt;/ref&gt; Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as [[health care]], [[employment]], [[economic productivity]], crime, security, and [[natural disaster]] and resource management.&lt;ref name=&quot;HilbertBigData2013&quot;/&gt;&lt;ref&gt;{{cite web|url=http://blogs.worldbank.org/ic4d/four-ways-to-talk-about-big-data/|title=Elena Kvochko, Four Ways To talk About Big Data (Information Communication Technologies for Development Series)|publisher=worldbank.org|accessdate=2012-05-30}}&lt;/ref&gt; However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues.&lt;ref name=&quot;HilbertBigData2013&quot;/&gt;

==Market==
&quot;Big Data&quot; has increased the demand of information management specialists in that [[Software AG]], [[Oracle Corporation]], [[IBM]], [[Microsoft]], [[SAP AG|SAP]], [[EMC Corporation|EMC]], [[HP]] and [[Dell]] have spent more than $15 billion on software firms only specializing in data management and analytics. In 2010, this industry on its own was worth more than $100 billion and was growing at almost 10&amp;nbsp;percent a year: about twice as fast as the software business as a whole.{{r|Economist}}

Developed economies make increasing use of data-intensive technologies. There are 4.6 billion mobile-phone subscriptions worldwide and there are between 1 billion and 2 billion people accessing the internet.{{r|Economist}} Between 1990 and 2005, more than 1 billion people worldwide entered the middle class which means more and more people who gain money will become more literate which in turn leads to information growth. The world's effective capacity to exchange information through [[telecommunication]] networks was 281 [[petabytes]] in 1986, 471 [[petabytes]] in 1993, 2.2 [[exabytes]] in 2000, 65 [[exabytes]] in 2007&lt;ref name=&quot;HilbertLopez2011&quot;/&gt; and it is predicted that the amount of traffic flowing over the internet will reach 667 [[exabyte]]s annually by 2014.{{r|Economist}} It is estimated that one third of the globally stored information is in the form of alphanumeric text and still image data,&lt;ref name=&quot;HilbertContent&quot;&gt;[http://www.tandfonline.com/doi/abs/10.1080/01972243.2013.873748 &quot;What Is the Content of the World's Technologically Mediated Information and Communication Capacity: How Much Text, Image, Audio, and Video?&quot;], Martin Hilbert (2014), [[The Information Society]]; free access to the article through this link: martinhilbert.net/WhatsTheContent_Hilbert.pdf&lt;/ref&gt; which is the format most useful for most big data applications. This also shows the potential of yet unused data (i.e. in the form of video and audio content).

'''Big Data Softwares:'''
* [[Hadoop]] - Apache Foundation
* [[MongoDB]] - MongoDB, Inc
* [[Splunk]] - Splunk Inc

==Architecture==
In 2004, Google published a paper on a process called [[MapReduce]] that used such an architecture. The MapReduce framework provides a parallel processing model and associated implementation to process huge amount of data.  With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the Map step). The results are then gathered and delivered (the Reduce step). The framework was very successful,&lt;ref&gt;Bertolucci, Jeff [http://www.informationweek.com/big-data/news/software-platforms/hadoop-from-experiment-to-leading-big-d/240157176 &quot;Hadoop: From Experiment To Leading Big Data Platform&quot;], &quot;Information Week&quot;, 2013. Retrieved on 14 November 2013.&lt;/ref&gt; so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open source project named [[Apache Hadoop|Hadoop]].&lt;ref&gt;Webster, John. [http://research.google.com/archive/mapreduce-osdi04.pdf &quot;MapReduce: Simplified Data Processing on Large Clusters&quot;], &quot;Search Storage&quot;, 2004. Retrieved on 25 March 2013.&lt;/ref&gt;

[[MIKE2.0 Methodology|MIKE2.0]] is an open approach to information management that acknowledges the need for revisions due to big data implications in an article titled &quot;Big Data Solution Offering&quot;.&lt;ref&gt;{{cite web|url=http://mike2.openmethodology.org/wiki/Big_Data_Solution_Offering|title=Big Data Solution Offering|publisher=MIKE2.0|accessdate=8 Dec 2013}}&lt;/ref&gt; The methodology addresses handling big data in terms of useful [[permutation]]s of data sources, [[complexity]] in interrelationships, and difficulty in deleting (or modifying) individual records.&lt;ref&gt;{{cite web|url=http://mike2.openmethodology.org/wiki/Big_Data_Definition|title=Big Data Definition|publisher=MIKE2.0|accessdate=9 March 2013}}&lt;/ref&gt;

Recent studies show that the use of a multiple layer architecture is an option for dealing with Big Data. The Distributed Parallel architecture distributes data across multiple processing units and parallel processing units provide data much faster, by improving processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end user by using a front end application server.&lt;ref&gt;{{cite journal|last=Boja|first=C|coauthors=Pocovnicu, A; Bătăgan, L.|title=Distributed Parallel Architecture for Big Data|journal=Informatica Economica|year=2012|volume=16|issue=2|pages=116–127|accessdate=2013-11-25}}&lt;/ref&gt;

==Technologies==

Big data requires exceptional technologies to efficiently process large quantities of data within tolerable elapsed times. A 2011 [[McKinsey &amp; Company|McKinsey]] report&lt;ref&gt;
{{cite book
 | last1 = Manyika
 | first1 = James
 | first2=Michael |last2=Chui |first3=Jaques |last3=Bughin |first4=Brad |last4=Brown |first5=Richard |last5=Dobbs |first6=Charles |last6=Roxburgh |first7=Angela Hung |last7=Byers
 | title = Big Data: The next frontier for innovation, competition, and productivity
 | publisher = McKinsey Global Institute
 | date = May 2011
 | url = http://www.mckinsey.com/Insights/MGI/Research/Technology_and_Innovation/Big_data_The_next_frontier_for_innovation
}}
&lt;/ref&gt;
suggests suitable technologies include [[A/B testing]],
[[crowdsourcing]],
[[data fusion]] and [[data integration|integration]],
[[genetic algorithms]],
[[machine learning]],
[[natural language processing]],
[[signal processing]],
[[simulation]],
[[time series analysis]] and
[[Visualization (computer graphics)|visualisation]].
Multidimensional big data can also be represented as [[tensor]]s, which can be more efficiently handled by tensor-based computation,&lt;ref&gt;{{cite web |title=Future Directions in Tensor-Based Computation and Modeling |date=May 2009|url=http://www.cs.cornell.edu/cv/tenwork/finalreport.pdf}}&lt;/ref&gt; such as [[multilinear subspace learning]].&lt;ref name=&quot;MSLsurvey&quot;&gt;{{cite journal
 |first=Haiping |last=Lu
 |first2=K.N. |last2=Plataniotis
 |first3=A.N. |last3=Venetsanopoulos
 |url=http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf
 |title=A Survey of Multilinear Subspace Learning for Tensor Data
 |journal=Pattern Recognition
 |volume=44 |number=7 |pages=1540–1551 |year=2011
 |doi=10.1016/j.patcog.2011.01.004
}}&lt;/ref&gt; Additional technologies being applied to big data include massively parallel-processing ([[Massive parallel processing|MPP]]) databases, [[search-based application]]s, data-mining grids, distributed file systems, distributed databases, cloud based infrastructure (applications, storage and computing resources) and the Internet.{{Citation needed|date=September 2011}}

Some but not all [[Massive parallel processing|MPP]] relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the [[RDBMS]].&lt;ref&gt;{{cite web |author=Monash, Curt |title=eBay’s two enormous data warehouses |date=30 April 2009 |url=http://www.dbms2.com/2009/04/30/ebays-two-enormous-data-warehouses/}}&lt;br/&gt;{{cite web |author=Monash, Curt |title=eBay followup&amp;nbsp;— Greenplum out, Teradata &gt; 10 petabytes, Hadoop has some value, and more |date=6 October 2010 |url=http://www.dbms2.com/2010/10/06/ebay-followup-greenplum-out-teradata-10-petabytes-hadoop-has-some-value-and-more/}}&lt;/ref&gt;

[[DARPA]]’s [[Topological Data Analysis]] program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called [[Ayasdi]].&lt;ref&gt;{{cite web|url=http://www.ayasdi.com/resources/|title=Resources on how Topological Data Analysis is used to analyze big data|publisher=Ayasdi}}&lt;/ref&gt;

The practitioners of big data analytics processes are generally hostile to slower shared storage,&lt;ref&gt;{{cite web |title=Storage area networks need not apply |author=CNET News |date=April 1, 2011 |url=http://news.cnet.com/8301-21546_3-20049693-10253464.html}}&lt;/ref&gt; preferring direct-attached storage ([[Direct-attached storage|DAS]]) in its various forms from solid state drive ([[Ssd|SSD]]) to high capacity [[Serial ATA|SATA]] disk buried inside parallel processing nodes. The perception of shared storage architectures—[[Storage area network]] (SAN) and [[Network-attached storage]] (NAS) —is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.

Real or near-real time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in memory is good—data on spinning disk at the other end of a [[Fiber connector|FC]] [[Storage area network|SAN]] connection is not. The cost of a [[Storage area network|SAN]] at the scale needed for analytics applications is very much higher than other storage techniques.

There are advantages as well as disadvantages to shared storage in big data analytics, but big data analytics practitioners {{As of|2011|lc=on}} did not favour it.&lt;ref&gt;{{cite web |title=How New Analytic Systems will Impact Storage |date=September 2011 |url=http://www.evaluatorgroup.com/document/big-data-how-new-analytic-systems-will-impact-storage-2/}}&lt;/ref&gt;

==Research activities==
Encrypted search &amp; cluster formation in Big Data is set for the demonstration in March 2014 at American Society of Engineering Education. Gautam Siwach engaged at Tackling the challenges of Big Data by [[MIT Computer Science and Artificial Intelligence Laboratory]] and Dr. Amir Esmailpour at UNH Research Group investigated the key features of big data as formation of clusters and their interconnections. They focused on the security of big data and the actual orientation of the term towards the presence of different type of data in an encrypted form at cloud interface by providing the raw definitions and real time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.&lt;ref&gt;http://ubconferences.org/ March 2014.&lt;/ref&gt;

In March 2012, The White House announced a national &quot;Big Data Initiative&quot; that consisted of six Federal departments and agencies committing more than $200 million to big data research projects.&lt;ref&gt;{{cite web |title=Obama Administration Unveils &quot;Big Data&quot; Initiative:Announces $200 Million In New R&amp;D Investments|publisher=The White House |url=http://www.whitehouse.gov/sites/default/files/microsites/ostp/big_data_press_release_final_2.pdf}}&lt;/ref&gt;

The initiative included a National Science Foundation &quot;Expeditions in Computing&quot; grant of $10 million over 5 years to the AMPLab&lt;ref&gt;{{cite web|url=http://amplab.cs.berkeley.edu |title=AMPLab at the University of California, Berkeley |publisher=Amplab.cs.berkeley.edu |accessdate=2013-03-05}}&lt;/ref&gt; at the University of California, Berkeley.&lt;ref&gt;{{cite web |title=NSF Leads Federal Efforts In Big Data|date=29 March 2012|publisher=National Science Foundation (NSF) |url=http://www.nsf.gov/news/news_summ.jsp?cntn_id=123607&amp;org=NSF&amp;from=news}}&lt;/ref&gt; The AMPLab also received funds from [[DARPA]], and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion&lt;ref&gt;{{cite conference|url=https://amplab.cs.berkeley.edu/publication/scaling-the-mobile-millennium-system-in-the-cloud-2/|author1=Timothy Hunter|date=October 2011|author2=Teodor Moldovan|author3=Matei Zaharia|author4=Justin Ma|author5=Michael Franklin|author6=Pieter Abbeel|author7=Alexandre Bayen|title=Scaling the Mobile Millennium System in the Cloud}}&lt;/ref&gt; to fighting cancer.&lt;ref&gt;{{cite news|title=Computer Scientists May Have What It Takes to Help Cure Cancer|author=David Patterson|publisher=The New York Times|date=5 December 2011|url=http://www.nytimes.com/2011/12/06/science/david-patterson-enlist-computer-scientists-in-cancer-fight.html?_r=0}}&lt;/ref&gt;

The White House Big Data Initiative also included a commitment by the  Department of Energy to provide $25 million in funding over 5 years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute,&lt;ref&gt;{{cite web|title=Secretary Chu Announces New Institute to Help Scientists Improve Massive Data Set Research on DOE Supercomputers |publisher=&quot;energy.gov&quot; |url=http://energy.gov/articles/secretary-chu-announces-new-institute-help-scientists-improve-massive-data-set-research-doe}}&lt;/ref&gt; led by the Energy Department’s [[Lawrence Berkeley National Laboratory]]. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the Department’s supercomputers.

The U.S. state of [[Massachusetts]] announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions.&lt;ref&gt;{{cite web |title=Governor Patrick announces new initiative to strengthen Massachusetts’ position as a World leader in Big Data |publisher=Commonwealth of Massachusetts |url=http://www.mass.gov/governor/pressoffice/pressreleases/2012/2012530-governor-announces-big-data-initiative.html}}&lt;/ref&gt;  The [[Massachusetts Institute of Technology]] hosts the Intel Science and Technology Center for Big Data in the [[MIT Computer Science and Artificial Intelligence Laboratory]], combining government, corporate, and institutional funding and research efforts.&lt;ref&gt;{{cite web|url=http://bigdata.csail.mit.edu/ |title=Big Data @ CSAIL |publisher=Bigdata.csail.mit.edu |date=2013-02-22 |accessdate=2013-03-05}}&lt;/ref&gt;

The European Commission is funding a 2-year-long [http://big-project.eu Big Data Public Private Forum] through their [[Seventh Framework Program]] to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the Big Data economy. Outcomes of this project will be used as input for [http://ec.europa.eu/research/horizon2020/index_en.cfm?pg=h2020 Horizon 2020], their next [[Framework Programmes for Research and Technological Development|framework program]].&lt;ref&gt;{{cite web|url=http://cordis.europa.eu/search/index.cfm?fuseaction=proj.document&amp;PJ_RCN=13267529 |title=Big Data Public Private Forum |publisher=Cordis.europa.eu |date=2012-09-01 |accessdate=2013-03-05}}&lt;/ref&gt;

The British government announced in March 2014 the founding of the [[Alan Turing]] Institute, named after the computer pioneer and code-breaker, which will focus on new ways of collecting and analysing large sets of data.&lt;ref&gt;{{cite web|url=http://www.bbc.co.uk/news/technology-26651179|title=Alan Turing Institute to be set up to research big data|publisher=[[BBC News]]|accessdate=2014-03-19}}&lt;/ref&gt;

The IBM sponsored 37th annual &quot;Battle of the Brains&quot; student Big Data championship will be held in July 2013.&lt;ref&gt;{{cite web|url=http://battleofthebrains.podbean.com/about/ |title=About &amp;#124; Battle of the Brains |publisher=Battleofthebrains.podbean.com |accessdate=2013-07-21}}&lt;/ref&gt; 
The inaugural professional 2014 Big Data World Championship is to be held in Dallas, Texas.&lt;ref&gt;{{cite web|url=http://www.texata.com |title=Big Data World Championships |publisher=Texata |accessdate=2013-07-21}}&lt;/ref&gt;

In order to make manufacturing more competitive in the United States (and globe), there is a need to integrate more American ingenuity and innovation into manufacturing ; Therefore, National Science Foundation has granted the Industry University cooperative research [http://www.imscenter.net center for Intelligent Maintenance Systems (IMS)] at [[university of Cincinnati]] to focus on developing advanced predictive tools and techniques to be applicable in Big Data environment.&lt;ref name=SMELettersPaper&gt;{{cite web|title=Center for Intelligent Maintenance Systems (IMS Center)|url=http://imscenter.net}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Lee|first=Jay|coauthors=Lapira, Edzel; Bagheri, Behrad; Kao, Hung-An|title=Recent Advances and Trends in Predictive Manufacturing Systems in Big Data Environment|journal=Manufacturing Letters|year=2013|volume=1|issue=1}}&lt;/ref&gt;  In May 2013, IMS Center held an industry advisory board meeting focusing on Big Data where presenters from various industrial companies discussed their concerns, issues and future goals in Big Data environment.

==Applications==

===Manufacturing===
Based on TCS 2013 Global Trend Study, huge improvements in supply planning and boost product quality is the greatest benefit of Big Data for manufacturing.&lt;ref&gt;{{cite web|url=http://sites.tcs.com/big-data-study/manufacturing-big-data-benefits-challenges/#}}&lt;/ref&gt; Big Data provides an infrastructure for transparency in manufacturing industry, which is the ability to unravel uncertainties such as inconsistent component performance and availability. Predictive manufacturing as an applicable approach toward near-zero downtime and transparency requires vast amount of data and advanced prediction tools for a systematic process of data into useful information.&lt;ref&gt;{{cite journal|last=Lee|first=Jay|coauthors=Wu, F.; Zhao, W.; Ghaffari, M.; Liao, L|title=Prognostics and health management design for rotary machinery systems—Reviews, methodology and applications|journal=Mechanical Systems and Signal Processing|date=Jan 2013|volume=42|issue=1}}&lt;/ref&gt; A conceptual framework of predictive manufacturing begins with data acquisition where different type of sensory data is available to acquire such as acoustics, vibration, pressure, current, voltage and controller data. Vast amount of sensory data in addition to historical data construct the “Big Data” in manufacturing. The generated Big Data acts as the input into predictive tools and preventive strategies such as [[Prognostics|Prognostics and Health Management]] (PHM).&lt;ref name=SMELettersPaper /&gt;

==Critique==
Critiques of the big data paradigm come in two flavors, those that question the implications of the approach itself, and those that question the way it is currently done.
[[File:Big data cartoon t gregorius.jpg|thumb|Cartoon critical of big data application]]

===Critiques of the big data paradigm===
&quot;A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data&quot;.&lt;ref name=&quot;Editorial&quot;/&gt; In their critique, Snijders, Matzat, and [[Ulf-Dietrich Reips|Reips]] point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. Mark Graham has leveled broad critiques at [[Chris Anderson (writer)|Chris Anderson]]'s assertion that big data will spell the end of theory: focusing in particular on the notion that big data will always need to be contextualized in their social, economic and political contexts.&lt;ref&gt;{{cite news |author=Graham M. |title=Big data and the end of theory? |newspaper=The Guardian |year=2012 |url=http://www.guardian.co.uk/news/datablog/2012/mar/09/big-data-theory}}&lt;/ref&gt; Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. To overcome this insight deficit, &quot;big data&quot;, no matter how comprehensive or well analyzed, needs to be complemented by &quot;big judgment&quot;, according to an article in the Harvard Business Review.&lt;ref&gt;{{cite web|title=Good Data Won't Guarantee Good Decisions. Harvard Business Review|url=http://hbr.org/2012/04/good-data-wont-guarantee-good-decisions/ar/1|work=Shah, Shvetank; Horne, Andrew; Capellá, Jaime;|publisher=HBR.org|accessdate=8 September 2012}}&lt;/ref&gt;

Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably &quot;informed by the world as it was in the past, or, at best, as it currently is&quot;.&lt;ref name=&quot;HilbertBigData2013&quot;&gt;[http://papers.ssrn.com/abstract=2205145 &quot;Big Data for Development: From Information- to Knowledge Societies&quot;], Martin Hilbert (2013), SSRN Scholarly Paper No. ID 2205145). Rochester, NY: Social Science Research Network; http://papers.ssrn.com/abstract=2205145&lt;/ref&gt;  Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past. If the systems dynamics of the future change, the past can say little about the future. For this, it would be necessary to have a thorough understanding of the systems dynamic, which implies theory.&lt;ref&gt;Anderson, C. (2008, June 23). The End of Theory: The Data Deluge Makes the Scientific Method Obsolete. Wired Magazine, (Science: Discoveries). http://www.wired.com/science/discoveries/magazine/16-07/pb_theory&lt;/ref&gt;  As a response to this critique it has been suggested to combine big data approaches with computer simulations, such as [[agent-based model]]s, for example.&lt;ref name=&quot;HilbertBigData2013&quot;/&gt; [[Agent-based models]] are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms.&lt;ref&gt;Rauch, J. (2002). Seeing Around Corners. The Atlantic, (April), 35–48. http://www.theatlantic.com/magazine/archive/2002/04/seeing-around-corners/302471/&lt;/ref&gt;&lt;ref&gt;Epstein, J. M., &amp; Axtell, R. L. (1996). Growing Artificial Societies: Social Science from the Bottom Up. A Bradford Book.&lt;/ref&gt; In addition, use of multivariate methods that probe for the latent structure of the data, such as [[factor analysis]] and [[cluster analysis]], have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.

In Health and biology, conventional scientific approaches are based on experimentation. For these approaches, the limiting factor are the relevant data that can confirm or refute the initial hypothesis.&lt;ref&gt;Delort P., Big data in Biosciences, Big Data Paris, 2012 http://www.bigdataparis.com/documents/Pierre-Delort-INSERM.pdf#page=5&lt;/ref&gt;
A new postulate is accepted now in biosciences : the information provided by the data in huge volumes ([[omics]]) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation.
In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor. The search logic is reversed and the limits of induction (&quot;Glory of Science and Philosophy scandal&quot;, [[C. D. Broad]], 1926) to be considered.

[[Consumer privacy|Privacy]] advocates are concerned about the threat to privacy represented by increasing storage and integration of [[personally identifiable information]]; expert panels have released various policy recommendations to conform practice to expectations of privacy.&lt;ref&gt;{{cite web |first=Paul |last=Ohm |title=Don't Build a Database of Ruin |publisher=Harvard Business Review |url=http://blogs.hbr.org/cs/2012/08/dont_build_a_database_of_ruin.html}}&lt;/ref&gt;&lt;ref&gt;Darwin Bond-Graham, ''[http://www.counterpunch.org/2013/12/03/iron-cagebook/ Iron Cagebook - The Logical End of Facebook's Patents],'' [[Counterpunch.org]], 2013.12.03&lt;/ref&gt;&lt;ref&gt;Darwin Bond-Graham, ''[http://www.counterpunch.org/2013/09/11/inside-the-tech-industrys-startup-conference/  Inside the Tech industry’s Startup Conference],'' [[Counterpunch.org]], 2013.09.11&lt;/ref&gt;

===Critiques of big data execution===
&lt;!-- Please stop edit warring, and go to the discussion page on the proper capitalization of Danah boyd. Thank you! Wikipedia uses *English language*, and the first word in a sentence is capitalized. --&gt;Researcher [[Danah Boyd]]&lt;!-- Read the previous comment. Sentences start with uppercase in English. Go to the discussion page before changing! --&gt; has raised concerns about the use of big data in [[science]] neglecting principles such as choosing a [[Sampling (statistics)|representative sample]] by being too concerned about actually handling the huge amounts of data.&lt;ref name=&quot;danah&quot;&gt;{{cite web | url=http://www.danah.org/papers/talks/2010/WWW2010.html | title=Privacy and Publicity in the Context of Big Data | author=[[danah boyd]] | work=[[World Wide Web Conference|WWW 2010 conference]] | date=2010-04-29 | accessdate = 2011-04-18}}&lt;/ref&gt; This approach may lead to results [[Bias (statistics)|bias]] in one way or another. Integration across heterogeneous data resources&amp;nbsp;— some that might be considered &quot;big data&quot; and others not&amp;nbsp;— presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.&lt;ref&gt;{{cite journal |last1=Jones |first1=MB |last2=Schildhauer |first2=MP |last3=Reichman |first3=OJ |last4=Bowers |first4=S |title=The New Bioinformatics: Integrating Ecological Data from the Gene to the Biosphere |journal=Annual Review of Ecology, Evolution, and Systematics |volume=37 |issue=1 |pages=519–544 |year=2006 |doi=10.1146/annurev.ecolsys.37.091305.110031 |url=http://www.pnamp.org/sites/default/files/Jones2006_AREES.pdf |format=PDF}}&lt;/ref&gt;

==See also==
{{portal|Information technology}}
* [[Data Defined Storage]]
* [[Nonlinear system identification]]
* [[Operations research]]
* [[Supercomputer]]
* [[Tuple space]]
* [[Unstructured data]]
* [[MapReduce]]
* [[Apache Hadoop]]
* [[Apache Accumulo]]
* [[Sqrrl]]
* [[Programming with Big Data in R]]
* [[Big Media]]
* [[Big Oil]]
* [[Big Pharma]]
* [[Big Tobacco]]
{{Div col end}}

==References==
{{Reflist|2|refs=

&lt;ref name=&quot;Economist&quot;&gt;
{{cite news |title=Data, data everywhere |url=http://www.economist.com/node/15557443 |newspaper=The Economist |date=25 February 2010 |accessdate=9 December 2012}}
&lt;/ref&gt;

}}

==External links==
{{Commons category|Big data}}
{{Wiktionary|big data}}

==Further reading==
* [http://arxiv.org/abs/1312.4722 Big Data Computing and Clouds: Challenges, Solutions, and Future Directions]. Marcos D. Assuncao, Rodrigo N. Calheiros, Silvia Bianchi, Marco A. S. Netto, Rajkumar Buyya. Technical Report CLOUDS-TR-2013-1, Cloud Computing and Distributed Systems Laboratory, The University of Melbourne, Dec. 17, 2013. 

* Gautam Siwach and Dr. Amir Esmailpour - Encrypted search &amp; cluster formation in Big Data |publisher ASEE 
* {{cite web|url=http://www.odbms.org/download/BigDataforGood.pdf|title=Big Data for Good|publisher=ODBMS.org|date=June 5, 2012|accessdate=2013-11-12}}
* {{cite journal
 | last1 = Hilbert
 | first1 = Martin
 | first2 = Priscila |last2=López
 | title = The World's Technological Capacity to Store, Communicate, and Compute Information
 | journal = Science
 | volume = 332
 | issue = 6025
 | pages = 60–65
 | year = 2011
 | doi = 10.1126/science.1200970
 | pmid = 21310967 |url=http://martinhilbert.net/WorldInfoCapacity.html |ref=harv
}}
* {{cite web|url=http://www.ge-ip.com/library/detail/13476/?cid=wiki_Rise_of_Industrial_Big_Data|title=The Rise of Industrial Big Data|publisher=GE Intelligent Platforms|accessdate=2013-11-12}}

* [http://www.winshuttle.com/big-data-timeline/ History of Big Data Timeline]. A visual history of Big Data with links to supporting articles. 

{{Database models}}
{{Databases}}
{{Software engineering}}
{{Use dmy dates|date=January 2012}}

{{DEFAULTSORT:Big Data}}
[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Technology forecasting]]
[[Category:Transaction processing]]
[[Category:Big data| ]]</text>
      <sha1>5oitqlznh2fsyi1re5b4a7uyas8cys9</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Cloud Analytics</title>
    <ns>0</ns>
    <id>40236851</id>
    <revision>
      <id>575074482</id>
      <parentid>572175839</parentid>
      <timestamp>2013-09-30T00:50:16Z</timestamp>
      <contributor>
        <username>Meatsgains</username>
        <id>15420856</id>
      </contributor>
      <comment>remove peacock term</comment>
      <text xml:space="preserve" bytes="4762">{{multiple issues|
{{cleanup-reorganize|date=August 2013}}
{{essay-like|date=August 2013}}
{{original research|date=August 2013}}
{{refimprove|date=August 2013}}
{{sections|date=August 2013}}
{{Underlinked|date=August 2013}}
{{Orphan|date=August 2013}}
}}
[[Big data]] requires advanced technologies to competently process large quantities of data within acceptable time frames.  
The traditional means by which official statistics are analyzed and disseminated, both commercially and privately, consists of a large capital outlay for data life-cycle management and infrastructure. Additionally, the business processes by which official statistics are extracted and disseminated are inefficient, costly and require specific expertise.(1)

An evolution in analytics has emerged with the introduction of [[en.wikipedia.org/wiki/Cloud computing|cloud based self-service]] models. In 2006 the [http://www.abs.com.au Australian Bureau of Statistics] (ABS) led the world in the online dissemination of statistics based on a desk-top self-service model. With the release of TableBuilder(2) (formerly CDATA Online) in 2008, powered by Space-Time Research’s SuperWEB2 platform.  This was the very first instance of a government providing a self-service dissemination of official statistics extracted directly from unit record [http://www.abs.gov.au/about/microdata microdata](3)

A step forward has now made Big Data Analytics a reality in the Cloud. In 2012 [http://www.spacetimeresearch.com Space-Time Research] embarked on a research and development project to re-architect the technology involved in desk-top self-service analytics.  The goal was to get analytics off the desktop and into the cloud. The project became known as [http://www.superdatahub.com SuperDataHub]. Before this model existed, a typical user would have started by searching out data-cubes and downloading the cubes into specialized desktop software. Storage, archive and retrieval, would have to be managed by the user, placing considerable burden on IT systems and requiring considerable technical expertise. The vision was to see organisations who produce official statistics, maintain data sovereignty without restricting inter-agency data merging or third-party application development.  End users would be able to upload and merge proprietary data with official statistics without granting organisational firewall access.  A cloud based platform would enable community access, sharing and commentary on official statistics to further promote evidence-based decision making.  There several software development firms leading the way in such computing. Your obvious IBM and Microsoft along with some smaller firms such as [http://www.spacetimeresearch.com Space-Time Research], [http://www.tableausoftware.com Tableau], [http://www.birst.com Birst], [http://www.clickview.com.au Click View] and [http://www.qlikview.com/au Qlik View] to name a few.

Cloud Analytics is designed to make official statistical data readily categorized and available with the click of a mouse via the users web browser.  Cubes are available in the cloud, so the user no longer has to spend time and cost downloading, storing or archiving large volumes of data.  The user does not have to install expensive software, perform updates or upgrades to newer versions. All this is now handled on an external servers hosted by various providers.

The real benefits to the world of analytics is that it brings all the advantages of cloud computing to data exploration, analysis and sharing. Organisations no longer face the task of managing individual client applications and data, there is one copy located on a central cloud-based server.  Every user has the latest version without the IT department spending endless time performing updates on individual machines.

The result of the development of Cloud Analytics has yielded higher velocity data, better computing techniques and semantic networks. This type of model could lead the way to automated correlation of datasets providing users with information opportunities we have never seen (or thought of) before.(4)

==References==
&lt;ref&gt;1 [http://www.stats.gov.cn/english/specialtopics/.../CS_19_2_Savage_Gretton.doc Statistics New Zealand's Move to Process-oriented Statistics Production: progress, lessons and the way forward.] Tracey Savage&lt;/ref&gt;

&lt;ref&gt;2 http://www.abs.gov.au/websitedbs/censushome.nsf/home/tablebuilder&lt;/ref&gt;
&lt;ref&gt;3 - “Making Census Data Available with CDATA Online”, Department of Innovation,  May 25, 2010. http://showcase.govspace.gov.au/item/making-census-data-available-with-cdata-online/&lt;/ref&gt;
&lt;ref&gt;4 Cloud-Based Self Service Analytics A.Naish http://www.statistics.gov.hk/wsc/CPS109-P12-S.pdf&lt;/ref&gt;
{{Reflist}}

[[Category:Big data]]</text>
      <sha1>f494rh1265hbel4ejm1t9sislkjc04v</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Analytics</title>
    <ns>0</ns>
    <id>487132</id>
    <revision>
      <id>597279309</id>
      <parentid>596167298</parentid>
      <timestamp>2014-02-26T21:18:57Z</timestamp>
      <contributor>
        <username>Julesd</username>
        <id>9086</id>
      </contributor>
      <minor/>
      <comment>/* Portfolio analysis */</comment>
      <text xml:space="preserve" bytes="12849">[[File:Google Analytics Sample Dashboard.jpg|thumb|alt=Alternative text|A sample Google Analytics dashboard. Tools like this help businesses identify trends and make decisions.]] '''Analytics''' is the discovery and communication of meaningful patterns in data. Especially valuable in areas rich with recorded information, analytics relies on the simultaneous application of [[statistics]], [[computer programming]] and [[operations research]] to quantify performance. Analytics often favors [[data visualization]] to communicate insight.

Firms may commonly apply analytics to business data, to describe, predict, and improve business performance. Specifically, arenas within analytics include [[enterprise decision management]], retail analytics, store assortment and [[stock-keeping unit]] optimization, marketing optimization and marketing mix analytics, [[web analytics]], sales force sizing and optimization, price and promotion modeling, predictive science, credit [[risk analysis (business)#|risk analysis]], and [[fraud detection|fraud analytics]]. Since analytics can require extensive computation (See [[Big Data]]), the algorithms and software used for analytics harness the most current methods in computer science, statistics, and mathematics.&lt;ref&gt;{{cite journal|last=Kohavi, Rothleder and Simoudis|title=Emerging Trends in Business Analytics|journal=Communications of the ACM|year=2002|volume=45|issue=8|pages=45–48}}&lt;/ref&gt;

==Analytics vs. analysis==
Analytics is a multi-dimensional discipline. There is extensive use of mathematics and statistics, the use of descriptive techniques and predictive models to gain valuable knowledge from data - data analysis. The insights from data are used to recommend action or to guide decision making rooted in business context. Thus, analytics is not so much concerned with individual analyses or analysis steps, but with the entire [[methodology]]. There is a pronounced tendency to use the term ''analytics'' in business settings e.g. [[text analytics]] vs. the more generic [[text mining]] to emphasize this broader perspective. {{Citation needed|date=October 2013}}. There is an increasing use of the term ''advanced analytics'',{{cn|date=January 2014}} typically used to describe the technical aspects of analytics, especially [[predictive modeling]], [[machine learning]] techniques, and [[neural networks]].

== Examples ==

=== Marketing optimization ===
Marketing has evolved from a creative process into a highly data-driven process.  Marketing organizations use analytics to determine the outcomes of campaigns or efforts and to guide decisions for investment and consumer targeting.  Demographic studies, customer segmentation, conjoint analysis and other techniques allow marketers to use large amounts of consumer purchase, survey and panel data to understand and communicate marketing strategy.

[[Web analytics]] allows marketers to collect session-level information about interactions on a website using an operation called [[sessionization]].  Those interactions provide the web analytics information systems with the information to track the referrer, search keywords, IP address, and activities of the visitor.  With this information, a marketer can improve the marketing campaigns, site creative content, and information architecture.

Analysis techniques frequently used in marketing include marketing mix modeling, pricing and promotion analyses, sales force optimization, customer analytics e.g.: segmentation. Web analytics and optimization of web sites and online campaigns now frequently work hand in hand with the more traditional marketing analysis techniques. A focus on digital media has slightly changed the vocabulary so that marketing mix modeling is commonly referred to as attribution modeling in the digital or [[Marketing mix modeling|mixed-media]] context.

These tools and techniques support both strategic marketing decisions (such as how much overall to spend on marketing and how to allocate budgets across a portfolio of brands and the marketing mix) and more tactical campaign support in terms of targeting the best potential customer with the optimal message in the most cost effective medium at the ideal time.

=== Portfolio analysis ===
A common application of business analytics is [[Portfolio finance|portfolio analysis]]. In this, a [[bank]] or lending agency has a collection of accounts of varying [[Value economics|value]] and [[risk]]. The accounts may differ by the social status (wealthy, middle-class, poor, etc.) of the holder, the geographical location, its net value, and many other factors.  The lender must balance the return on the [[loan]] with the risk of default for each loan. The question is then how to evaluate the portfolio as a whole.

The least risk loan may be to the very wealthy, but there are a very limited number of wealthy people.  On the other hand there are many poor that can be lent to, but at greater risk.  Some balance must be struck that maximizes return and minimizes risk.  The analytics solution may combine [[time series]] analysis with many other issues in order to make decisions on when to lend money to these different borrower segments, or decisions on the interest rate charged to members of a portfolio segment to cover any losses among members in that segment.

=== Risk analytics ===

Predictive models in banking industry is widely developed to bring certainty across the risk scores for individual customers. Credit scores are built to predict individual’s delinquency behaviour and also scores are widely used to evaluate the credit worthiness of each applicant and rated while processing loan application. Furthermore, risk analyses are carried out in the scientific world and the insurance industry.

===Digital analytics ===
Digital analytics is a set of business and technical activities that define, create, collect, verify or transform digital data into reporting, research, analyses, recommendations, optimizations, predictions, and automations.&lt;ref&gt;Phillips, Judah &quot;Building a Digital Analytics Organization&quot; Financial Times Press, 2013, pp 7–8. &lt;/ref&gt;

==Challenges==
In the industry of commercial analytics software, an emphasis has emerged on solving the challenges of analyzing massive, complex data sets, often when such data is in a constant state of change. Such data sets are commonly referred to as [[big data]]. Whereas once the problems posed by big data were only found in the scientific community, today big data is a problem for many businesses that operate transactional systems online and, as a result, amass large volumes of data quickly.&lt;ref&gt;{{cite web|last=Naone|first=Erica|title=The New Big Data|url=http://www.technologyreview.com/computing/38397/|publisher=Technology Review, MIT|accessdate=August 22, 2011}}&lt;/ref&gt;

The analysis of [[unstructured data]] types is another challenge getting attention in the industry. Unstructured data differs from [[structured data]] in that its format varies widely and cannot be stored in traditional relational databases without significant effort at data transformation.&lt;ref&gt;{{cite book|last1=Inmon|first1=Bill|last2=Nesavich|first2=Anthony|title=Tapping Into Unstructured Data|year=2007|publisher=Prentice-Hall|isbn=978-0-13-236029-6}}&lt;/ref&gt; Sources of unstructured data, such as email, the contents of word processor documents, PDFs, geospatial data, etc., are rapidly becoming a relevant source of [[business intelligence]] for businesses, governments and universities.&lt;ref&gt;{{cite web|last=Wise|first=Lyndsay|title=Data Analysis and Unstructured Data|url=http://www.dashboardinsight.com/articles/business-performance-management/data-analysis-and-unstructured-data.aspx|publisher=Dashboard Insight|accessdate=February 14, 2011}}&lt;/ref&gt; For example, in Britain the discovery that one company was illegally selling fraudulent doctor's notes in order to assist people in defrauding employers and insurance companies,&lt;ref&gt;{{cite news|title=Fake doctors' sick notes for Sale for £25, NHS fraud squad warns|url=http://www.telegraph.co.uk/news/uknews/2626120/Fake-doctors-sick-notes-for-sale-on-web-for-25-NHS-fraud-squad-warns.html|publisher=The Telegraph|accessdate=August 2008|location=London}}&lt;/ref&gt; is an opportunity for insurance firms to increase the vigilance of their unstructured data analysis. The McKinsey Global Institute estimates that big data analysis could save the American health care system $300 billion per year and the European public sector €250 billion.&lt;ref&gt;{{cite news|title=Big Data: The next frontier for innovation, competition and productivity as reported in Building with Big Data|url=http://www.economist.com/node/18741392|accessdate=May 26, 2011 | work=The Economist|date=May 26, 2011| archiveurl= http://web.archive.org/web/20110603031738/http://www.economist.com/node/18741392| archivedate= 3 June 2011 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

These challenges are the current inspiration for much of the innovation in modern analytics information systems, giving birth to relatively new machine analysis concepts such as [[complex event processing]], full text search and analysis, and even new ideas in presentation.&lt;ref&gt;{{cite web|last=Ortega|first=Dan|title=Mobililty: Fueling a Brainier Business Intelligence|url=http://www.itbusinessedge.com/cm/community/features/guestopinions/blog/mobility-fueling-a-brainier-business-intelligence/?cs=47491|publisher=IT Business Edge|accessdate=June 21, 2011}}&lt;/ref&gt; One such innovation is the introduction of grid-like architecture in machine analysis, allowing increases in the speed of massively parallel processing by distributing the workload to many computers all with equal access to the complete data set.&lt;ref&gt;{{cite web|last=Khambadkone|first=Krish|title=Are You Ready for Big Data?|url=http://www.infogain.com/company/perspective-big-data.jsp|publisher=InfoGain|accessdate=February 10, 2011}}&lt;/ref&gt;

Analytics is increasingly used in [[education]], particularly at the district and government office levels. However, the complexity of student performance measures presents challenges when educators try to understand and use analytics to discern patterns in student performance, predict graduation likelihood, improve chances of student success, etc. For example, in a study involving districts known for strong data use, 48% of teachers had difficulty posing questions prompted by data, 36% did not comprehend given data, and 52% incorrectly interpreted data.&lt;ref&gt;U.S. Department of Education Office of Planning, Evaluation and Policy Development (2009). ''Implementing data-informed decision making in schools: Teacher access, supports and use.'' United States Department of Education (ERIC Document Reproduction Service No. ED504191)&lt;/ref&gt; To combat this, some analytics tools for educators adhere to an [[over-the-counter data]] format (embedding labels, supplemental documentation, and a help system, and making key package/display and content decisions) to improve educators’ understanding and use of the analytics being displayed.&lt;ref&gt;Rankin, J. (2013, March 28). [https://sas.elluminate.com/site/external/recording/playback/link/table/dropin?sid=2008350&amp;suid=D.4DF60C7117D5A77FE3AED546909ED2 How data Systems &amp; reports can either fight or propagate the data analysis error epidemic, and how educator leaders can help.] ''Presentation conducted from Technology Information Center for Administrative Leadership (TICAL) School Leadership Summit.''&lt;/ref&gt;

One more emerging challenge is dynamic regulatory needs. For example, in the banking industry, Basel III and future capital adequacy needs are likely to make even smaller banks adopt internal risk models.  In such cases, cloud computing and open source [[R (programming language)]] can help smaller banks to adopt risk analytics and support branch level monitoring by applying predictive analytics.{{citation needed|date=November 2012}}

== See also ==
{{Col-begin}}
{{Col-1-of-2}}
* [[Analysis]]
* [[Big data]]
* [[Business analytics]]
* [[Business intelligence]]
* [[Complex event processing]]
* [[Data mining]]
* [[Data presentation architecture]]
* [[Learning analytics]]
{{Col-2-of-2}}
* [[List of software engineering topics]]
* [[Machine learning]]
* [[Online analytical processing]]
* [[Online video analytics]]
* [[Operations research]]
* [[Predictive analytics]]
* [[Prescriptive Analytics|Prescriptive analytics]]
* [[Statistics]]
* [[Web analytics]]
* [[Smart grid]]
{{col-end}}

== References ==
{{reflist}}
*

== External links ==
* [http://analyticsmagazine.com/ INFORMS' bi-monthly, digital magazine on the analytics profession]

{{Wiktionary}}

[[Category:Analytics| ]]
[[Category:Financial data analysis]]
[[Category:Mathematical finance]]
[[Category:Formal sciences]]
[[Category:Business terms]]
[[Category:Business intelligence]]
[[Category:Big data]]</text>
      <sha1>5qw0lvzuqlyyi3js9prbnvndcy3px2i</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Prescriptive analytics</title>
    <ns>0</ns>
    <id>35757264</id>
    <revision>
      <id>598230435</id>
      <parentid>597965255</parentid>
      <timestamp>2014-03-05T08:25:03Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. - using [[Project:AWB|AWB]] (9957)</comment>
      <text xml:space="preserve" bytes="16295">{{multiple issues|refimprove=June2012|notability=June2012}}
'''Prescriptive analytics''' is the third and final phase of [[business analytics]] [[Business analytics|(BA)]] which includes descriptive, [[Predictive analytics|predictive]] and prescriptive analytics.&lt;ref&gt;{{cite journal|last=Evans,James R. and Lindner, Carl H.|title=Business Analytics: The Next Frontier for Decision Sciences|journal=Decision Line|date=March 2012|volume=43|issue=2}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Lustig,Irv, Dietric, Brenda, Johnson, Christer, and Dziekan, Christopher|title=The Analytics Journey|journal=Analytics|date=Nov–Dec 2010}}&lt;/ref&gt;

Prescriptive analytics automatically synthesizes [[big data]], multiple disciplines of [[mathematical sciences]] and [[computational science]]s, and [[business rules]], to make [[predictions]] and then suggests decision options to take advantage of the predictions.  The first stage of business analytics is descriptive analytics, which still accounts for the majority of all business analytics today.&lt;ref&gt;{{cite journal|last=Davenport,Tom |title=The three '..tives' of business analytics; predictive, prescriptive and descriptive|journal=CIO Enterprise Forum|date=November 2012}}&lt;/ref&gt; Descriptive analytics answers the questions what happened and why did it happen. Descriptive analytics looks at past performance and understands that performance by mining historical data to look for the reasons behind past success or failure. Most management reporting - such as [[sales]], [[marketing]], [[Business operations|operations]], and [[finance]] - uses this type of post-mortem analysis.

The next phase is [[predictive analytics]]. Predictive analytics answers the question what will happen. This is when historical performance data is combined with rules, [[algorithms]], and occasionally external data to determine the probable future outcome of an event or the likelihood of a situation occurring. The final phase is prescriptive analytics,&lt;ref&gt;{{cite journal|last=Haas, Peter J., Maglio, Paul P., Selinger, Patricia G., and Tan, Wang-Chie|title=Data is Dead…Without What-If Models|journal=Proceedings of the VLDB Endowment|year=2011|volume=4|number=12}}&lt;/ref&gt; which goes beyond predicting future outcomes by also suggesting actions to benefit from the predictions and showing the implications of each decision option.&lt;ref&gt;{{cite journal|last=Stewart, Thomas. R., and McMillan, Claude, Jr. |title=Descriptive and Prescriptive Models for Judgment and Decision Making: Implications for Knowledge Engineering|journal=NATO AS1 Senes, Expert Judgment and Expert Systems,|year=1987|volume=F35|pages=314–318}}&lt;/ref&gt;

Prescriptive analytics not only anticipates what will happen and when it will happen, but also why it will happen. Further, prescriptive analytics suggests decision options on how to take advantage of a future opportunity or mitigate a future risk and shows the implication of each decision option. Prescriptive analytics can continually take in new data to re-predict and re-prescribe, thus automatically improving prediction accuracy and prescribing better decision options. Prescriptive analytics ingests hybrid data, a combination of structured (numbers, categories) and unstructured data (videos, images, sounds, texts), and business rules to predict what lies ahead and to prescribe how to take advantage of this predicted future without compromising other priorities.&lt;ref&gt;{{cite journal|last=Riabacke, Mona, Danielson, Mats, and Ekenber, Love |title=State-of-the-Art Prescriptive Criteria Weight Elicitation|journal=Advances in Decision Sciences|year=2012}}&lt;/ref&gt;

All three phases of analytics can be performed through professional services or technology or a combination.  In order to scale, prescriptive analytics technologies need to be adaptive to take into account the growing volume, velocity, and variety of data that most mission critical processes and their environments may produce.

==History==

Prescriptive analytics has been around since about 2003. The technology behind prescriptive analytics synergistically combines hybrid [[data]], business rules with [[mathematical model]]s and [[computational model]]s. The data inputs to prescriptive analytics may come from multiple sources: internal, such as inside a corporation; and external, also known as environmental data.  The data may be structured, which includes numbers and categories, as well as [[unstructured data]], such as texts, images, sounds, and videos. Unstructured data differs from [[structured data]] in that its format varies widely and cannot be stored in traditional relational databases without significant effort at data transformation.&lt;ref&gt;{{cite book|last=Inmon|first=Bill|coauthor=Nesavich, Anthony|title=Tapping Into Unstructured Data|year=2007|publisher=Prentice-Hall|isbn=978-0-13-236029-6}}&lt;/ref&gt; More than 80% of the world's data today is unstructured, according to IBM.  In addition to this variety of data types and growing data volume, incoming data can also evolve with respect to velocity, that is, more data being generated at a faster or a variable pace. Business rules define the [[business process]] and include objectives constraints, preferences, policies, best practices, and boundaries. Mathematical models and computational models are techniques derived from mathematical sciences, computer science and related disciplines such as applied statistics, machine learning, operations research, natural language processing, computer vision, pattern recognition, image processing, speech recognition, and signal processing.

==Applications in healthcare==

Multiple factors are driving [[healthcare]] providers to dramatically improve business processes and operations as the United States healthcare industry embarks on the necessary migration from a largely fee-for service, volume-based system to a fee-for-performance, value-based system. Prescriptive analytics is playing a key role to help improve the performance in a number of areas involving various stakeholders: payers, providers and pharmaceutical companies.

Prescriptive analytics can help providers improve effectiveness of their clinical care delivery to the population they manage and in the process achieve better patient satisfaction and retention.  Providers can do better population health management by identifying appropriate intervention models for risk stratified population combining data from the in-facility care episodes and home based telehealth.

Prescriptive analytics can also benefit healthcare providers in their capacity planning by using analytics to leverage operational and usage data combined with data of external factors such as economic data, population demographic trends and population health trends, to more accurately plan for future capital investments such as new facilities and equipment utilization as well as understand the trade-offs between adding additional beds and expanding an existing facility versus building a new one.&lt;ref&gt;{{cite journal|last=Foster, Roger|title=Big data and public health, part 2: Reducing Unwarranted Services|journal=Government Health IT|date=May 2012}}&lt;/ref&gt;

Prescriptive analytics can help pharmaceutical companies to expedite their drug development by identifying patient cohorts that are most suitable for the clinical trials worldwide - patients who are expected to be compliant and will not drop out of the trial due to complications.  Analytics can tell companies how much time and money they can save if they choose one patient cohort in a specific country vs. another.

In provider-payer negotiations, [[Health care provider|providers]] can improve their negotiating position with health insurers by developing a robust understanding of future service utilization. By accurately predicting utilization, providers can also better allocate personnel.

==Applications in oil and gas==

Energy is the largest industry in the world ($6 trillion in size). The processes and decisions related to oil and natural gas exploration, development and production generate large amounts of data. Many types of captured data are used to create models and images of the Earth’s structure and layers 5,000 - 35,000 feet below the surface and to describe activities around the wells themselves, such as depositional characteristics, machinery performance, oil flow rates, reservoir temperatures and pressures.&lt;ref&gt;{{cite journal|last= Basu, Atanu|title= How Prescriptive Analytics Can Reshape Fracking in Oil and Gas Fields|journal= Data-Informed|date=November 2012}}&lt;/ref&gt; Prescriptive analytics software can help with both finding and producing oil and gas &lt;ref&gt;{{cite journal|last=  Basu, Atanu |title= How Data Analytics Can Help Frackers Find Oil |journal= Datanami|date=December 2013}}&lt;/ref&gt;
. It can take in seismic data, well log data, production data, and other related data sets to prescribe where to drill, how to drill (to maximize production, and minimize cost and environmental impact).

Prescriptive analytics software can accurately predict production issues by modeling numerous internal and external variables simultaneously. Prescriptive analytics software can also provide decision options and show the impact of each decision option so the operations managers can proactively take appropriate actions, on time, to guarantee future exploration and production performance. In the area of  [[Health, Safety and Environment|Health, Safety, and Environment]], prescriptive analytics can predict and preempt incidents that can lead to reputational and financial loss for oil and gas companies.

Pricing is another area of focus. [[Natural gas prices]] fluctuate dramatically depending upon supply, demand, [[econometrics]], [[geopolitics]], and weather conditions. Gas producers, pipeline transmission companies and [[Utility companies|utility firms]] have a keen interest in more accurately predicting gas prices so that they can lock in favorable terms while hedging downside risk. Prescriptive analytics software can accurately predict prices by modeling internal and external variables simultaneously and also provide decision options and show the impact of each decision option.&lt;ref&gt;{{cite journal|last=Dr. Watson, Michael|title=Advanced Analytics in Supply Chain - What is it, and is it Better than Non-Advanced Analytics?|journal=Supply Chain Digest|date=November 2012}}&lt;/ref&gt;

==Further reading==
{{refbegin|30em}}
* [[Thomas H. Davenport|Davenport, Thomas H]]., Kalakota, Ravi, Taylor, James, Lampa, Mike, Franks, Bill, Jeremy, Shapiro, Cokins, Gary, Way, Robin, King, Joy, Schafer, Lori, Renfrow, Cyndy and Sittig, Dean,  [http://iianalytics.com/wp-content/uploads/2011/12/2012-IIA-Predictions-Brief-Final.pdf ''Predictions for Analytics in 2012''] International Institute for Analytics (December 15, 2011)
* Bertolucci, Jeff, [http://www.informationweek.com/big-data/news/big-data-analytics/prescriptive-analytics-and-big-data-nex/240152863 ''Prescriptive Analytics and Data: Next Big Thing?''] InformationWeek. (April 15, 2013).
* Basu, Atanu, [http://www.analytics-magazine.org/march-april-2013/755-executive-edge-five-pillars-of-prescriptive-analytics-success ''Five Pillars of Prescriptive Analytics Success''] Analytics. (March / April 2013).
* Laney, Douglas and Kart, Lisa, (March 20, 2012). [http://www.parabal.com/uploads/docs/Greenplum/Emerging%20Role%20of%20the%20Data%20Scientist%20and%20the%20Art%20of%20Data%20Science.pdf ''Emerging Role of the Data Scientist and the Art of Data Science''] [[Gartner]].
* [[Robert R. McCormick School of Engineering and Applied Science|McCormick Northwestern Engineering]] [http://www.analytics.northwestern.edu/analytics-examples/prescriptive-analytics.html ''Prescriptive analytics is about enabling smart decisions based on data''].
* [http://business.gwu.edu/decisionsciences/i2sds/pdf/Program%20in%20BA%20presentation.pdf ''Business Analytics Information Event''], I2SDS and Department of Decision Sciences, School of Business, [[The George Washington University]] (February 10, 2011).
* [http://www.or-exchange.com/questions/1344/difference-between-operations-research-and-business-analysis &quot;The Difference Between Operations Research and Business Analysis&quot;] [[Informs|OR Exchange / Informs]] (April 2011).
* Farris, Adam, [http://www.analytics-magazine.org/november-december-2011/695-how-big-data-is-changing-the-oil-a-gas-industry &quot;How Big Data is Changing the Oil &amp; Gas Industry&quot;] Analytics. (November / December 2012).
* Venter, Fritz and Stein, Andrew [http://www.analytics-magazine.org/november-december-2011/694-images-a-videos-really-big-data &quot;Images &amp; Videos: Reall Big Data&quot;] Analytics. (November / December 2012).
* Venter, Fritz and Stein, Andrew [http://www.analytics-magazine.org/november-december-2011/694-images-a-videos-really-big-data &quot;The Technology Behind Image Analytics&quot;] Analytics. (November / December 2012).
* Horner, Peter and Basu, Atanu, [http://www.analytics-magazine.org/januaryfebruary-2012/503-analytics-a-the-future-of-healthcare ''Analytics and the Future of Healthcare''] Analytics. (January / February 2012).
* Ghosh, Rajib, Basu, Atanu and Bhaduri, Abhijit, [http://www.analytics-magazine.org/septemberoctober-2011/402-health-care-a-analytics.html ''From ‘Sick’ Care to ‘Health’ Care''] Analytics. (July / August 2011).
* Fischer, Eric, Basu, Atanu, Hubele, Joachim and Levine, Eric, [http://www.analytics-magazine.org/march-april-2011/278-predictive-analytics-tv-ads-wanamakers-dilemma-a-analytics.html ''TV ads, Wanamaker’s Dilemma &amp; Analytics''] Analytics. (March / April 2011)
* Basu, Atanu and Worth, Tim, [http://www.analytics-magazine.org/july-august-2010/128-predictive-analytics-game-changer.html ''Predictive Analytics Practical ways to Drive Customer Service, Looking Forward''] Analytics. (July / August 2010).
* Jain, Rajeev, Basu, Atanu, and Levine, Eric, [https://www.ayata.com/pdfs/Predictive-Analytics-In-Clean-Energy.pdf ''Putting Major Energy Decisions through their Paces, A Framework for a Better Environment through Analytics''] OR/MS Today (December 2010).
* Bhaduri, Abhijit and Basu, Atanu, [http://www.ayata.com/company/resources?start=6 Predictive Human Resources, Can Math Help Improve HR Mandates in an Organization?] OR/MS Today (October 2010).
* Brown, Scott, Basu, Atanu and Worth, Tim, [http://www.analytics-magazine.org/november-december-2010/98-predictive-analytics-in-field-service ''Predictive Analytics in Field Service, Practical Ways to Drive Field Service, Looking Forward] Analytics''. (November / December 2010).
* Pease, Andrew [http://support.sas.com/resources/papers/proceedings12/165-2012.pdf ''Bringing Optimization to the Business''], SAS Global Forum 2012, Paper 165-2012 (2012).
*  Basu, Atanu [http://www.wired.com/insights/2014/01/big-data-analytics-can-deliver-u-s-energy-independence/ &quot;What The Frack: U.S. Energy Prowess with Shale, Big Data Analytics&quot;] WIRED Blog. (January 2014).
{{refend}}

==See also==
{{Col-begin}}
{{Col-1-of-2}}
* [[Analytics]]
* [[Applied statistics|Applied Statistics]]
* [[Big data|Big Data]]
* [[Business analytics]]
* [[Business intelligence|Business Intelligence]]
* [[Data mining]]
{{Col-2-of-2}}
* [[Forecasting]]
* [[Hadoop]]
* [[Map reduce|MapReduce]]
* [[OLTP]]
* [[Operations research|Operations Research]]
* [[Statistics]]
{{col-end}}

==External links==
&lt;!-- Do not add forum, reseller, spam links to this article --&gt;
* [http://analyticsmagazine.com/ INFORMS' bi-monthly, digital magazine on the analytics profession]
* [http://www.analytics.northwestern.edu/analytics-examples/prescriptive-analytics.html Northwestern University Master of Science in Analytics]
* [http://business.gwu.edu/decisionsciences/i2sds/businessanalytics.cfm The George Washington University]
* [http://www.ayata.com AYATA]
* [http://www.youtube.com/watch?v=3l__n5zlVRQ&amp;feature=plcp Prescriptive Analytics Overview]
* Menon, Jai [http://www.youtube.com/watch?v=VtETirgVn9c &quot;Why Data Matters: Moving Beyond Prediction&quot;] IBM
* [http://www.mma.ugent.be University of Ghent Analytics program]

==References==
&lt;div class=&quot;references-small&quot;&gt;

{{reflist}}

[[Category:Analytics]]
[[Category:Big data|analytics]]
[[Category:Business intelligence]]
[[Category:Business terms]]
[[Category:Formal sciences]]
[[Category:Health care]]</text>
      <sha1>36emdezs9clrs6b50cn6hbwe2e4cbmf</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>ECL (data-centric programming language)</title>
    <ns>0</ns>
    <id>31108124</id>
    <revision>
      <id>572331952</id>
      <parentid>572177228</parentid>
      <timestamp>2013-09-10T11:55:53Z</timestamp>
      <contributor>
        <username>Frap</username>
        <id>612852</id>
      </contributor>
      <text xml:space="preserve" bytes="5855">{{Infobox programming language
| name                   = ECL
| developer              = [[HPCC|HPCC Systems]], LexisNexis Risk Solutions
| logo                   = 
| paradigm               = [[declarative programming|declarative]], [[structured]], [[Data-centric programming language|data-centric]]
| typing                 = [[type system#Static typing|static]], [[type system#Strong and weak typing|strong]], [[type system#Safely and unsafely typed systems|safe]]
| major implementations  = [[Windows Cluster]], [[Linux Cluster]]
| year                   = 2000
| designer               = 
| latest release version = 
| latest release date    = 
| influenced_by          = [[Prolog]], [[Pascal (programming language)|Pascal]], [[SQL]], [[Snobol4]], [[C++]], [[Clarion (programming language)|Clarion]]
| influenced             = 
| operating_system       = [[Linux]]
| license                = 
| website                = http://hpccsystems.com/
}}

'''ECL''' is a declarative, data centric programming language designed in 2000 to allow a team of programmers to process [[big data]] across a high performance computing cluster without the programmer being involved in many of the lower level, imperative decisions.&lt;ref&gt;[http://www.lexisnexis.com/risk/about/guides/program-guide.html A Guide to ECL, [[Lexis-Nexis]].]&lt;/ref&gt;&lt;ref&gt;&quot;Evaluating use of data flow systems for large graph analysis,&quot; by A. Yoo, and I. Kaplan. Proceedings of the 2nd Workshop on Many-Task Computing on Grids and Supercomputers, MTAGS, 2009&lt;/ref&gt;

== History ==
ECL was initially designed and developed in 2000 by David Bayliss as an in-house productivity tool within [[Lexis-Nexis|Seisint Inc]] and was considered to be a ‘secret weapon’ that allowed Seisint to gain market share in its data business. Equifax had an SQL-based process for predicting who would go bankrupt in the next 30 days, but it took 26 days to run the data. The first ECL implementation solved the same problem in 6 minutes. The technology was cited as a driving force behind the acquisition of Seisint by [[LexisNexis]] and then again as a major source of synergies when LexisNexis acquired ChoicePoint Inc.&lt;ref&gt;[http://www.reed-elsevier.com/mediacentre/pressreleases/2004/Pages/AcquisitionofSeisint.aspx Acquisition of Seisint]&lt;/ref&gt;

== Language constructs ==
ECL, at least in its purest form, is a declarative, data centric language. Programs, in the strictest sense, do not exist. Rather an ECL application will specify a number of core datasets (or data values) and then the operations which are to be performed on those values.

=== Hello world ===
ECL is to have succinct solutions to problems and sensible defaults. The &quot;Hello World&quot; program is characteristically short:
&quot;Hello World&quot;.
Perhaps a more flavorful example would take a list of strings, sort them into order, and then return that as a result instead.

&lt;pre&gt;
// First declare a dataset with one column containing a list of strings
// Datasets can also be binary, CSV, XML or externally defined structures

D := DATASET([{'ECL'},{'Declarative'},{'Data'},{'Centric'},{'Programming'},{'Language'}],{STRING Value;});
SD := SORT(D,Value);
output(SD)
&lt;/pre&gt;

The statements containing a &lt;code&gt;:=&lt;/code&gt; are defined in ECL as attribute definitions. They do not denote an action; rather a definition of a term. Thus, logically, an ECL program can be read: &quot;bottom to top&quot;

OUTPUT(SD)

What is an SD?

SD := SORT(D,Value); 

SD is a D that has been sorted by ‘Value’

What is a D?

D := DATASET([{'ECL'},{'Declarative'},{'Data'},{'Centric'},{'Programming'},{'Language'}],{STRING Value;});

D is a dataset with one column labeled ‘Value’ and containing the following list of data.

=== ECL primitives ===
ECL primitives that act upon datasets include: SORT, ROLLUP, DEDUP, ITERATE, PROJECT, JOIN, NORMALIZE, DENORMALIZE, PARSE, CHOSEN, ENTH, TOPN, DISTRIBUTE

=== ECL encapsulation ===
Whilst ECL is terse and LexisNexis claims that 1 line of ECL is roughly equivalent to 120 lines of C++ it still has significant support for large scale programming including data encapsulation and code re-use. The constructs available include: MODULE, FUNCTION, INTERFACE, MACRO, EXPORT, SHARED

=== Support for Parallelism in ECL ===
In the [[HPCC]] implementation, by default, most ECL constructs will execute in parallel across the hardware being used. Many of the primitives also have a LOCAL option to specify that the operation is to occur locally on each node.

=== Comparison to Map-Reduce ===
The Hadoop Map-Reduce paradigm actually consists of three phases which correlate to ECL primitives as follows.
{| class=&quot;wikitable&quot;
|-
! Hadoop Name/Term
! ECL equivalent
! Comments
|-
| MAPing within the MAPper	
| PROJECT/TRANSFORM
| Takes a record and converts to a different format; in the [[Hadoop]] case the conversion is into a key-value pair
|-
| SHUFFLE (Phase 1)
| DISTRIBUTE(,HASH(KeyValue))
| The records from the mapper are distributed dependent upon the KEY value
|-
| SHUFFLE (Phase 2)
| SORT(,LOCAL)
| The records arriving at a particular reducer are sorted into KEY order
|-
| REDUCE
| ROLLUP(,Key,LOCAL)
| The records for a particular KEY value are now combined together
|}

== References ==
&lt;!--- See [[Wikipedia:Footnotes]] on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

== External links ==
* [http://www.nytimes.com/2008/02/21/technology/21iht-reed.4.10279549.html Reed Elsevier to acquire ChoicePoint for $3.6 billion]
* [http://www.bloomberg.com/apps/news?pid=newsarchive&amp;sid=aBuqYZDOSPL4&amp;refer=uk  Reed Elsevier's LexisNexis Buys Seisint for $775 Mln]
* [http://www.reuters.com/finance/stocks/keyDevelopments?symbol=ENL&amp;pn=15  Reed Elsevier]

[[Category:Declarative programming languages]]
[[Category:Data-centric programming languages]]
[[Category:Big data]]</text>
      <sha1>efpr8np95yz6iivj3msede8f7xewq6i</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Data literacy</title>
    <ns>0</ns>
    <id>39675445</id>
    <revision>
      <id>590927971</id>
      <parentid>585606674</parentid>
      <timestamp>2014-01-16T05:32:02Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>/* Definitions */Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated date parameter errors]]</comment>
      <text xml:space="preserve" bytes="3611">'''Data literacy''' is the ability to read, create and communicate data as information and has been formally described in varying ways. Discussion of the skills inherent to data literacy and possible instructional methods have emerged as [[data collection]] becomes routinized and talk of [[data analysis]] and [[big data]] has become commonplace in the news, business,&lt;ref&gt;{{cite book|last=Hey, A. J., Tony Hey, Tansley, S. and Tolle, K., Eds.|title=The fourth paradigm: data-intensive scientific discovery.|year=2009|publisher=Microsoft}}&lt;/ref&gt; government&lt;ref&gt;{{cite web|title=Open Data Philly|url=http://opendataphilly.org/|accessdate=14 June 2013}}&lt;/ref&gt;  and society in countries across the world.&lt;ref&gt;{{cite journal|last=Na, L., &amp; Yan, Z.|title=Promote Data-intensive Scientific Discovery, Enhance Scientific and Technological Innovation Capability: New Model, New Method, and New Challenges Comments on&quot; The Fourth Paradigm: Data-intensive Scientific Discovery|journal=Bulletin of Chinese Academy of Sciences|year=2013|volume=1|issue=16}}&lt;/ref&gt;

==Related terms==

Data literacy focuses on the ability to build knowledge from data, and to communicate that meaning to others. It is related to other fields, including:
* [[Media literacy]]
* [[Information literacy]]
* [[New literacies]]
* [[Numeracy]]
* [[Transliteracy]]
* [[21st-century skills]]

== Definitions ==
* A traditional view emphasizes the numeric, statistical nature of data as information, including &quot;... understanding what data mean, including how to read graphs and charts appropriately, draw correct conclusions from data, and recognize when data are being used in misleading or inappropriate ways&quot;.&lt;ref&gt;{{cite journal|last=Carlson, J. R., Fosmire, M., Miller, C., Sapp Nelson, M.|title=Determining Data Information Literacy Needs: A Study of Students and Research Faculty|journal=Libraries Faculty and Staff Scholarship and Research|year=2011|volume=23|url=http://docs.lib.purdue.edu/lib_fsdocs/23}}&lt;/ref&gt;
* A more progressive view describes data literacy as &quot;... the ability to: formulate and answer questions using data as part of evidence-based thinking; use appropriate data, tools, and representations to support this thinking;interpret information from data; develop and evaluate data-based inferences and explanations;and use data to solve real problems and communicate their solutions.&quot;&lt;ref&gt;{{cite journal|last=Vahey, P., Yarnall, L., Patton, C., Zalles, D., &amp; Swan, K.|title=Mathematizing middle school: Results from a cross-disciplinary study of data literacy.|journal=American Educators Research Association Annual Conference|date=April 2006|volume=5}}&lt;/ref&gt;
* A workforce-focused example includes varying technical and digital formats by describing data literacy as &quot;... competence in finding, manipulating, managing, and interpreting data, including not just numbers but also text and images.&quot;&lt;ref&gt;{{cite web|last=Harris|first=Jeanne|title=Data Is Useless Without the Skills to Analyze It|url=http://blogs.hbr.org/cs/2012/09/data_is_useless_without_the_skills.html|work=Harvard Business Review|accessdate=14 June 2013}}&lt;/ref&gt;
==Applications==
* Journalism&lt;ref&gt;{{cite web|title=Become Data Literate in 3 Simple Steps|url=http://datajournalismhandbook.org/1.0/en/understanding_data_0.html}}&lt;/ref&gt;
* Education&lt;ref&gt;{{cite web|title=Data Literacy|url=http://ites.ncdpi.wikispaces.net/Data+Literacy}}&lt;/ref&gt;
== References ==
{{reflist}}

{{DEFAULTSORT:Literacy}}
[[Category:Computing and society]]
[[Category:Technology in society]]
[[Category:Literacy]]
[[Category:Mathematics education]]
[[Category:Big data]]
{{Education-stub}}</text>
      <sha1>31wjfo52ebkxuj8wae3af9bvtmc0qnj</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Industrial Internet</title>
    <ns>0</ns>
    <id>38232204</id>
    <revision>
      <id>601816704</id>
      <parentid>589824756</parentid>
      <timestamp>2014-03-29T15:37:24Z</timestamp>
      <contributor>
        <ip>108.197.177.2</ip>
      </contributor>
      <text xml:space="preserve" bytes="4495">The '''Industrial Internet''' is a term coined by [[GE]] &lt;ref&gt;{{cite web|last=Leber |first=Jessica |url=http://www.technologyreview.com/news/507831/general-electric-pitches-an-industrial-internet/ |title=General Electric’s San Ramon Software Center Takes Shape &amp;#124; MIT Technology Review |publisher=Technologyreview.com |date=2012-11-28 |accessdate=2013-08-18}}&lt;/ref&gt; and refers to the integration of complex physical machinery with networked sensors and software. The industrial Internet draws together fields such as [[machine learning]], [[big data]], the [[Internet of things]] and [[Machine to machine|machine-to-machine communication]] to ingest data from machines, analyze it (often in real-time), and use it to adjust operations.

==Examples==
The [[Google driverless car]] takes in environmental data from roof-mounted [[LIDAR]], uses [[Machine vision|machine-vision]] techniques to identify road geometry and obstacles, and controls the car’s throttle, brakes and steering mechanism in real-time.&lt;ref&gt;{{cite news|url=http://www.nytimes.com/2011/12/18/sunday-review/the-internet-gets-physical.html|author=Steve Lahor|title=The Internet Gets Physical|newspaper=[[The New York Times]]|accessdate=2013-08-18}}&lt;/ref&gt;

The [[Union Pacific Railroad]] mounts infrared thermometers, microphones and ultrasound scanners alongside its tracks. These sensors scan every train as it passes and send readings to the railroad’s data centers, where [[Pattern matching|pattern-matching]] software identifies equipment at risk of failure.&lt;ref&gt;{{cite web|author=Chris Murphy |url=http://www.informationweek.com/global-cio/interviews/union-pacific-delivers-internet-of-thing/240004930 |title=Union Pacific Delivers Internet Of Things Reality Check - Global Cio |publisher=Informationweek.com |date=2012-08-08 |accessdate=2013-08-18}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Chris Murphy |url=http://www.informationweek.com/global-cio/interviews/silicon-valley-needs-to-get-out-more/240143972 |title=Silicon Valley Needs To Get Out More - Global Cio - Executive |publisher=Informationweek.com |date=2012-12-07 |accessdate=2013-08-18}}&lt;/ref&gt; Falling prices for computing power and networked sensors mean that similar techniques can be applied to small, common devices like [[machine tool]]s.&lt;ref&gt;{{cite web|author=Jon Bruner |url=http://radar.oreilly.com/2012/10/listening-for-tired-machinery.html |title=Listening for tired machinery - O'Reilly Radar |publisher=Radar.oreilly.com |date=2012-10-29 |accessdate=2013-08-18}}&lt;/ref&gt;

==See also==
* [[Big data]]
* [[SCADA]]
* [[Internet of Things]]
* [[Machine to machine]]
* [[Industrial control system]]
* [[Industry 4.0]]

==References==
{{Reflist}}

==External links==
* Lohr, Steve. [http://www.nytimes.com/2011/12/18/sunday-review/the-internet-gets-physical.html &quot;The Internet Gets Physical&quot;] New York Times, December 17, 2011.
* Bruner, Jon. [http://radar.oreilly.com/2013/01/defining-the-industrial-internet.html &quot;Defining the industrial Internet&quot;] O'Reilly Radar, January 11, 2013.
* Murphy, Chris. [http://www.informationweek.com/global-cio/interviews/silicon-valley-needs-to-get-out-more/240143972 &quot;Silicon Valley Needs To Get Out More&quot;] InformationWeek, December 7, 2012.
* Loukides, Mike. [http://radar.oreilly.com/2012/11/to-eat-or-be-eaten.html &quot;To eat or be eaten?&quot;] O’Reilly Radar, November 30, 2012.	
* N.P., Ullekh. [http://articles.economictimes.indiatimes.com/2012-12-16/news/35837159_1_ge-internet-data &quot;How GE’s over $100 billion investment in ‘industrial internet’ will add $15 trillion to world GDP&quot;] Economic Times, December 16, 2012.
* Smarr, Larry. [http://www.nytimes.com/2011/12/06/science/larry-smarr-an-evolution-toward-a-programmable-world.html &quot;An Evolution Toward a Programmable Universe&quot;] New York Times, December 5, 2011.
* Evans, Peter C. and Marco Annunziata. [http://www.ge.com/docs/chapters/Industrial_Internet.pdf &quot;Industrial Internet: Pushing the Boundaries of Minds and Machines&quot;] GE white paper, November 26, 2012.
* Bacidore, Mike. [http://www.plantservices.com/voices/from_the_editor.html &quot;Are your prepared to work in an autonomous plant?&quot;], PlantService, March 2013.
* 
*{{cite web
  |url=http://www.industrialinternet.us
  |title= Industrial Internet 101 - A Beginner's Guide to the Next Industrial Revolution
}}

{{DEFAULTSORT:Industrial Internet}}
[[Category:Industrial automation]]
[[Category:Industrial computing]]
[[Category:Internet of Things]]
[[Category:Technology forecasting]]
[[Category:Big data]]</text>
      <sha1>rir4uflosmkg1eh8yas5azypwydd52m</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Industry 4.0</title>
    <ns>0</ns>
    <id>39773873</id>
    <revision>
      <id>601937715</id>
      <parentid>601035087</parentid>
      <timestamp>2014-03-30T09:28:07Z</timestamp>
      <contributor>
        <ip>125.25.112.93</ip>
      </contributor>
      <comment>/* What it means for manufacturing conglomerates */</comment>
      <text xml:space="preserve" bytes="9353">'''Industry 4.0''' is a project in the high-tech strategy of the [[German government]], which promotes the [[Digital Revolution|computerization]] of traditional industries such as manufacturing.&lt;ref&gt;[http://www.bmbf.de/de/19955.php Zukunftsprojekt Industrie 4.0]&lt;/ref&gt; The goal is the intelligent [[factory]] (Smart Factory), which is characterized by adaptability, resource efficiency and [[ergonomics]] as well as the integration of customers and business partners in business and value processes.&lt;ref&gt;[http://www.pt-it.pt-dlr.de/de/3069.php Webseite Projektträger DLR], Last download on 08. January 2013&lt;/ref&gt;
&lt;ref&gt;T. M. Böhler:''[http://www.produktion.de/automatisierung/industrie-4-0-smarte-produkte-und-fabriken-revolutionieren-die-industrie/ Industrie 4.0 – Smarte Produkte und Fabriken revolutionieren die Industrie].'' In: ''Produktion Magazin'', 10. Mai 2012; Last download on 5. September 2012&lt;/ref&gt; Technological basis are [[cyber-physical system]]s and the [[Internet of Things]].&lt;ref&gt;J. Jasperneite:''[http://www.computer-automation.de/steuerungsebene/steuernregeln/fachwissen/article/93559/0/Was_hinter_Begriffen_wie_Industrie_40_steckt/ Was hinter Begriffen wie Industrie 4.0 steckt].'' In: ''Computer &amp; Automation'', 19. Dezember 2012; Last download on 23. December 2012&lt;/ref&gt;

Experts believe that Industry 4.0 or the fourth industrial revolution could be a reality in about 10 to 20 years.

Meanwhile, in the United States, an initiative known as the [https://smartmanufacturingcoalition.org/ Smart Manufacturing Leadership Coalition] is also working on the future of manufacturing. Smart Manufacturing Leadership Coalition (SMLC)is a non-profit organization of manufacturing practitioners, suppliers, and technology companies; manufacturing consortia; universities; government agencies and laboratories. The aim of this coalition is to enable stakeholders in the manufacturing industry to form collaborative R &amp; D, implementation and advocacy groups for development of the approaches, standards, platforms and shared infrastructure that facilitate the broad adoption of manufacturing intelligence.

Similarly, GE has been working on an initiative called 'The Industrial Internet'.&lt;ref&gt;[The Industrial Internet, http://www.ge.com/docs/chapters/Industrial_Internet.pdf]&lt;/ref&gt; The Industrial Internet aims to bring together the advances of two transformative revolutions: the myriad machines, facilities, fleets and networks that arose from the Industrial
Revolution, and the more recent powerful advances in computing, information and communication systems brought to the fore by the Internet Revolution. According to GE, together these developments bring together three elements, which embody the essence 
of the Industrial Internet: INTELLIGENT MACHINES, ADVANCED ANALYTICS and PEOPLE AT WORK.

== Description ==
The term &quot;industrie 4.0&quot; refers to the fourth [[industrial revolution]]. The first industrial revolution was the mechanization of production using water and steam power, it was followed by the [[second industrial revolution]] which introduced mass production with the help of [[electric power]], followed by the [[digital revolution]], the use of electronics and IT to further automate production.&lt;ref&gt;[http://boerse.ard.de/meldungen/chart-infografik-evolution-industrie100~_v-large.jpg Die Evolution zur Industrie 4.0 in der Produktion] Last download on 14. April 2013&lt;/ref&gt;

The term was first used in 2011 at the [[Hanover Fair]].&lt;ref&gt;[http://www.vdi-nachrichten.com/artikel/Industrie-4-0-Mit-dem-Internet-der-Dinge-auf-dem-Weg-zur-4-industriellen-Revolution/52570/1 Industrie 4.0: Mit dem Internet der Dinge auf dem Weg zur 4. industriellen Revolution, VDI-Nachrichten, April 2011]&lt;/ref&gt; In October 2012 the Working Group on Industry 4.0 chaired by Siegfried Dais ( [[Robert Bosch GmbH]] ) and [[Henning Kagermann|Kagermann]] ([[acatech]]) presented a set of Industry 4.0 implementation recommendations to the German federal government. On 8 April 2013 at the Hanover Fair the final report of the Working Group Industry 4.0 was presented.&lt;ref&gt;[http://www.plattform-i40.de Industrie 4.0 Plattform] Last download on 15. Juli 2013&lt;/ref&gt;

== Meaning ==
Characteristic for industrial production in an Industry 4.0 environment are the strong customization of products under the conditions of high flexibilized (mass-) production. The required  automation technology is improved by the introduction of methods of self-optimization, self-configuration,&lt;ref&gt;[http://www.youtube.com/watch?v=JtC3DAfLTxw  Selbstkonfiguierende Automation für Intelligente Technische Systeme], Video, last download on 27. Dezember 2012&lt;/ref&gt; Self-diagnosis, cognition and intelligent support of workers in their increasingly complex work.&lt;ref&gt;Jasperneite, Jürgen; Niggemann, Oliver: Intelligente Assistenzsysteme zur Beherrschung der Systemkomplexität in der Automation. In: ATP edition - Automatisierungstechnische Praxis, 9/2012, Oldenbourg Verlag, München, September 2012&lt;/ref&gt; The largest project in Industry 4.0 at the present time is the BMBF leading-edge cluster &quot;Intelligent Technical Systems OstWestfalenLippe (it's OWL)&quot;. Another major project is the BMBF project RES-COM,&lt;ref&gt;[http://www.res-com-projekt.de/ Projekt RES-COM]&lt;/ref&gt; as well as the Cluster of Excellence &quot;Integrative Production Technology for High-Wage Countries&quot;.&lt;ref&gt;[http://www.production-research.de/ Webseite Exzellenzcluster &quot;Integrative Produktionstechnik für Hochlohnländer&quot;, Last download on 15. July 2013]&lt;/ref&gt;

== Industry 4.0 - what it means for the future industry ==
Recently, McKinsey &lt;ref&gt;[http://www.mckinsey.com/insights/business_technology/the_internet_of_things_and_the_future_of_manufacturing The Internet of Things and the future of manufacturing],&lt;/ref&gt; released an interview featuring an expert discussion between executives at Robert Bosch - Siegfried Dais (Partner of the Robert Bosch Industrietreuhand KG) and Heinz Derenbach (CEO of Bosch Software Innovations GmbH), and McKinsey experts. This interview addressed the prevalence of the Internet of Things in manufacturing and the consequent technology-driven changes that promise to trigger a new industrial revolution. At Bosch, and generally in Germany, this phenomenon is referred to as Industry 4.0. The basic principle of Industry 4.0 is that by connecting machines, work pieces and systems, we are creating intelligent networks along the entire value chain that can control each other autonomously.

Some examples for Industry 4.0 are machines that predict failures and trigger maintenance processes autonomously or self-organized logistics that react to unexpected changes in the production.

So, what effects does this change have on the classic manufacturing value chain? According to Siegfried Dais, “it is highly likely that the world of production will become more and more networked until everything is interlinked with everything else.” While this sounds like a fair assumption and the driving force behind the Internet of Things, it also means that the complexity of production and supplier networks will grow enormously. Networks and processes have so far been limited to one factory. But in an Industry 4.0 scenario, these boundaries of individual factories will most likely no longer exist. Instead, they will be lifted in order to interconnect multiple factories or even geographical regions.

== What are the challenges ==
1. Lack of adequate skill-sets to expedite the march towards fourth industrial revolution&lt;br /&gt;
2. Threat of redundancy of the corporate IT department&lt;br /&gt;
3. General reluctance to change by stakeholders

== Role of Big Data and Analytics ==
Modern information and communication technologies like Cyber-Physical Systems, Big Data or Cloud Computing will help predict the possibility to increase productivity, quality and flexibility within the manufacturing industry and thus to understand advantages within the competition.

== Impact of the Industry 4.0 ==
There are many areas that are foreseen to have an impact with the advent of the fourth industrial revolution. Of which four key impact areas emerge:

1. Machine Safety&lt;br /&gt;
2. Industry value chain&lt;br /&gt;
3. Workers&lt;br /&gt;
4. Socio-economic

== What it means for manufacturing conglomerates ==

GE &lt;br /&gt;
Dassault Systèmes &lt;br /&gt;
Bosch &lt;br /&gt;
Siemens &lt;br /&gt;
Mitsubishi &lt;br /&gt;
Schneider Electric

[to be updated]

== Current Status of Industry 4.0 ==

[Coming soon]

==See also==
* [[Big data]]
* [[SCADA]]
* [[Internet of Things]]
* [[Machine to machine]]
* [[Industrial control system]]
* [[Industrial Internet]]

==References==
{{reflist}}

==External links==
* [http://www.hightech-strategie.de/de/59.php Industrie 4.0] – Hightech-Strategie der Bundesregierung
* [http://www.bmbf.de/de/19955.php Bundesministerium für Forschung und Entwicklung] - Zukunftsprojekt Industrie 4.0
* [http://www.plattform-i40.de Plattform Industrie 4.0]
* [http://www.its-owl.de BMBF-Spitzencluster„Intelligente technische Systeme OstwestfalenLippe it's OWL]
* [http://www.production-research.de Exzellenzcluster Integrative Produktionstechnik für Hochlohnländer]

{{DEFAULTSORT:Industry 4.0}}
[[Category:Industrial automation]]
[[Category:Industrial computing]]
[[Category:Internet of Things]]
[[Category:Technology forecasting]]
[[Category:Big data]]
[[Category:Industrial Revolution]]</text>
      <sha1>cqgpiaaxjlsufn2juekwip04710ct06</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Social IT</title>
    <ns>0</ns>
    <id>39520913</id>
    <revision>
      <id>588834558</id>
      <parentid>572178878</parentid>
      <timestamp>2014-01-02T16:48:49Z</timestamp>
      <contributor>
        <username>Jasenlee</username>
        <id>36654</id>
      </contributor>
      <minor/>
      <comment>added see also to ITRPM</comment>
      <text xml:space="preserve" bytes="4931">'''Social IT''' involves the use of collaboration-based tools and solutions that transform the way IT professionals and related business line leaders enhance communication, productivity, knowledge-sharing, collaboration and decision-making. Social IT redefines the way IT professionals manage IT operations within corporate, government, academia, and other organizational settings.

_______________________________________________________________
*Industry analysts: Social IT is an [http://blogs.gartner.com/cameron_haight/2011/09/06/social-it-management-social-media-for-it-operations/ important next step] for IT management.
*Industry analysts: Current tools are [http://www.gartner.com/technology/reprints.do?id=1-1BS56X7&amp;ct=120821&amp;st=sb inadequate] because they lack Social IT.
*Current state of Social IT [http://www.cioinsight.com/it-management/innovation/slideshows/social-it-emerges-as-a-silo-smasher-for-cios/ research].
_______________________________________________________________

There are two main aspects of Social IT.  The first type of Social IT is the use of [[social media]] based communication methods to monitor, engage, and communicate with IT users.  This includes IT departments monitoring popular social media sites (both internal or external) to understand if employees or customers are dissatisfied with current services or in need of help or guidance.  It further includes the use of social media techniques and sites to communicate the status of IT services, new services being provided, tips and tricks, etc.  Finally, this “outbound” aspect of Social IT also includes the ability for users to directly request support or help from IT departments and service providers via social channels.  Many IT management tool providers provide add-on support for one or more of the above Social IT capabilities.  The value of this aspect of Social IT can be measured in terms of improved IT user satisfaction as well as the avoidance of IT service calls when IT proactively contacts users in order to notify them of new IT services or to resolve IT issues. For more perspective on Social IT's role in the support process visit http://www.servicesphere.com/blog/2012/5/9/what-is-social-it.html

The second aspect of Social IT is the internal use of social collaboration techniques to foster the capturing and sharing of knowledge in order to improve the management of IT environments as well as the enhancement of traditional IT processes such as those found in the [[Information Technology Infrastructure Library|IT Infrastructure Library]] (ITIL).  This type of Social IT has even greater potential than “outbound” Social IT to improve IT staff productivity and service performance.  The main benefits from improved knowledge collaboration and the use of social principles in day-to-day IT operations are a better use of staff expertise which generates significant improvements in productivity, speeds the [[Mean time to recovery|Mean Time to Restore Service (MTRS)]], decreases incident resolution time, reduces the risk associated with IT changes, and further assists with making more accurate and timely decisions in areas such as disaster recovery and business continuity, application release planning and rollout.  Essentially, any IT activity or decision that would benefit from crowd-sourced and peer reviewed IT knowledge will benefit from Social IT collaboration.

Social IT is one of several major trends that are re-shaping IT. These other related trends include: [[big data]]; [[cloud computing]]; Bring Your Own Device (BYOD); DevOps (the closer alignment and collaboration of application development and IT operations) and the increasing threat of insider attack and increasing [[cybersecurity]] risks.  The combination of these trends is driving IT organizations to re-evaluate their IT management approaches, and Social IT is gaining interest as a means of leverage the human element in IT along with traditional process-based approaches.  In addition to offering Social IT as an add-on capability, there is an emerging class of IT management vendors who are building solutions that are based on Social IT principles.

Leading industry analyst firms, such as Gartner, are calling for enhanced solutions from vendors that offer social IT capabilities.  For example, this was called out as an area of weakness from traditional ITSM vendors in the August 2012 [http://www.gartner.com/technology/reprints.do?id=1-1BS56X7&amp;ct=120821&amp;st=sb#dv_1_as_of Magic Quadrant] for IT Service Support Management and recently vendors that are basing their offerings on social IT collaboration were highlighted in the [http://www.gartner.com/technology/reprints.do?id=1-1F5NGML&amp;ct=130423&amp;st=sg ‘Cool Vendors’ in IT Operations Management report] for 2013.

== See also ==
* [[ITRPM|IT Resource Performance Management]]

[[Category:Social media]]
[[Category:Knowledge_management]]
[[Category:Big data]]</text>
      <sha1>2nzyr6mhd4je2xb89x8o3a1h46of4au</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Medio</title>
    <ns>0</ns>
    <id>39903888</id>
    <revision>
      <id>591477572</id>
      <parentid>587829356</parentid>
      <timestamp>2014-01-19T22:52:35Z</timestamp>
      <contributor>
        <ip>24.17.154.90</ip>
      </contributor>
      <comment>Removed orphan link, Now links from the venture funding companies</comment>
      <text xml:space="preserve" bytes="5317">{{Infobox company
| name     = Medio
| logo     = MedioLogo 2013.png
| type     = Corporation
| foundation       = [[Seattle]] (2004)  
| location         = 701 Pike Street Suite 1500, Seattle, WA 98101
| key_people       = Brian Lent (Founder)  Rob Lilleness (CEO) 
| industry         = [[Mobile web analytics|Mobile analytics]] and [[big data]]
| homepage         = [http://www.medio.com Medio.com]
}}

'''Medio '''is a [[Business-to-business|B2B]] [[mobile web analytics|mobile analytics]] provider based in [[Seattle]], WA. The company processes pre-existing data&lt;ref&gt;{{cite news| url=http://www.unr.edu/nevada-today/news/2012/engineering-grad-high-tech-frontier | title=Engineering grad Lent shares story| author=John Trent | publisher=UNR| date=October 5, 2012| accessdate=2013-08-02}}&lt;/ref&gt; to provide historic and [[predictive analytics]]. Medio is built on a cloud-based&lt;ref&gt;{{cite news| url=http://www.bizjournals.com/seattle/blog/techflash/2011/10/ibm-takes-medio-systems-to-the-cloud.html | title=IBM powers Seattle's Medio Systems in the cloud| author=Greg Lamm | publisher=Puget Sound Business Journal | date=October 26, 2011}}&lt;/ref&gt; [[Hadoop]] platform and is designed to interpret [[big data]] for mobile enterprise. Medio has had various partners including: [[IBM]], [[Rovio Entertainment|Rovio]],&lt;ref&gt;{{cite web |author=Nicole Perlroth  |url=http://www.forbes.com/sites/nicoleperlroth/2011/08/19/angry-birds-developer-partners-with-medio-as-it-heads-into-billion-dollar-valuation-territory/ |title=Angry Birds Developer Partners With Medio As It Heads Into Billion Dollar Valuation Territory |publisher=Forbes |date=August 19, 2011 |accessdate=2013-08-30}}&lt;/ref&gt; [[Verizon]], [[T-Mobile]],&lt;ref&gt;{{cite web|last=Sharma |first=Amol |url=http://online.wsj.com/article/SB119482551027089534.html |title=T-Mobile Wagers Deal With Google Is Worth the Risk - WSJ.com |publisher=Online.wsj.com |date=November 12, 2007 |accessdate=2013-08-11}}&lt;/ref&gt; [[American Broadcasting Company|ABC]], and [[Disney]]&lt;ref name=&quot;beat&quot; /&gt;

== History ==

Medio was founded in 2004 by Brian Lent, Bill Bryant, David Bluhm, and Michael Libes and employed 40 people.&lt;ref&gt;{{cite web| author=John Cook| date=October 27, 2005| url=http://www.seattlepi.com/news/article/Venture-Capital-Aiming-to-establish-mobile-search-1186148.php |title=Venture Capital: Aiming to establish mobile search |publisher=seattlepi.com  |accessdate=2013-08-14}}&lt;/ref&gt; Founded to be the '[[Google]]' of mobile search engines,&lt;ref name=&quot;times&quot; /&gt; Medio was backed by $30 Million dollars in initial venture funding from various tech companies including: [[Accel Partners]], [[Mohr Davidow Ventures]], and [[Frazier Technology Ventures]].

Medio received $11 Million more in 2006&lt;ref name=&quot;million&quot;&gt;{{cite web|author=John Cook |url=http://www.seattlepi.com/news/article/Medio-attracts-30-million-1219933.phpm |title=Medio attracts $30 million: Seattle mobile search startup will expand |publisher=seattlepi.com |date=2006-11-15 |accessdate=2013-08-30}}&lt;/ref&gt; to create a mobile analytics search engine capable of searching for ringtones, graphics, and internet-delivered information.&lt;ref name=&quot;times&quot;&gt;{{cite web|last=Duryee |first=Tricia |url=http://seattletimes.com/html/businesstechnology/2003625160_btmedio19.html |title=Business &amp; Technology &amp;#124; Medio launches cellphone ad network &amp;#124; Seattle Times Newspaper |publisher=Seattletimes.com |date=2007-03-19 |accessdate=2013-08-30}}&lt;/ref&gt; This sparked employment to over 100 employees for some time, but in 2009 Google released their new mobile search engine. Medio, unable to compete with the search giant, took that as an opportunity to refocus as a predictive analytics and data science provider, using their recommendations engine as a key component of their newly focused company. The shift resulted in lay-offs of much of the staff, scaling back to nearly 60 employees.&lt;ref name=&quot;allthingsd&quot;&gt;{{cite web| author=Ina Fried| date=March 17, 2011| url=http://allthingsd.com/20110317/onetime-mobile-search-player-medio-aims-for-rebirth-as-analytics-company/ |title=Onetime Mobile Search Player Medio Aims for Rebirth as Analytics Company| publisher=AllThingsD |accessdate=2013-07-26}}&lt;/ref&gt;

By the end of 2010 the company became profitable, nearly tripling its sales from previous years.&lt;ref&gt;{{cite web| date=August 19, 2008| author=Brier Dudley| url=http://seattletimes.com/html/technologybrierdudleysblog/2015957105_angry_birds_maker_rovio_taps_s.html |title=Angry Birds maker Rovio taps Seattle's Medio &amp;#124; Seattle Times Newspaper |publisher=Seattletimes.com |accessdate=2013-08-15}}&lt;/ref&gt; With the latest version of the Medio Platform and the release of products like K-Invite, Medio has grown to 70 employees with a total of $44 Million in venture funding.&lt;ref name=&quot;beat&quot;&gt;{{cite web| date= May 14, 2013| author=Dean Takahashi| url=http://venturebeat.com/2013/05/14/medio-launches-way-for-game-and-app-developers-to-get-more-users/ |title=Medio launches way for game and app developers to get more users |publisher=VentureBeat |accessdate=2013-07-06}}&lt;/ref&gt;

== References ==

{{Reflist}}

== External links ==
* [http://medio.com/ Medio.com Website]

{{DEFAULTSORT:Medio}}
[[Category:Analytics]]
[[Category:Big data]]
[[Category:Mobile technology]]
[[Category:Companies established in 2004]]</text>
      <sha1>amb94lqfc5v6c4fuixjnlt41qav9lke</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Predictive analytics</title>
    <ns>0</ns>
    <id>4141563</id>
    <revision>
      <id>601087176</id>
      <parentid>601087096</parentid>
      <timestamp>2014-03-24T20:39:46Z</timestamp>
      <contributor>
        <ip>128.229.4.2</ip>
      </contributor>
      <comment>/* Tools */</comment>
      <text xml:space="preserve" bytes="49346">{{Refimprove|date=June 2011}}
'''Predictive analytics''' encompasses a variety of statistical techniques from [[Predictive modelling|modeling]], [[machine learning]], and [[data mining]] that analyze current and historical facts to make [[prediction]]s about future, or otherwise unknown, events.&lt;ref name=&quot;Nyce&quot;&gt;{{citation|last=Nyce|first=Charles|title=Predictive Analytics White Paper|url=http://www.aicpcu.org/doc/predictivemodelingwhitepaper.pdf|publisher=American Institute for Chartered Property Casualty Underwriters/Insurance Institute of America|year=2007|page=1}}&lt;/ref&gt;&lt;ref name=&quot;Eckerson&quot;&gt;{{citation|last=Eckerson|first=Wayne|title=Extending the Value of Your Data Warehousing Investment|url=http://tdwi.org/articles/2007/05/10/predictive-analytics.aspx?sc_lang=en|publisher=The Data Warehouse Institute|date=May 10, 2007}}&lt;/ref&gt;

In business, predictive models exploit [[pattern detection|patterns]] found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding [[decision making]] for candidate transactions.

Predictive analytics is used in [[actuarial science]],&lt;ref name=&quot;Conz&quot;&gt;{{citation|last=Conz|first=Nathan|title=Insurers Shift to Customer-focused Predictive Analytics Technologies|url=http://www.insurancetech.com/business-intelligence/210600271|magazine=Insurance &amp; Technology|date=September 2, 2008}}&lt;/ref&gt; [[marketing]],&lt;ref&gt;{{citation|last=Fletcher|first=Heather|title=The 7 Best Uses for Predictive Analytics in Multichannel Marketing|url=http://www.targetmarketingmag.com/article/7-best-uses-predictive-analytics-modeling-multichannel-marketing/1#|magazine=Target Marketing|date=March 2, 2011}}&lt;/ref&gt; [[financial services]],&lt;ref&gt;{{citation|last=Korn|first=Sue|title=The Opportunity for Predictive Analytics in Finance|url= http://www.hpcwire.com/hpcwire/2011-04-21/the_opportunity_for_predictive_analytics_in_finance.html|magazine=HPC Wire|date=April 21, 2011}}&lt;/ref&gt; [[insurance]], [[telecommunications]],&lt;ref name=&quot;Barkin&quot;&gt;{{citation|last=Barkin|first=Eric|title=CRM + Predictive Analytics: Why It All Adds Up|url=http://www.destinationcrm.com/Articles/Editorial/Magazine-Features/CRM---Predictive-Analytics-Why-It-All-Adds-Up-74700.aspx|magazine=Destination CRM|date=May 2011}}&lt;/ref&gt; [[retail]],&lt;ref&gt;{{citation|last1=Das|first1=Krantik|last2=Vidyashankar|first2=G.S.|title=Competitive Advantage in Retail Through Analytics: Developing Insights, Creating Value|url=http://www.information-management.com/infodirect/20060707/1057744-1.html|magazine=Information Management|date=July 1, 2006}}&lt;/ref&gt; [[travel]],&lt;ref&gt;{{citation|last=McDonald|first=Michèle|title=New Technology Taps 'Predictive Analytics' to Target Travel Recommendations|url=http://www.travelmarketreport.com/technology?articleID=4259&amp;LP=1,|magazine=Travel Market Report|date=September 2, 2010}}&lt;/ref&gt; [[healthcare]],&lt;ref&gt;{{citation|last=Stevenson|first=Erin|title=Tech Beat: Can you pronounce health care predictive analytics?|url=http://www.times-standard.com/business/ci_19561141|newspaper=Times-Standard|date=December 16, 2011}}&lt;/ref&gt; [[Pharmaceutical company|pharmaceuticals]]&lt;ref&gt;{{citation|last=McKay|first=Lauren|title=The New Prescription for Pharma|url=http://www.destinationcrm.com/articles/Web-Exclusives/Web-Only-Bonus-Articles/The-New-Prescription-for-Pharma-55774.aspx|magazine=Destination CRM|date=August 2009}}&lt;/ref&gt; and other fields.

One of the most well known applications is [[credit scoring]],&lt;ref name=&quot;Nyce&quot; /&gt; which is used throughout [[financial services]]. Scoring models process a customer's [[credit history]], [[loan application]], customer data, etc., in order to rank-order individuals by their likelihood of making future credit payments on time. A well-known example is the [[Credit score (United States)#FICO score|FICO score]].

==Definition==
Predictive analytics is an area of data mining that deals with [[information extraction|extracting information]] from data and using it to predict [[trend analysis|trend]]s and behavior patterns. Often the unknown event of interest is in the future, but predictive analytics can be applied to any type of unknown whether it be in the past, present or future. For example, identifying suspects after a crime has been committed, or credit card fraud as it occurs. The core of predictive analytics relies on capturing relationships between [[explanatory variable]]s and the predicted variables from past occurrences, and exploiting them to predict the unknown outcome. It is important to note, however, that the accuracy and usability of results will depend greatly on the level of data analysis and the quality of assumptions.

== Types ==
Generally, the term predictive analytics is used to mean [[predictive modeling]], &quot;scoring&quot; data with predictive models, and [[forecasting]]. However, people are increasingly using the term to refer to related analytical disciplines, such as descriptive modeling and decision modeling or optimization. These disciplines also involve rigorous data analysis, and are widely used in business for segmentation and decision making, but have different purposes and the statistical techniques underlying them vary.

===Predictive models===
Predictive models are models of the relation between the specific performance of a unit in a sample and one or more known attributes or features of the unit. The objective of the model is to assess the likelihood that a similar unit in a different sample will exhibit the specific performance. This category encompasses models that are in many areas, such as marketing, where they seek out subtle data patterns to answer questions about customer performance, such as fraud detection models. Predictive models often perform calculations during live transactions, for example, to evaluate the risk or opportunity of a given customer or transaction, in order to guide a decision. With advancements in computing speed, individual agent modeling systems have become capable of simulating human behaviour or reactions to given stimuli or scenarios.

The available sample units with known attributes and known performances is referred to as the “training sample.” The units in other sample, with known attributes but un-known  performances, are referred to as “out of [training] sample” units. The out of sample bare no chronological relation to the training sample units. For example, the training sample may consists of literary attributes of writings by Victorian authors, with known attribution, and the out-of sample unit may be newly found writing with unknown authorship; a predictive model may aid the attribution of the unknown author. Another example is given by analysis of blood splatter in simulated crime scenes in which the out-of sample unit is the actual blood splatter pattern from a crime scene. The out of sample unit may be from the same time as the training units, from a previous time, or from a future time.

===Descriptive models===
Descriptive models quantify relationships in data in a way that is often used to classify customers or prospects into groups. Unlike predictive models that focus on predicting a single customer behavior (such as credit risk), descriptive models identify many different relationships between customers or products. Descriptive models do not rank-order customers by their likelihood of taking a particular action the way predictive models do. Instead, descriptive models can be used, for example, to categorize customers by their product preferences and life stage. Descriptive modeling tools can be utilized to develop further models that can simulate large number of individualized agents and make predictions.

===Decision models===
[[Decision model]]s describe the relationship between all the elements of a decision — the known data (including results of predictive models), the decision, and the forecast results of the decision — in order to predict the results of decisions involving many variables. These models can be used in optimization, maximizing certain outcomes while minimizing others. Decision models are generally used to develop decision logic or a set of business rules that will produce the desired action for every customer or circumstance.

==Applications==
Although predictive analytics can be put to use in many applications, we outline a few examples where predictive analytics has shown positive impact in recent years.

===Analytical customer relationship management (CRM)===
Analytical [[Customer Relationship Management]] is a frequent commercial application of Predictive Analysis. Methods of predictive analysis are applied to customer data to pursue CRM objectives, which involve constructing a holistic view of the customer no matter where their information resides in the company or the department involved. CRM uses predictive analysis in applications for marketing campaigns, sales, and customer services to name a few. These tools are required in order for a company to posture and focus their efforts effectively across the breadth of their customer base.  They must analyze and understand the products in demand or have the potential for high demand, predict customers' buying habits in order to promote relevant products at multiple touch points, and proactively identify and mitigate issues that have the potential to lose customers or reduce their ability to gain new ones.  Analytical Customer Relationship Management can be applied throughout the [[Customer lifecycle management|customers lifecycle]] ([[Customer acquisition management|acquisition]], [[Cross-selling|relationship growth]], [[Customer retention|retention]], and win-back).  Several of the application areas described below (direct marketing, cross-sell, customer retention) are part of Customer Relationship Managements.

===Clinical decision support systems===
Experts use predictive analysis in health care primarily to determine which patients are at risk of developing certain conditions, like diabetes, asthma, heart disease, and other lifetime illnesses. Additionally, sophisticated [[clinical decision support system]]s incorporate predictive analytics to support medical decision making at the point of care. A working definition has been proposed by Robert Hayward of the Centre for Health Evidence: &quot;Clinical Decision Support Systems link health observations with health knowledge to influence health choices by clinicians for improved health care.&quot;{{Citation needed|date=June 2012}}

===Collection analytics===
Every portfolio has a set of delinquent customers who do not make their payments on time. The financial institution has to undertake collection activities on these customers to recover the amounts due. A lot of collection resources are wasted on customers who are difficult or impossible to recover. Predictive analytics can help optimize the allocation of collection resources by identifying the most effective collection agencies, contact strategies, legal actions and other strategies to each customer, thus significantly increasing recovery at the same time reducing collection costs.

===Cross-sell===
Often corporate organizations collect and maintain abundant data (e.g. [[customer record]]s, sale transactions) as exploiting hidden relationships in the data can provide a competitive advantage. For an organization that offers multiple products, predictive analytics can help analyze customers' spending, usage and other behavior, leading to efficient [[cross-selling|cross sales]], or selling additional products to current customers.&lt;ref name=&quot;Eckerson&quot; /&gt; This directly leads to higher profitability per customer and stronger customer relationships.

===Customer retention===
With the number of competing services available, businesses need to focus efforts on maintaining continuous [[consumer satisfaction]], rewarding [[consumer loyalty]] and minimizing [[customer attrition]]. Businesses tend to respond to customer attrition on a reactive basis, acting only after the customer has initiated the process to terminate service. At this stage, the chance of changing the customer's decision is almost impossible. Proper application of predictive analytics can lead to a more proactive retention strategy. By a frequent examination of a customer’s past service usage, service performance, spending and other behavior patterns, predictive models can determine the likelihood of a customer terminating service sometime soon.&lt;ref  name=&quot;Barkin&quot; /&gt; An intervention with lucrative offers can increase the chance of retaining the customer. Silent attrition, the behavior of a customer to slowly but steadily reduce usage, is another problem that many companies face. Predictive analytics can also predict this behavior, so that the company can take proper actions to increase customer activity.

===Direct marketing===
When [[marketing]] consumer products and services, there is the challenge of keeping up with competing products and consumer behavior. Apart from identifying prospects, predictive analytics can also help to identify the most effective combination of product versions, marketing material, communication channels and timing that should be used to target a given consumer. The goal of predictive analytics is typically to lower the [[cost per order]] or [[cost per action]].

===Fraud detection===
[[Fraud]] is a big problem for many businesses and can be of various types: inaccurate credit applications, fraudulent [[financial transaction|transactions]] (both offline and online), [[identity theft]]s and false [[insurance claim]]s. These problems plague firms of all sizes in many industries. Some examples of likely victims are [[Credit card fraud|credit card issuers]], insurance companies,&lt;ref name = &quot;Schiff&quot;&gt;{{citation|last=Schiff|first=Mike|title=BI Experts: Why Predictive Analytics Will Continue to Grow|url=http://tdwi.org/Articles/2012/03/06/Predictive-Analytics-Growth.aspx?Page=1|publisher=The Data Warehouse Institute|date=March 6, 2012}}&lt;/ref&gt; retail merchants, manufacturers, business-to-business suppliers and even services providers. A predictive model can help weed out the &quot;bads&quot; and reduce a business's exposure to fraud.

Predictive modeling can also be used to identify high-risk fraud candidates in business or the public sector. [[Mark Nigrini]] developed a risk-scoring method to identify audit targets. He describes the use of this approach to detect fraud in the franchisee sales reports of an international fast-food chain. Each location is scored using 10 predictors. The 10 scores are then weighted to give one final overall risk score for each location. The same scoring approach was also used to identify high-risk check kiting accounts, potentially fraudulent travel agents, and questionable vendors. A reasonably complex model was used to identify fraudulent monthly reports submitted by divisional controllers.&lt;ref&gt;{{cite web
| last = Nigrini
| first = Mark
| title = Forensic Analytics: Methods and Techniques for Forensic Accounting Investigations
| publisher = John Wiley &amp; Sons Inc.
| location = Hoboken, NJ
| ISBN = 978-0-470-89046-2
| date = June 2011
| url = http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470890460.html
}}&lt;/ref&gt;

The [[IRS|Internal Revenue Service (IRS) of the United States]] also uses predictive analytics to mine tax returns and identify [[tax fraud]].&lt;ref name=&quot;Schiff&quot; /&gt;

Recent{{When|date=October 2011}} advancements in technology have also introduced predictive behavior analysis for [[web fraud]] detection. This type of solution utilizes [[heuristics]] in order to study normal web user behavior and detect anomalies indicating fraud attempts.

===Portfolio, product or economy-level prediction===
Often the focus of analysis is not the consumer but the product, portfolio, firm, industry or even the economy. For example, a retailer might be interested in predicting store-level demand for inventory management purposes. Or the Federal Reserve Board might be interested in predicting the unemployment rate for the next year. These types of problems can be addressed by predictive analytics using time series techniques (see below). They can also be addressed via machine learning approaches which transform the original time series into a feature vector space, where the learning algorithm finds patterns that have predictive power.&lt;ref&gt;{{cite journal |last=Dhar |first=Vasant |title=Prediction in Financial Markets: The Case for Small Disjuncts |journal=ACM Transactions on Intelligent Systems and Technologies|date=April 2011|volume=2|issue=3|url=http://dl.acm.org/citation.cfm?id=1961191}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Dhar|first=Vasant|coauthors=Chou, Dashin and Provost Foster|title=Discovering Interesting Patterns in Investment Decision Making with GLOWER – A Genetic Learning Algorithm Overlaid With Entropy Reduction|journal=Data Mining and Knowledge Discovery|date=October 2000|volume=4|issue=4|url=http://dl.acm.org/citation.cfm?id=593502}}&lt;/ref&gt;

===Risk management===

When employing risk management techniques, the results are always to predict and benefit from a future scenario. The [[Capital asset pricing model]] (CAP-M) &quot;predicts&quot; the best portfolio to maximize return, [[Probabilistic Risk Assessment]] (PRA)--when combined with mini-[[Delphi method|Delphi Techniques]] and statistical approaches yields accurate forecasts and [[RiskAoA]] is a stand-alone predictive tool.&lt;ref&gt;https://acc.dau.mil/CommunityBrowser.aspx?id=126070&lt;/ref&gt; These are three examples of approaches that can extend from project to market, and from near to long term. [[Underwriting]] (see below) and other business approaches identify risk management as a predictive method.

===Underwriting===

Many businesses have to account for risk exposure due to their different services and determine the cost needed to cover the risk. For example, auto insurance providers need to accurately determine the amount of premium to charge to cover each automobile and driver. A financial company needs to assess a borrower's potential and ability to pay before granting a loan. For a health insurance provider, predictive analytics can analyze a few years of past medical claims data, as well as lab, pharmacy and other records where available, to predict how expensive an enrollee is likely to be in the future. Predictive analytics can help [[underwrite]] these quantities by predicting the chances of illness, [[Default (finance)|default]], [[bankruptcy]], etc. Predictive analytics can streamline the process of customer acquisition by predicting the future risk behavior of a customer using application level data.&lt;ref name=&quot;Conz&quot; /&gt; Predictive analytics in the form of credit scores have reduced the amount of time it takes for loan approvals, especially in the mortgage market where lending decisions are now made in a matter of hours rather than days or even weeks. Proper predictive analytics can lead to proper pricing decisions, which can help mitigate future risk of default.

==Technology and big data influences==
[[Big data]] is a collection of data sets that are so large and complex that they become awkward to work with using traditional database management tools. The volume, variety and velocity of big data have introduced challenges across the board for capture, storage, search, sharing, analysis, and visualization. Examples of big data sources include web logs, RFID and sensor data, social networks, Internet search indexing, call detail records, military surveillance, and complex data in astronomic, biogeochemical, genomics, and atmospheric sciences. Thanks to technological advances in computer hardware—faster CPUs, cheaper memory, and MPP architectures-–and new technologies such as [[Hadoop]], [[MapReduce]], and [[In-database processing|in-database]] and [[text analytics]] for processing big data, it is now feasible to collect, analyze, and mine massive amounts of structured and [[unstructured data]] for new insights.&lt;ref name=&quot;Schiff&quot; /&gt; Today, exploring big data and using predictive analytics is within reach of more organizations than ever before and new methods that are capable for handling such datasets are proposed &lt;ref&gt;{{cite paper
|url= http://www.eng.tau.ac.il/~bengal/DID.pdf|title=Efficient Construction of Decision Trees by the Dual Information Distance Method|author= Ben-Gal I. Dana A., Shkolnik N. and Singer|publisher= Quality Technology &amp; Quantitative Management (QTQM), 11( 1), 133-147 |year=2014}}&lt;/ref&gt;[http://www.eng.tau.ac.il/~bengal/DID.pdf]
&lt;ref&gt;{{cite paper
|url= http://www.eng.tau.ac.il/~bengal/genre_statistics.pdf|title=Peer-to-peer information retrieval using shared-content clustering|author=Ben-Gal I., Shavitt Y., Weinsberg E., Weinsberg U. |publisher= Knowl Inf Syst
DOI 10.1007/s10115-013-0619-9 |year=2014}}&lt;/ref&gt;[http://www.eng.tau.ac.il/~bengal/genre_statistics.pdf]

==Analytical Techniques==

The approaches and techniques used to conduct predictive analytics can broadly be grouped into regression techniques and machine learning techniques.

===Regression techniques===

[[Regression analysis|Regression]] models are the mainstay of predictive analytics. The focus lies on establishing a mathematical equation as a model to represent the interactions between the different variables in consideration. Depending on the situation, there is a wide variety of models that can be applied while performing predictive analytics. Some of them are briefly discussed below.

====Linear regression model====

The [[linear regression model]] analyzes the relationship between the response or dependent variable and a set of independent or predictor variables. This relationship is expressed as an equation that predicts the response variable as a linear function of the parameters. These parameters are adjusted so that a measure of fit is optimized. Much of the effort in model fitting is focused on minimizing the size of the residual, as well as ensuring that it is randomly distributed with respect to the model predictions.

The goal of regression is to select the parameters of the model so as to minimize the sum of the squared residuals.  This is referred to as '''[[ordinary least squares]]''' (OLS) estimation and results in best linear unbiased estimates (BLUE) of the parameters if and only if the [[Gauss–Markov theorem|Gauss-Markov]] assumptions are satisfied.

Once the model has been estimated we would be interested to know if the predictor variables belong in the model – i.e. is the estimate of each variable's contribution reliable? To do this we can check the statistical significance of the model’s coefficients which can be measured using the t-statistic. This amounts to testing whether the coefficient is significantly different from zero. How well the model predicts the dependent variable based on the value of the independent variables can be assessed by using the R² statistic. It measures predictive power of the model i.e. the proportion of the total variation in the dependent variable that is &quot;explained&quot; (accounted for) by variation in the independent variables.

====Discrete choice models====

Multivariate regression (above) is generally used when the response variable is continuous and has an unbounded range. Often the response variable may not be continuous but rather discrete. While mathematically it is feasible to apply multivariate regression to discrete ordered dependent variables, some of the assumptions behind the theory of multivariate linear regression no longer hold, and there are other techniques such as discrete choice models which are better suited for this type of analysis. If the dependent variable is discrete, some of those superior methods are [[logistic regression]], [[multinomial logit]] and [[probit]] models. Logistic regression and probit models are used when the dependent variable is [[binary numeral system|binary]].

====Logistic regression====
{{details|logistic regression}}
In a classification setting, assigning outcome probabilities to observations can be achieved through the use of a logistic model, which is basically a method which transforms information about the binary dependent variable into an unbounded continuous variable and estimates a regular multivariate model (See Allison's Logistic Regression for more information on the theory of Logistic Regression).

The [[Wald test|Wald]] and [[likelihood-ratio test]] are used to test the statistical significance of each coefficient ''b'' in the model (analogous to the t tests used in OLS regression; see above). A test assessing the goodness-of-fit of a classification model is the &quot;percentage correctly predicted&quot;.

====Multinomial logistic regression====

An extension of the [[binary logit model]] to cases where the dependent variable has more than 2 categories is the [[multinomial logit model]]. In such cases collapsing the data into two categories might not make good sense or may lead to loss in the richness of the data. The multinomial logit model is the appropriate technique in these cases, especially when the dependent variable categories are not ordered (for examples colors like red, blue, green). Some authors have extended multinomial regression to include feature selection/importance methods such as [[Random multinomial logit]].

====Probit regression====

[[Probit model]]s offer an alternative to logistic regression for modeling categorical dependent variables. Even though the outcomes tend to be similar, the underlying distributions are different. Probit models are popular in social sciences like economics.

A good way to understand the key difference between probit and logit models is to assume that there is a latent variable z.

We do not observe z but instead observe y which takes the value 0 or 1. In the logit model we assume that y follows a [[logistic distribution]]. In the probit model we assume that y follows a standard normal distribution. Note that in social sciences (e.g. economics), probit is often used to model situations where the observed variable y is continuous but takes values between 0 and 1.

====Logit versus probit====

The [[Probit model]] has been around longer than the [[logit model]]. They behave similarly, except that the [[logistic distribution]] tends to be slightly flatter tailed. One of the reasons the logit model was formulated was that the probit model was computationally difficult due to the requirement of numerically calculating integrals. Modern computing however has made this computation fairly simple. The coefficients obtained from the logit and probit model are fairly close. However, the [[odds ratio]] is easier to interpret in the logit model.

Practical reasons for choosing the probit model over the logistic model would be:
* There is a strong belief that the underlying distribution is normal
* The actual event is not a binary outcome (''e.g.'', bankruptcy status) but a proportion (''e.g.'', proportion of population at different debt levels).

====Time series models====

[[Time series]] models are used for predicting or forecasting the future behavior of variables. These models account for the fact that data points taken over time may have an internal structure (such as autocorrelation, trend or seasonal variation) that should be accounted for. As a result standard regression techniques cannot be applied to time series data and methodology has been developed to decompose the trend, seasonal and cyclical component of the series. Modeling the dynamic path of a variable can improve forecasts since the predictable component of the series can be projected into the future.

Time series models estimate difference equations containing stochastic components. Two commonly used forms of these models are [[autoregressive model]]s (AR) and [[Moving average model|moving average]] (MA) models. The [[Box-Jenkins]] methodology (1976) developed by George Box and G.M. Jenkins combines the AR and MA models to produce the [[Autoregressive moving average model|ARMA]] (autoregressive moving average) model which is the cornerstone of stationary time series analysis. [[Autoregressive integrated moving average|ARIMA]](autoregressive integrated moving average models) on the other hand are used to describe non-stationary time series. Box and Jenkins suggest differencing a non stationary time series to obtain a stationary series to which an ARMA model can be applied. Non stationary time series have a pronounced trend and do not have a constant long-run mean or variance.

Box and Jenkins proposed a three stage methodology which includes: model identification, estimation and validation. The identification stage involves identifying if the series is stationary or not and the presence of seasonality by examining plots of the series, autocorrelation and partial autocorrelation functions.  In the estimation stage, models are estimated using non-linear time series or maximum likelihood estimation procedures. Finally the validation stage involves diagnostic checking such as plotting the residuals to detect outliers and evidence of model fit.

In recent years time series models have become more sophisticated and attempt to model conditional heteroskedasticity with models such as ARCH ([[autoregressive conditional heteroskedasticity]]) and GARCH (generalized autoregressive conditional heteroskedasticity) models frequently used for financial time series. In addition time series models are also used to understand inter-relationships among economic variables represented by systems of equations using VAR (vector autoregression) and structural VAR models.

====Survival or duration analysis====

[[Survival analysis]] is another name for time to event analysis. These techniques were primarily developed in the medical and biological sciences, but they are also widely used in the social sciences like economics, as well as in engineering (reliability and failure time analysis).

Censoring and non-normality, which are characteristic of survival data, generate difficulty when trying to analyze the data using conventional statistical models such as multiple [[linear regression]]. The [[normal distribution]], being a symmetric distribution, takes positive as well as negative values, but duration by its very nature cannot be negative and therefore normality cannot be assumed when dealing with duration/survival data. Hence the normality assumption of regression models is violated.

The assumption is that if the data were not censored it would be representative of the population of interest. In survival analysis, censored observations arise whenever the dependent variable of interest represents the time to a terminal event, and the duration of the study is limited in time.

An important concept in survival analysis is the [[hazard rate]], defined as the probability that the event will occur at time t conditional on surviving until time t. Another concept related to the hazard rate is the survival function which can be defined as the probability of surviving to time t.

Most models try to model the hazard rate by choosing the underlying distribution depending on the shape of the hazard function.  A distribution whose hazard function slopes upward is said to have positive duration dependence, a decreasing hazard shows negative duration dependence whereas constant hazard is a process with no memory usually characterized by the exponential distribution. Some of the distributional choices in survival models are: F, gamma, Weibull, log normal, inverse normal, exponential etc. All these distributions are for a non-negative random variable.

Duration models can be parametric, non-parametric or semi-parametric. Some of the models commonly used are [[Kaplan-Meier]] and Cox proportional hazard model (non parametric).

====Classification and regression trees====
{{main|decision tree learning}}
Hierarchical Optimal Discriminant Analysis (HODA), (also called classification tree analysis) is a generalization of [[Optimal discriminant analysis]] that may be used to identify the statistical model that has maximum accuracy for predicting the value of a categorical dependent variable for a dataset consisting of categorical and continuous variables. The output of HODA is a non-orthogonal tree that combines categorical variables and cut points for continuous variables that yields maximum predictive accuracy, an assessment of the exact Type I error rate, and an evaluation of potential cross-generalizability of the statistical model. Hierarchical Optimal Discriminant analysis may be thought of as a generalization of Fisher's linear discriminant analysis. Optimal discriminant analysis is an alternative to ANOVA (analysis of variance) and regression analysis, which attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA and regression analysis give a dependent variable that is a numerical variable, while hierarchical optimal discriminant analysis gives a dependent variable that is a class variable.

Classification and regression trees (CART) is a [[non-parametric statistics|non-parametric]] [[decision tree learning]] technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively.

[[Decision trees]] are formed by a collection of rules based on variables in the modeling data set:
* Rules based on variables' values are selected to get the best split to differentiate observations based on the dependent variable
* Once a rule is selected and splits a node into two, the same process is applied to each &quot;child&quot; node (i.e. it is a recursive procedure)
* Splitting stops when CART detects no further gain can be made, or some pre-set stopping rules are met. (Alternatively, the data are split as much as possible and then the tree is later [[Pruning (decision trees)|pruned]].)

Each branch of the tree ends in a terminal node. Each observation falls into one and exactly one terminal node, and each terminal node is uniquely defined by a set of rules.

A very popular method for predictive analytics is Leo Breiman's [[Random forests]] or derived versions of this technique like [[Random multinomial logit]].

====Multivariate adaptive regression splines====

[[Multivariate adaptive regression splines]] (MARS) is a [[Non-parametric statistics|non-parametric]] technique that builds flexible models by fitting [[piecewise]] [[linear regression]]s.

An important concept associated with regression splines is that of a knot.  Knot is where one local regression model gives way to another and thus is the point of intersection between two splines.

In multivariate and adaptive regression splines, [[basis function]]s are the tool used for generalizing the search for knots. Basis functions are a set of functions used to represent the information contained in one or more variables.
Multivariate and Adaptive Regression Splines model almost always creates the basis functions in pairs.

Multivariate and adaptive regression spline approach deliberately [[overfit]]s the model and then prunes to get to the optimal model. The algorithm is computationally very intensive and in practice we are required to specify an upper limit on the number of basis functions.

===Machine learning techniques===

[[Machine learning]], a branch of artificial intelligence, was originally employed to develop techniques to enable computers to learn. Today, since it includes a number of advanced statistical methods for regression and classification, it finds application in a wide variety of fields including [[medical diagnostics]], [[credit card fraud detection]], [[Face recognition|face]] and [[speech recognition]] and analysis of the [[stock market]].  In certain applications it is sufficient to directly predict the dependent variable without focusing on the underlying relationships between variables. In other cases, the underlying relationships can be very complex and the mathematical form of the dependencies unknown. For such cases, machine learning techniques emulate human [[cognition]] and learn from training examples to predict future events.

A brief discussion of some of these methods used commonly for predictive analytics is provided below. A detailed study of machine learning can be found in Mitchell (1997).

====Neural networks====

[[Neural networks]] are [[Nonlinearity|nonlinear]] sophisticated modeling techniques that are able to [[Model (abstract)|model]] complex functions. They can be applied to problems of [[Time series|prediction]], [[Statistical classification|classification]] or [[Control theory|control]] in a wide spectrum of fields such as [[finance]], [[cognitive psychology]]/[[cognitive neuroscience|neuroscience]], [[medicine]], [[engineering]], and [[physics]].

Neural networks are used when the exact nature of the relationship between inputs and output is not known. A key feature of neural networks is that they learn the relationship between inputs and output through training. There are three types of training in neural networks used by different networks, [[Supervised learning|supervised]] and [[Unsupervised learning|unsupervised]] training, reinforcement learning, with supervised being the most common one.

Some examples of neural network training techniques are [[backpropagation]], quick propagation, [[Conjugate gradient method|conjugate gradient descent]], [[Radial basis function|projection operator]], Delta-Bar-Delta etc. Some unsupervised network architectures are multilayer [[perceptron]]s, [[Self-organizing map|Kohonen network]]s, [[Hopfield network]]s, etc.

====Multilayer Perceptron (MLP)====
The Multilayer Perceptron (MLP) consists of an input and an output layer with one or more hidden layers of nonlinearly-activating nodes or sigmoid nodes. This is determined by the weight vector and it is necessary to adjust the weights of the network. The backpropogation employs gradient fall to minimize the squared error between the network output values and desired values for those outputs. The weights adjusted by an iterative process of repetitive present of attributes. Small changes in the weight to get the desired values are done by the process called training the net and is done by the training set (learning rule).

====Radial basis functions====
A [[radial basis function]] (RBF) is a function which has built into it a distance criterion with respect to a center. Such functions can be used very efficiently for interpolation and for smoothing of data. Radial basis functions have been applied in the area of [[neural network]]s where they are used as a replacement for the [[Sigmoid function|sigmoidal]] transfer function. Such networks have 3 layers, the input layer, the hidden layer with the RBF non-linearity and a linear output layer. The most popular choice for the non-linearity is the Gaussian. RBF networks have the advantage of not being locked into local minima as do the [[Feed forward (control)|feed-forward]] networks such as the multilayer [[perceptron]].

====Support vector machines====

[[Support Vector Machine]]s (SVM) are used to detect and exploit complex patterns in data by clustering, classifying and ranking the data. They are learning machines that are used to perform binary classifications and regression estimations.  They commonly use kernel based methods to apply linear classification techniques to non-linear classification problems. There are a number of types of SVM such as linear, polynomial, sigmoid etc.

====Naïve Bayes====

[[Naive Bayes classifier|Naïve Bayes]] based on Bayes conditional probability rule is used for performing classification tasks. Naïve Bayes assumes the predictors are statistically independent which makes it an effective classification tool that is easy to interpret. It is best employed when faced with the problem of ‘[[curse of dimensionality]]’ i.e. when the number of predictors is very high.

====''k''-nearest neighbours====

The [[K-nearest neighbor algorithm|nearest neighbour algorithm]] (KNN) belongs to the class of pattern recognition statistical methods. The method does not impose a priori any assumptions about the distribution from which the modeling sample is drawn. It involves a training set with both positive and negative values.  A new sample is classified by calculating the distance to the nearest neighbouring training case. The sign of that point will determine the classification of the sample. In the k-nearest neighbour classifier, the k nearest points are considered and the sign of the majority is used to classify the sample. The performance of the kNN algorithm is influenced by three main factors: (1) the distance measure used to locate the nearest neighbours; (2) the decision rule used to derive a classification from the k-nearest neighbours; and (3) the number of neighbours used to classify the new sample. It can be proved that, unlike other methods, this method is universally asymptotically convergent, i.e.: as the size of the training set increases, if the observations are [[iid|independent and identically distributed (i.i.d.)]], regardless of the distribution from which the sample is drawn, the predicted class will converge to the class assignment that minimizes misclassification error. See Devroy et al.

====Geospatial predictive modeling====

Conceptually, [[Geospatial Predictive Modeling|geospatial predictive modeling]] is rooted in the principle that the occurrences of
events being modeled are limited in distribution. Occurrences of events are neither uniform
nor random in distribution – there are spatial environment factors (infrastructure, sociocultural,
topographic, etc.) that constrain and influence where the locations of events occur.
Geospatial predictive modeling attempts to describe those constraints and influences by
spatially correlating occurrences of historical geospatial locations with environmental factors
that represent those constraints and influences. Geospatial predictive modeling is a process
for analyzing events through a geographic filter in order to make statements of likelihood for
event occurrence or emergence.

==Tools==
Historically, using predictive analytics tools—as well as understanding the results they delivered—required advanced skills. However, modern predictive analytics tools are no longer restricted to IT specialists{{fact|date=March 2014}}. As more organizations adopt predictive analytics into decision-making processes and integrate it into their operations, they are creating a shift in the market toward business users as the primary consumers of the information. Business users want tools they can use on their own. Vendors are responding by creating new software that removes the mathematical complexity, provides user-friendly graphic interfaces and/or builds in short cuts that can, for example, recognize the kind of data available and suggest an appropriate predictive model.&lt;ref name=&quot;Halper&quot;&gt;{{citation|last=Halper|first=Fern|title=The Top 5 Trends in Predictive Analytics|url=http://www.information-management.com/issues/21_6/the-top-5-trends-in-redictive-an-alytics-10021460-1.html|magazine=Information Management|date=November 1, 2011}}&lt;/ref&gt; Predictive analytics tools have become sophisticated enough to adequately present and dissect data problems{{fact|date=March 2014}}, so that any data-savvy information worker can utilize them to analyze data and retrieve meaningful, useful results.&lt;ref name=&quot;Eckerson&quot; /&gt; For example, modern tools present findings using simple charts, graphs, and scores that indicate the likelihood of possible outcomes.&lt;ref&gt;{{citation|last=MacLennan|first=Jamie|title=5 Myths about Predictive Analytics|url=http://tdwi.org/articles/2012/05/01/5-predictive-analytics-myths.aspx|publisher=The Data Warehouse Institute|date=May 1, 2012}}&lt;/ref&gt;

There are numerous tools available in the marketplace that help with the execution of predictive analytics. These range from those that need very little user sophistication to those that are designed for the expert practitioner. The difference between these tools is often in the level of customization and heavy data lifting allowed.

Notable open source predictive analytic tools include:
{{columns-list|3|
* [[KNIME]]
* [[Orange (software)|Orange]]
* [[R (programming language)|R]]
* [[RapidMiner]]
* [[Weka (machine learning)|Weka]]
* [[GNU Octave]]
* [[Apache Mahout]]
}}

Notable commercial predictive analytic tools include:
{{columns-list|3|
* [[Alpine Data Labs]]
* [[Actuate Corporation|BIRT Analytics]]
* [[Angoss|Angoss KnowledgeSTUDIO]]
* [[SPSS|IBM SPSS Statistics]] and [[SPSS Modeler|IBM SPSS Modeler]]
* [[KXEN Inc.|KXEN Modeler]]
* [[Logical Glue]]
* [[Mathematica]]
* [[MATLAB]]
* [[Minitab]]
* [[Oracle Corporation|Oracle Data Mining (ODM)]]
* [[Pervasive Software|Pervasive]]
* [[SAP AG|SAP]]
* [[SAS (software)|SAS]] and [[SAS (software)#Components|SAS Enterprise Miner]]
* [[STATISTICA]]
* [[Tibco Software|TIBCO]]
}}

===PMML===
In an attempt to provide a standard language for expressing predictive models, the [[Predictive Model Markup Language]] (PMML) has been proposed. Such an XML-based language provides a way for the different tools to define predictive models and to share these between PMML compliant applications. PMML 4.0 was released in June, 2009.

==Criticism==
There are plenty of skeptics when it comes to computers and algorithms abilities to predict the future, including [[Gary King (political scientist)|Gary King]], a professor from Harvard University and the director of the Institute for Quantitative Social Science.  
&lt;ref&gt;{{citation|last=Temple-Raston|first=Dina|title=Predicting The Future: Fantasy Or A Good Algorithm?|url=http://www.npr.org/2012/10/08/162397787/predicting-the-future-fantasy-or-a-good-algorithm|publisher=NPR|date=Oct 8, 2012}}&lt;/ref&gt;
People are influenced by their environment in innumerable ways. Trying to understand what people will do next assumes that all the influential variables can be known and measured accurately. &quot;People's environments change even more quickly than they themselves do. Everything from the weather to their relationship with their mother can change the way people think and act. All of those variables are unpredictable. How they will impact a person is even less predictable. If put in the exact same situation tomorrow, they may make a completely different decision. This means that a statistical prediction is only valid in sterile laboratory conditions, which suddenly isn't as useful as it seemed before.&quot;
&lt;ref&gt;{{citation|last=Alverson|first=Cameron|title=Polling and Statistical Models Can't Predict the Future |url=http://www.cameronalverson.com/2012/09/polling-and-statistical-models-cant.html|publisher=Cameron Alverson|date=Sep 2012}}&lt;/ref&gt;

==See also==
*[[Criminal Reduction Utilising Statistical History]]
*[[Data mining]]
*[[Learning analytics]]
*[[Odds algorithm]]
*[[Pattern recognition]]
*[[Prescriptive Analytics|Prescriptive analytics]]
*[[Predictive modeling]]
*[[RiskAoA]] a predictive tool for discriminating future decisions.
{{more footnotes|date=October 2011}}

==References==
{{Reflist|2}}

===Further reading===
* {{cite book | author=Agresti, Alan| title=Categorical Data Analysis| location= Hoboken | publisher=John Wiley and Sons| year=2002 | isbn=0-471-36093-7}}
* Coggeshall, Stephen, Davies, John, [[Roger Jones (physicist and entrepreneur)|Jones, Roger.]], and Schutzer, Daniel, &quot;Intelligent Security Systems,&quot; in {{cite book | author=Freedman, Roy S., Flein, Robert A., and Lederman, Jess, Editors  | title=Artificial Intelligence in the Capital Markets | location= Chicago | publisher=Irwin| year=1995 | isbn=1-55738-811-3}}
* {{cite book | author=L. Devroye, L. Györfi, G. Lugosi| title=A Probabilistic Theory of Pattern Recognition| location= New York | publisher=Springer-Verlag| year=1996 | id=}}
* {{cite book | author=Enders, Walter| title=Applied Time Series Econometrics| location= Hoboken | publisher=John Wiley and Sons| year=2004 | isbn=0-521-83919-X }}
* {{cite book | author=Greene, William| title=Econometric Analysis, 7th Ed| location=London  | publisher=Prentice Hall| year=2012 | isbn=978-0-13-139538-1}}
* {{cite book | author=Guidère, Mathieu; Howard N, Sh. Argamon| title=Rich Language Analysis for Counterterrrorism| location=Berlin, London, New York  | publisher=Springer-Verlag| year=2009 | isbn=978-3-642-01140-5}}
* {{cite book | author=Mitchell, Tom| title=Machine Learning| location=New York  | publisher=McGraw-Hill| year=1997 | isbn=0-07-042807-7}}
* {{cite book | author=Siegel, Eric|title=Predictive Analytics: The Power to Predict Who Will Click, Buy, Lie, or Die|publisher=John Wiley|year=2013|isbn=978-1-1183-5685-2}}
* {{cite book | author=Tukey, John| title=Exploratory Data Analysis| location=New York  | publisher=Addison-Wesley| year=1977 | isbn=0-201-07616-0}}
* {{cite book | author=Finlay, Steven| title=Credit Scoring, Response Modeling and Insurance Rating. A Practical Guide to Forecasting Customer Behavior| location=Basingstoke  | publisher=Palgrave Macmillan| year=2012 | isbn=0-230-34776-2}}


[[Category:Statistical models]]
[[Category:Business intelligence]]
[[Category:Insurance]]
[[Category:Big data|analytics]]</text>
      <sha1>5cyz4x12aqkxcj5kzemerw53b8byh91</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>CloverETL</title>
    <ns>0</ns>
    <id>39740150</id>
    <revision>
      <id>602164311</id>
      <parentid>599138628</parentid>
      <timestamp>2014-03-31T20:39:36Z</timestamp>
      <contributor>
        <username>Editorsaurus</username>
        <id>18950257</id>
      </contributor>
      <minor/>
      <comment>/* Extensions */</comment>
      <text xml:space="preserve" bytes="11985">{{Infobox software
| name                   = CloverETL
| logo                   = [[File:The CloverETL logo.png|180px]]
| developer              = [http://www.javlininc.com/ Javlin Inc.]
| released               = 2002
| latest_release_version = 3.5.0
| latest_release_date    = January 2014
| operating_system       = [[Cross-platform]]
| genre                  = [[:Category:ETL tools|ETL tools]]
| license                = dual [[GNU Lesser General Public License|LGPL]], commercial
| website                = http://www.cloveretl.com/
}}
{{Portal|Free software}}

'''CloverETL''' is a [[Java (programming language)|Java]]-based [[data integration]] framework designed to transform, cleanse, and distribute data into applications, databases, and [[Data Warehouse|data warehouses]]. A family of products that starts with an open source runtime engine, CloverETL's commercial offerings include a fully featured Designer and Server platform. The Server adds automation and workflow orchestration, allowing customers to deploy full production environments, with the possibly to scale to a cluster for added performance and robustness. Its goal is to be flexible and light-footed, so that it can be customized and embedded into third party applications. The [[Open Source|open source]] and commercial products are developed and supported by Javlin, a data integration software and solutions provider.

Javlin's offices are located in the Washington DC area; London, UK; and Prague, Czech Republic and serve customers in North America, Europe, Asia, and Australia. With approximately 60 employees, Javlin serves more than 3,000 customers, including five [[OEM]] partners.&lt;ref name=Topsy&gt;Topsy. N.p., n.d. Web. 20 June 2013. &lt;http://topsy.com/s/cloveretl&gt;.&lt;/ref&gt; Parts of the CloverETL platform – the Engine, Designer, and Server – can be embedded on an OEM basis.

Customers include [[Oracle Corporation|Oracle]], [[International Business Machines Corporation|Initiate Systems/IBM]], [[Comcast]], [[SUNY]], and other Fortune 500 companies.

== History ==

In 2002, the CloverETL project – named jETeL – was launched as the first Java-based open source ETL tool.{{citation needed|date=September 2013}}&lt;!-- need source for &quot;first&quot; --&gt; In 2006, it was renamed to clover.ETL, followed by CloverETL, now a registered trademark, in 2009. Starting out as a proof of concept, its purpose was to bring the performance and functionality of big enterprise [[Extract, transform, load|ETL tools]] to regular users who, at the time, did not have access to enterprise-level systems. Over time, it evolved into a data integration toolset ranging from the original core library (CloverETL Engine) to a full-fledged enterprise platform.

The CloverETL Engine is offered for free under [[GNU Lesser General Public License|LGPL]] with vendor support for the open-source ETL community.&lt;ref name=Elucidates&gt;Roy, Krishna. &quot;Javlin Elucidates CloverETL Strategy as It Continues to Take Aim at Data Integration.&quot; MIS Impact Report (2013): 1–4.&lt;/ref&gt; In 2010, a visual data transformation designer was also made public for free use.

Javlin, the official developer and support of CloverETL, was founded in 2005 under the name “Javlin Consulting”. The company’s founder and president, David Pavlis, is also the creator of CloverETL.

== Architecture ==

CloverETL is a [[Java (programming language)|Java]]-based ETL tool with [[Open Source|open source]] components. It is either used in standalone mode – as a command-line or server application – or embedded in other applications – as a Java library.  CloverETL is accompanied by the CloverETL Designer [[graphical user interface]] available as either an [[Eclipse (software)|Eclipse]] plug-in or standalone application.

A [[data transformation]] in CloverETL is represented by a transformation dataflow, or graph, containing a set of interconnected components joined by edges. A component can either be a source (reader), a transformation (reformat, sort, filter, joiner, etc.) or a target (writer). The edges act as pipes, transferring data from one component to another. Each edge has a certain metadata assigned to it that describes the format of the data it transfers. The transformation graphs are represented in  [[XML]] files and can be dynamically generated.

Each component runs in a separate [[Thread (computer science)|thread]] and acts either as a consumer or a producer. This is used to drive data through the transformation for both simple and complex graphs and makes the platform extendable by building custom components, connections etc. Transformation graphs can then be combined into a jobflow, which defines the sequence in which the individual graphs are executed.

===Fundamental aspects===
* ''Java based'' – supported platforms include [[Windows]], [[Unix]], [[Linux]], [[OS X]] and others
* ''Visual design'' – data transformations are designed visually in the CloverETL Designer (based on [[Eclipse java]])
* ''XML-based resources'' – resources such as graphs, connections, [[metadata]], etc. are stored in XML format
* ''Engine based'' – deploy a data transformation engine that executes transformation prescriptions
* ''CloverETL Transformation Language (CTL) ''–  A data-oriented programming language used to define business logic for data transformations. Offers direct access to data and functions. Syntax highlighting,    code assist, and automatic code generation included.
* ''Performance'' – utilizes multiple [[Central processing unit|CPUs]]/cores and can run on a cluster of computers to increase performance – see [[Massively parallel (computing)]]
* ''Transaction-oriented setups'' – Web-services, [[Service Oriented Architecture|SOA]], [[Enterprise service bus|ESB]]

The Server version of CloverETL supports parallel execution of transformations and runs inside a JavaEE [[Web container|application container]].

== Suite of Products ==

* ''[http://sourceforge.net/projects/cloveretl/ CloverETL Engine]'' – the core for running data transformation graphs- available under LGPLv2 or commercial license (consulting)
* ''CloverETL Designer'' – a commercial visual data integration tool for standalone or enterprise, used to design and execute transformation graphs
* ''CloverETL Server'' – an enterprise automation and monitoring data integration platform. Offers features such as workflows, scheduling, monitoring, user management, or real-time ETL abilities.
* ''CloverETL Cluster'' – an offering for big data, parallel data processing, and robustness – uses a pipeline for parallel data processing

=== Extensions ===

* ''CloverETL Data Profiler''– a data profiling extension for data quality tasks and assessing the current condition of data quality
* ''Event Analyzer (CEP)''– an extension that provides a toolset for processing data based on events such as log records, transactions, measurements, etc. Developed by MycroftMind.

Open Source solutions typically appeal to [[independent software vendor]]s (ISVs) and [[systems integrator]]s (SIs) who see these solutions as attractive alternatives to writing code.&lt;ref name=Vendors&gt;&quot;Data Integration Vendors.&quot; Adeptia. N.p., n.d. Web. 20 June 2013. &lt;http://www.adeptia.com/products/etl_vendor_comparison.html&gt;.&lt;/ref&gt; Products can be embedded into solutions for [[Enterprise Service Bus]] (ESB), [[Business Intelligence]] (BI), etc.&lt;ref name=GoodData&gt;&quot;GoodData Selects CloverETL to Enrich Data Integration – GoodData.&quot; GoodData. N.p., 6 December 2012. Web. 20 June 2013. &lt;http://www.gooddata.com/in-the-news/gooddata-selects-cloveretl-to-enrich-data-integration/&gt;&lt;/ref&gt;
&lt;ref name= &quot;Research&quot;&gt;Wang, Qian. &quot;Research of ETL on University Data Exchange Platform.&quot; IEEE Xplore. N.p., n.d. Web. 20 June 2013. &lt;http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&gt;.&lt;/ref&gt;

CloverETL is embedded in the [[Oracle Corporation|Oracle Endeca Information Discovery Integrator]] as well as [[GoodData]] CloudConnect&lt;ref name=GoodData /&gt;&lt;ref name=OIBEE&gt;&quot;Oracle Endeca Information Discovery- CloverETL.&quot; OBIEE, Endeca and ODI. N.p., 25 Oct. 2012. Web. 20 June 2013. &lt;http://www.varanasisaichand.com/2012/10/oracle-endeca-information-discovery.html&gt;.&lt;/ref&gt;&lt;ref name=Oracle&gt;&quot;Introduction – Oracle Identity Analytics Business Administrator's Guide.&quot; Oracle. N.p., n.d. Web. 20 June 2013. &lt;http://docs.oracle.com/cd/E27119_01/doc.11113/e23124/businessadministratorsguideprintable23.html&gt;.&lt;/ref&gt;&lt;ref name=Endeca&gt;&quot;Endeca – Information Discovery Integrator (CloverETL).&quot; GerardNicocom Weblog RSS. N.p., n.d. Web. 20 June 2013. &lt;http://gerardnico.com/wiki/cloveretl/cloveretl&gt;.&lt;/ref&gt;

===CloverETL Community Edition===
The CloverETL Community Edition is based on the Open Source transformation engine and also includes a limited CloverETL Designer. It is for users with modest data transformations and ETL requirements. The CloverETL Community Edition is free. The current version of CloverETL Community comes with a Graphic User Interface (GUI). In the past, the Community Edition used a command line style prompt to create and design data management projects.

CloverETL Community is Java-based and has been deployed on the following Operating System platforms: Linux both 32 &amp; 64 bit), Windows (both 32 &amp; 64 bit), HP-UX, AIX, AS/400 (IBM System I), Solaris, and Mac OS X. The Community edition contains connectors for the following data sources: text file delimited, fix-length and combined, XML, XLS, RDBMS through JDBC, WebServices through REST/SOAP protocols, JMS, LDAP, dBase/FoxBase/FoxPro, bulk-loaders for Oracle, DB2, MS SQL, Informix, MySQL and PostgreSQL, and QuickBase.&lt;ref&gt;Gutierrez, Jeremiah, Kent Lawson, Eddie Molina, Nestor Rodriguez. “Data Warehousing Tool Evaluation – ETL Focused.&quot; Southwest Decision Sciences Institute. 2012. 8-9. &lt;http://www.swdsi.org/swdsi2012/proceedings_2012/papers/Papers/PA151.pdf&gt;&lt;/ref&gt;

With the Community Edition, users have access to the transformation components that allow them to accomplish common data transformations tasks such as reformatting, filtering, and sorting data. Users also can use available components for aggregating, merging, or deduplicating data. The CloverETL Community Edition provides the Hash Join component and allows use of the DBExecute, System Execute, and HTTPConector components as well.

== Partners ==

* GoodData
* [[IBM]]
* [[Oracle Corporation|Oracle]]
* [[MuleSoft]]
* AddressDoctor
* MycroftMind
* DataMotion

== Technical specifications ==

* [[Java (programming language)|Java]]/[[JavaEE]]/[[Eclipse (software)|Eclipse]] (Java 6+)
* Supported platforms
* Windows 32/64
* Linux 32/64
* Mac OS X (64)
* HP-UX
* AIX
* AS/400
* Solaris
* Embeddable as a library or service
* Parallel data processing / bulk &amp; transaction processing

=== Connectors ===

* CSV and text files delimited, fix-length &amp; combined
* [[XML]], large XML files support
* XLS/XLSX ([[MS Excel]])
* Most [[RDBMS]] through [[JDBC]]
* WebServices through [[XML]]/[[JSON]] protocols
* [[JMS]]
* [[LDAP]], [[Lotus Notes]]
* dBase/FoxBase/[[FoxPro]]
* bulk-loaders for Oracle, DB2, MS SQL, Informix, MySQL and PostgreSQL
* [[QuickBase]] ''(by Intuit)'', [[Infobright]]
* Supports remote reading/writing through FTP/SFTP/HTTP/HTTPS protocols and also from ZIP/GZIP/TAR archives

== Competitors ==

Other ETL frameworks include:&lt;ref name=Vendors /&gt;

* [[Ab Initio (company)|Ab Initio]]
* [[Apatar]]
* [[Talend Open Studio]]
* [http://www.expressor-software.com/ Expressor]
* [http://www.enhydra.org/tech/octopus/index.html Enhydra Octopus] (launches from web browser via [[Java Web Start]])
* [[Pentaho Data Integration]]
* [[Informatica]]

== References ==

{{Reflist|35em}}

== External links ==
* [http://www.cloveretl.com/ CloverETL website]
* [http://www.endeca123.com/category/data-integratorclover-etl/ Oracle Endeca123 blog] embedded CloverETL

{{DEFAULTSORT:Cloveretl}}
[[Category:ETL tools]]
[[Category:Big data]]
[[Category:Data warehousing products]]</text>
      <sha1>04k5viiakxb2rie2meeuhoi65b4ef7c</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Compuverde</title>
    <ns>0</ns>
    <id>35236254</id>
    <revision>
      <id>601855918</id>
      <parentid>576183474</parentid>
      <timestamp>2014-03-29T19:27:28Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes + other fixes using [[Project:AWB|AWB]] (10067)</comment>
      <text xml:space="preserve" bytes="2700">{{multiple issues|
{{Orphan|date=June 2012}}
{{advert|date=October 2013}}
}}

'''Compuverde''' is an IT company with a focus on [[big data]] storage. The company was founded by Stefan Bernbo in 1994 and still has, even though they act on the global arena, it’s headquarters are the southeastern part of [[Sweden]].&lt;ref&gt;http://www.trademarkia.com/compuverde-85518126.html&lt;/ref&gt;&lt;ref&gt;http://compuverde.com/&lt;/ref&gt; 
As a provider of big data storage, Compuverde is a member of the worldwide nonprofit association [[Storage Networking Industry Association|SNIA]] (Storage Networking Industry Association) which aims to work towards the goal of promoting acceptance, deployment, and confidence in storage-related architectures, systems, services, and technologies across IT and business communities.&lt;ref&gt;http://snia.org/&lt;/ref&gt;

Compuverde’s executive chairman Mikael Blomqvist is a Swedish entrepreneur and apart from his mission at Compuverde Mikael is a board member of [[Blekinge Institute of Technology]], Krigskassan and Sparbanken Kronan.&lt;ref&gt;http://www.bth.se/info/pressreleaser.nsf/sidor/2f454db5425ce3c6c125770e0035f350?OpenDocument&lt;/ref&gt;&lt;ref&gt;http://www.sparbanksstiftelsenkronan.se/website1/1.0.1.0/8/1/&lt;/ref&gt; In 1989 Mikael Blomqvist founded the cable insulation producer [[Roxtec]] where he also acted as CEO.&lt;ref&gt;http://www.roxtec.com/fileadmin/InfoBase/global/documents/Press_information_Roxtec.pdf&lt;/ref&gt;

January 2012 Compuverde, [[Blekinge Institute of Technology]] and [[Ericsson]] received recognition from the Development of Knowledge and Competence (KK-stiftelsen) in Sweden for a joint venture project on big data storage and cloud computing.&lt;ref&gt;http://compuverde.com/news-room/press-and-media/millions-to-the-cloud/&lt;/ref&gt;&lt;ref&gt;http://www.bltsydostran.se/nyheter/karlskrona/telecom-city-och-bth-forbereder-sig(3161504).gm&lt;/ref&gt;

Compuverde’s software platform enables redundant object storage using cluster of standardized servers to store 100+ [[petabytes]] of accessible data.

* Runs on commodity hardware.
* Linear scaling for extreme storage needs.
* Designed for both cloud services and corporate use.
* Hardware independent symmetric architecture &lt;ref&gt;http://compuverde.com/&lt;/ref&gt;

{| class=&quot;wikitable&quot;
|-
! Compuverde !!
|-
| Type || Privately held 
|-
| Industry || IT
|-
| Type of mark   || Service Mark 
|-
| Founded  || 1994
|-
| Founder || Stefan Bernbo
|-
| Headquarters  || Karlskrona, Blekinge, Sweden 
|-
| Area served   || Worldwide 
|-
| Key people || Stefan Bernbo (Founder,CEO,Chairman) 
Mikael Blomqvist (Executive Chairman) 
|}

==References==
&lt;references /&gt;

[[Category:Companies established in 1994]]
[[Category:Companies of Sweden]]
[[Category:Big data]]</text>
      <sha1>1takl2knjql5lg703lsb4unnru4o0dz</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Continuuity</title>
    <ns>0</ns>
    <id>37988515</id>
    <revision>
      <id>578184510</id>
      <parentid>572865730</parentid>
      <timestamp>2013-10-21T23:30:34Z</timestamp>
      <contributor>
        <ip>50.240.223.122</ip>
      </contributor>
      <comment>Needed a facelift, mostly old stuff</comment>
      <text xml:space="preserve" bytes="5279">
{{Infobox company
| name = Continuuity
| type = [[Privately held company]]
| logo =
| industry = [[Big Data]], [[Cloud Computing]]
| foundation = 2011
| founders = Todd Papaioannou, Jonathan Gray, Nitin Motgi
| location = [[Palo Alto, California]], [[United States|USA]]
| key_people = {{nowrap begin}}Jonathan Gray &lt;small&gt;(Co-founder and CEO)&lt;/small&gt;{{wrap}}Nitin Motgi &lt;small&gt;(Co-founder)&lt;/small&gt;{{wrap}}Tom Leonard &lt;small&gt;(Sales)&lt;/small&gt;{{wrap}}Vikram Bhan &lt;small&gt;(Operations)&lt;/small&gt;{{wrap}} Don Mosites &lt;small&gt;(User Experience)&lt;/small&gt;{{wrap}}Andreas Neumann &lt;small&gt;(Software)&lt;/small&gt;{{wrap}}
| num_employees = 24
| revenue =
| net_income =
| products = Continuuity Reactor&lt;sup&gt;TM&lt;/sup&gt;
| homepage = [http://www.continuuity.com// www.continuuity.com]
| footnotes =
}}

'''Continuuity''' is a cloud-based [[Big Data]] application platform for developers. Rather than simply providing another cloud service for writing and running [[Hadoop]] jobs, Continuuity allows developers to more easily build, deploy and manage Big Data applications on top of the components within the Hadoop ecosystem.&lt;ref&gt;http://gigaom.com/data/ex-yahoo-facebook-big-data-vets-launch-paas-for-hadoop/&lt;/ref&gt;

Packaged in Single-node, Hosted Private cloud, On-premise Private cloud and Public Cloud Editions, Continuuity has built a cloud-based application runtime platform called the Continuuity Reactor. The company also provides an SDK and suite of developer tools that promise &quot;push-button deployment&quot; from the developer's local single node Reactor to the cloud.&lt;ref&gt;http://www.drdobbs.com/cloud/big-data-was-a-data-play-now-its-a-devel/240012595&lt;/ref&gt;

==Product==

The company calls its product the Continuuity Reactor, and it’s essentially a runtime for scalable Big Data applications. There are four core capabilities that comprise the Reactor:&lt;ref&gt;http://gigaom.com/data/ex-yahoo-facebook-big-data-vets-launch-paas-for-hadoop/&lt;/ref&gt;
*Streams – allow users to send and store their data into the platform
*Flows – allow users to process their data in real-time 
*Datasets – data and tables that store and index the output of applications that run in the Flows
*Procedures – allow users to access their data directly or write and deploy their own server-side query processors

These components sit on the Distributed App Fabric and Data Fabric that enables reading and writing to the underlying Hadoop infrastructure.

Continuuity Reactor Development Kit allow developers to build applications in their IDE, run, test and debug them on their local machines, and when ready, &quot;push to cloud&quot; with a single click.&lt;ref&gt;http://www.datanami.com/datanami/2012-10-27/hadoop_world_special:_this_week_s_big_data_top_ten.html?page=8&lt;/ref&gt;  

Continuuity has simplified the entire Big Data application development lifecycle.&lt;ref&gt;http://devopsangle.com/2012/10/24/continuuity-launches-industrys-first-big-data-application-fabric/&lt;/ref&gt;

The platform currently supports Java applications only — a natural first step given that Hadoop is written in Java — but Continuuity plans to support other languages going forward.&lt;ref&gt;http://gigaom.com/data/ex-yahoo-facebook-big-data-vets-launch-paas-for-hadoop/&lt;/ref&gt;

From an integration perspective, Continuuity supports REST interfaces.&lt;ref&gt;http://www.enterpriseappstoday.com/business-intelligence/former-yahoo-cloud-chief-aims-to-ease-hadoop-development.html&lt;/ref&gt;

==Leadership==

Continuuity was started in 2011 by a team with deep domain expertise when it comes to Big Data,&lt;ref&gt;http://www.forbes.com/sites/davefeinleib/2012/10/23/big-data-startup-continuuity-comes-out-of-stealth-mode/&lt;/ref&gt; including Todd Papaioannou, former chief cloud architect at [[Yahoo]] and entrepreneur-in-residence at [[Battery Ventures]], and Jonathan Gray, founder of personalized news streaming startup Streamy and more recently a top engineer at [[Facebook]], where he worked on the company’s Messages product.&lt;ref&gt;http://pandodaily.com/2012/10/23/continuuity-comes-out-of-stealth-promising-big-data-for-small-developers/&lt;/ref&gt;

Papaioannou, along with cofounders Nitin Motgi and Jonathan Gray, were reportedly early adopters of the Hbase big data technology while working at companies such as Facebook and Yahoo.&lt;ref&gt;http://adtmag.com/articles/2012/10/24/continuuity-targets-big-data-developers.aspx&lt;/ref&gt;

==Funding==
The startup received $2.5 million in seed funding in January 2012 from Battery Ventures, Andreessen Horowitz, Ignition Partners, and a group of angel investors&lt;ref&gt;http://pandodaily.com/2012/10/23/continuuity-comes-out-of-stealth-promising-big-data-for-small-developers/&lt;/ref&gt; On November 14, 2012, Continuuity, raised $10 million in a Series A round. The investors include Battery Ventures, Ignition Partners, Andreessen Horowitz, Data Collective and Amplify Partners.&lt;ref&gt;http://www.forbes.com/sites/tomtaulli/2012/11/14/continuuity-nabs-10m-to-drive-an-app-economy-for-big-data/&lt;/ref&gt;

==See also==
* [[Platform as a Service]]
* [[Big Data]]

==References==
{{Reflist}}

==External links==
* [http://www.continuuity.com// Continuuity Corporate Site ]

[[Category:Big data]]
[[Category:Information technology companies of the United States]]
[[Category:Privately held companies based in California]]
[[Category:Companies established in 2011]]</text>
      <sha1>8oa33fqoj0sp44e8epbiq8o8ju81bqe</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Flytxt</title>
    <ns>0</ns>
    <id>36854280</id>
    <revision>
      <id>598936377</id>
      <parentid>593758525</parentid>
      <timestamp>2014-03-10T04:25:38Z</timestamp>
      <contributor>
        <ip>202.88.237.119</ip>
      </contributor>
      <text xml:space="preserve" bytes="4184">{{Infobox company
|  company_name     = Flytxt 
|  company_type     = Private Ltd
|  company_logo     = [[File:Flytxt_Logo.png]]
|  company_slogan   = DATA&gt;&gt;&gt;&gt;ECONOMIC VALUE
|  foundation       = 2007
|  location         = [[Thiruvananthapuram]]
|  key_people       = [[Vinod Vasudevan]], Group CEO
|  industry         = [[Telecommunications]], [[Mobile marketing]], [[Mobile advertising]]
|  products         = NEON&lt;br /&gt;
QREDA
|  num_employees    = ~200
|  homepage         = [http://www.flytxt.com/ www.flytxt.com], 
}}

'''Flytxt ''' is a [[software]] product company that specializes in [[Big data|Big Data]] Analytics enabled advanced [[mobile marketing]] and [[mobile advertising|advertising]] for [[Telecom]] industry.&lt;ref&gt;http://www.ciol.com/News/News-Reports/Flytxt-strengthens-focus-on-mobile-marketing/241109128070/0/&lt;/ref&gt;&lt;ref&gt;{{cite web|title=‘Made in India’ software is all set to rule the world|url=http://www.deccanchronicle.com/131104/news-businesstech/article/%E2%80%98made-india%E2%80%99-software-all-set-rule-world|publisher=Deccan Chronicle}}&lt;/ref&gt; The company has its headquarters  in [[Amsterdam]], [[Netherlands]], corporate office in [[Dubai]] and global delivery centres at [[Thiruvananthapuram]] and [[Mumbai]] in  [[India]]. The company has presence in [[London]], [[Kuala Lampur]], [[Lagos]], [[Nairobi]], [[Dhaka]] and [[New Delhi]].

Flytxt has operations in [[South Asia]] and [[Africa]].&lt;ref&gt;http://www.thehindubusinessline.com/industry-and-economy/info-tech/article3832145.ece&lt;/ref&gt; [[Vinod Vasudevan]] is the Group [[Chief Executive Officer|CEO]] of Flytxt.&lt;ref&gt;http://www.siliconindia.com/shownews/Flytxt_appoints_Vinod_V_Vasudevan_as_the_group_CEO-nid-51045-cid-2.html&lt;/ref&gt;

==Products==
Flytxt has two products in the mobile marketing business domain, NEON&lt;ref&gt;http://www.moneycontrol.com/news/business/flytxt-launches-neon-mobile-platformindia_326599.html&lt;/ref&gt; and QREDA.&lt;ref&gt;http://www.indiainfoline.com/Markets/News/Flytxt-launches-QREDA/5054116760&lt;/ref&gt; 

==Achievements==
*Flytxt NEON's entry to Exemplar quadrant of NASSCOM's Product Excellent Matrix for marketing analytics applications, 2013.&lt;ref&gt;{{cite news|title=Flytxt platform gets into NASSCOM Product Excellence Matrix|url=http://keralaitnews.com/1327/flytxt-platform-gets-into-nasscom-product-excellence-matrix|newspaper=Kerala IT News}}&lt;/ref&gt; 
*Aegis Graham Bell award, 2013&lt;ref&gt;{{cite web|title=Flytxt wins Graham Bell award|url=http://www.thehindubusinessline.com/industry-and-economy/info-tech/flytxt-wins-graham-bell-award/article5335600.ece|publisher=The Hindu Business Line}}&lt;/ref&gt; 
* Cool Vendor in Emerging Markets, 2013 by [[Gartner]]&lt;ref&gt;http://articles.timesofindia.indiatimes.com/2013-05-03/telecom/39008570_1_big-data-analytics-gartner-flytxt&lt;/ref&gt;
* Winner, Red Herring Asia Top 100 companies, 2012&lt;ref&gt;http://www.redherring.com/red-herring-asia/2012-asia-top-100/&lt;/ref&gt;
* Winner, [[Institute of Electrical and Electronics Engineers]] Cloud Computing Challenge (C3) at [[Athens]], [[Greece]], 2011&lt;ref&gt;http://www.thehindubusinessline.com/industry-and-economy/info-tech/article2698511.ece&lt;/ref&gt;
* Finalist, Top 50 Innovative Companies, [[NASSCOM]] Emerge List, 2011&lt;ref&gt;http://www.thehindu.com/news/cities/Thiruvananthapuram/article2530648.ece&lt;/ref&gt;
* Finalist, Asia Communication Awards, 2012

==Corporate social responsibility==
Flytxt has an in-house staff club named ''Konnekt'' that organises charity activities and staff welfare programs.&lt;ref&gt;{{Cite web|url = http://www.thehindu.com/features/metroplus/joyful-times/article5478384.ece|title = Joyful times}}&lt;/ref&gt;

==See also==
*[[Intent marketing]]
* [[Big data|Big Data]] 
* [[Mobile advertising|Mobile Advertising]]

==References==
{{reflist}}

==External links==
*[http://www.flytxt.com Official website]
*[http://www.financialexpress.com/news/india-is-now-a-hot-market-for-mobiles/707959/ Interview with Flytxt CEO]

{{DEFAULTSORT:Flytxt Mobile Solutions}}
[[Category:Software companies of India]]
[[Category:CRM software companies]]
[[Category:Indian  companies established in 2007]]
[[Category:Big data]]
[[Category:Companies based in Thiruvananthapuram]]
[[Category:Companies of the Netherlands]]</text>
      <sha1>9izrqyeimq6o9l8kaitkfczxe6ga9xd</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Greenplum</title>
    <ns>0</ns>
    <id>20294190</id>
    <revision>
      <id>582293091</id>
      <parentid>582290646</parentid>
      <timestamp>2013-11-19T00:21:02Z</timestamp>
      <contributor>
        <username>W Nowicki</username>
        <id>9177019</id>
      </contributor>
      <comment>sigh, most of the urls are dead now; one more from archive.org</comment>
      <text xml:space="preserve" bytes="15912">{{multiple issues|
{{advert|date=July 2012}}
{{Refimprove|date=April 2007}}
}}

{{Infobox company|
  name   = Greenplum  |
  logo   = [[Image:greenplumlogotype.jpg|200px]] |
  type   = Division of [[GoPivotal]] |
  foundation     = 2003 |
  location       = [[San Mateo, California|San Mateo]], [[California]], [[United States]] |
  industry       = [[Big Data]] technologies|
  products       = Unfied Analytics Platform (UAP), Database Software, Chorus Software, Enterprise-Ready Hadoop, Data Computing Appliance (DCA), Analytics Labs |
  homepage       =  &lt;!-- {{URL |www.greenplum.com}} redirect now to GoPivotal --&gt;
}}

'''Greenplum''' was a [[big data]] analytics company headquartered in [[San Mateo, California|San Mateo]], [[California]].&lt;ref&gt;{{cite web|url=http://techcrunch.com/2012/11/30/paul-maritz-to-lead-new-group-at-emc-that-merges-greenplum-with-vmwares-cloud-foundry-springsource-and-gemstome/|title=Paul Maritz To Lead New Group At EMC That Merges Greenplum With VMware’s Cloud Foundry, SpringSource, And Gemstome |publisher=Tech Crunch|accessdate=4 October 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://gigaom.com/2013/02/25/emc-to-hadoop-competition-see-ya-wouldnt-wanna-be-ya/|title=EMC to Hadoop competition: “See ya, wouldn’t wanna be ya.”|publisher=Gigaom|accessdate=4 October 2013}}&lt;/ref&gt;
Greenplum's products include its Unified Analytics Platform, Data Computing Appliance, Analytics Lab, Database, HD and Chorus. Greenplum was acquired by [[EMC Corporation]] in July 2010,&lt;ref name=&quot;acquired&quot;&gt;
{{cite news 
| url = http://www.emc.com/about/news/press/2010/20100706-01.htm
| title = EMC to Acquire Greenplum |work= Press release
| date = July 6, 2010
| accessdate  = 2012-07-05
}}
&lt;/ref&gt; and then became part of [[GoPivotal]] in 2012.&lt;ref name=&quot;pivot&quot;&gt;{{cite news |url= http://www.bizjournals.com/sanjose/blog/2012/12/paul-maritz-to-run-emcs-new.html |title= Paul Maritz to run EMC's new Greenplum, Pivotal Labs mashup |work= Silicon Valley Business Journal |author= Cromwell Schubarth |date= December 4, 2012 |accessdate= November 18, 2013 }}&lt;/ref&gt; 

== Company ==
Greenplum was founded in September 2003 by Scott Yara and Luke Lonergan.&lt;ref&gt;
{{cite web 
|url = http://www.greenplum.com/about-us/management-team
|title = Management Team |work= Old company web site
|accessdate  = November 18, 2013
|deadurl=yes |archivedate= August 13, 2008
|archiveurl= http://web.archive.org/web/20080813021921/http://www.greenplum.com/about-us/management-team
}}
&lt;/ref&gt;
It was a merger of two smaller companies Metapa in [[Los Angeles]] and Didera in [[Fairfax, Virginia]].&lt;ref&gt;{{Cite news |title= Metapa Buys Didera |author= Maureen O'Gara |date= September 26, 2003 |work= Linux Business News |url= http://www0.cloudcomputingexpo.com/node/35438 |accessdate= November 18, 2013 }}&lt;/ref&gt;
Investors included SoundView Ventures, Hudson Ventures and Royal Wulff Ventures. A total of $20 million in funding was announced at the merger.&lt;ref&gt;{{Cite news |title= Metapa Acquires Didera and Closes Additional Funding; Industry Pioneers in High-Performance Computing Combine to Create Breakthrough Linux Database Clustering Solution for Decision Support |work= Press release |date= September 23, 2003 |url= http://www.businesswire.com/news/home/20030923005198/en/Metapa-Acquires-Didera-Closes-Additional-Funding-Industry |accessdate= November 18, 2013 }}&lt;/ref&gt;
Greenplum, based in  in [[San Mateo, California]], released its [[database management system]] software in April 2005 calling it Bizgres.&lt;ref&gt;{{Cite news |title= Greenplum Unveils the Bizgres Project |work= Press release |date= April 18, 2005 |url= http://www.greenplum.com/press_041805-bizgres.html |deadurl=yes |archiveurl= http://web.archive.org/web/20051103072101/http://www.greenplum.com/press_041805-bizgres.html |archivedate= November 3, 2005 |accessdate= November 18, 2013 }}&lt;/ref&gt;
In July 2006 a partnership with [[Sun Microsystems]] was announced.&lt;ref name=&quot;history&quot; /&gt;
Greenplum was acquired by [[EMC Corporation]] in July 2010,&lt;ref name=&quot;acquired&quot;/&gt; becoming the foundation of EMC's Big Data Division.
Its [[computer appliance]] was announced in October 2010.
In 2011 Greenplum announced more products and services. 
In May 2012 Greenplum released its Analytics Workbench, and in October 2012 Chorus.&lt;ref name=&quot;history&quot;&gt;{{cite web |title= About Greenplum: History |url= http://www.greenplum.com/about-us |work= Company website |deadurl=yes |archiveurl= http://web.archive.org/web/20121025213918/http://www.greenplum.com/about-us |archivedate= October 25, 2012 |accessdate= November 18, 2013 }}&lt;/ref&gt; 
In December 2012 it became part of a joint venture of EMC and [[VMware]] which took the name [[GoPivotal|Pivotal]] in March 2013.&lt;ref&gt;{{Cite web |title= The Pivotal Initiative, in case you were wondering, is now official |author= Barb Darrow |date= March 13, 2013 |work= GigaOm VMware blog |url= http://gigaom.com/2013/03/13/the-pivotal-initiative-in-case-you-were-wondering-is-now-official |accessdate= November 18, 2013 }}&lt;/ref&gt;&lt;ref name=&quot;pivot&quot; /&gt;

==Technology ==

The Greenplum Database builds on the foundations of [[open source]] database [[PostgreSQL]].&lt;ref&gt;{{cite web |url=http://intelligent-enterprise.informationweek.com/showArticle.jhtml;jsessionid=3IEDTLRVJ3LDLQE1GHOSKH4ATMY32JVN?articleID=206801311 |title=Greenplum Updates Open-Source Based Database |date=February 22, 2008 |last=Gonsalves |first=Antone}}&lt;/ref&gt; It primarily functions as a [[data warehouse]] and utilizes a [[shared-nothing architecture|shared-nothing]], [[massively parallel (computing)|massively parallel]] processing (MPP) architecture. In this architecture, data is partitioned across multiple segment servers, and each segment owns and manages a distinct portion of the overall data; there is no disk-level sharing nor data contention among segments.

Greenplum Database's parallel [[query optimizer]] converts each query into a physical execution plan.&lt;ref name=&quot;mania&quot;&gt;
{{cite web 
| url = http://www.technology-mania.com/2011/04/understanding-greenplum.html
| title = Understanding Greenplum
| accessdate  = August 7, 2012
}}
&lt;/ref&gt; Greenplum's optimizer uses a cost-based algorithm to evaluate potential execution plans, takes a global view of execution across the [[computer cluster]], and factors in the cost of moving data between nodes. &lt;ref name=&quot;mania&quot; /&gt; The resulting query plans contain traditional [[relational database]] operations as well as parallel &quot;motion&quot; operations that describe when and how data should be transferred between nodes during query execution.&lt;ref name=&quot;manual&quot;&gt;
{{cite web 
| url = http://bitcast-a.v1.sjc1.bitgravity.com/greenplum/Greenplum_CE_Database/documentation/4.2.1.0/greenplum_database_4.2_administrator_guide.pdf
| title = Greenplum Database Release 4.2.1.0 Administrator's Guide
| accessdate  = November 18, 2013  |date= February 17, 2012
}}
&lt;/ref&gt;
Commodity [[Gigabit Ethernet]] and [[10-gigabit Ethernet]] technology is used for the transfer between nodes.
During execution of each node in the plan, multiple relational operations are processed by [[Pipeline (computing)|pipelining]]:  the ability to begin a task before its predecessor task has completed, to increase effective parallelism. For example, while a table scan is taking place, rows selected can be pipelined into a join process.&lt;ref&gt;
{{cite web 
|url = http://www.greenplum.com/technology/gnet |deadurl=yes
|title = gNet Software Interconnect |work= Company web page
|accessdate= November 18, 2013  |archivedate= April 15, 2012
|archiveurl= http://web.archive.org/web/20120415143851/http://www.greenplum.com/technology/gnet
}}&lt;/ref&gt;

Internally, the Greenplum system utilizes log shipping and segment-level replication and provides automated failover. At the storage level, [[RAID]] techniques can mask disk failures. At the system level, Greenplum replicates segment and master data to other nodes to ensure that the loss of a machine will not impact the overall database availability.&lt;ref&gt;
{{cite web 
| url = http://www.greenplum.com/technology/fault-tolerance
| title = Multi-Level Fault Tolerance
| accessdate  = August 7, 2012
}}
&lt;/ref&gt;
In 2009 technology was announced to use parallel streams of data for [[extract, transform and load]] operations.
This technology is exposed to customers via a programmable &quot;external table&quot; interface and a traditional command-line loading interface.&lt;ref&gt;
{{cite web 
|url = http://www.zdnet.com/blog/gardner/greenplum-aims-to-eliminate-massive-data-load-choke-points-with-scattergather-technology/2866
|title = Greenplum aims to eliminate massive data load 'choke points' with Scatter/Gather technology |date= March 18, 2009 
|author= Dana Gardner |publisher= ZDNet |work= Briefings Direct blog
|accessdate  = November 18, 2013
}}&lt;/ref&gt;

In addition to traditional [[SQL|Structured Query Language]] (SQL), in 2008 support was announced for [[MapReduce]] queries within a parallel dataflow engine, to run analytics against datasets stored in and outside of the Greenplum Database.&lt;ref&gt;
{{cite web 
|url = http://www.prweb.com/releases/2008/08/prweb1248014.htm
|title = Greenplum Brings MapReduce to the Enterprise |work= Press release
|date= August 25, 2008 |accessdate = November 18, 2013
}}&lt;/ref&gt;&lt;ref&gt;
{{cite web 
|url = http://briefingsdirectblog.blogspot.com/2008/09/greenplum-delivers-mapreduce-and.html
|title = Greenplum pushes envelope with MapReduce and parallelism enhancements to its extreme-scale data offering |date= September 29, 2008 
|author= Dana Gardner |work= Briefings Direct blog
|accessdate  = November 18, 2013
}}&lt;/ref&gt;

For each table (or partition of a table), database administrators can select the storage, execution and compression settings that suit the way that table will be accessed. Greenplum DB transparently abstracts the details of any table or partition, allowing a variety of underlying models: traditional row-oriented tables, optimized for read-mostly scans and bulk append loads, or [[column-oriented DBMS|column-oriented]].&lt;ref&gt;
{{cite web 
|url= http://www.dbms2.com/2009/10/14/greenplum-hybrid-columnar/
|title= Greenplum is going hybrid columnar as well  |work= DBMS2 |date= October 14, 2009 
|accessdate= November 18, 2013
}}
&lt;/ref&gt;
Database administrators also can tune the storage types and compression settings of different partitions within the same table.&lt;ref&gt;
{{Cite news
|url= http://www.prweb.com/releases/2009/10/prweb3049904.htm |work= Press release
|title= Greenplum Adds Column-Oriented Table Feature to Greenplum Database |date= October 14, 2009
|accessdate = November 18, 2013
}}
&lt;/ref&gt;

Greenplum HD is a supported version of [[Apache Hadoop]]. It includes Hadoop's Distributed File System (HDFS), [[Apache Hive|Hive]], [[Pig (programming tool)|Pig]], [[HBase]], and [[Apache ZooKeeper|ZooKeeper]].&lt;ref&gt;
{{cite web 
| url = http://wikibon.org/blog/emc-marries-isilon-with-greenplum-hadoop-distribution-2/
| title = EMC Marries Isilon with Greenplum Hadoop Distribution
| accessdate  = August 7, 2012
}}
&lt;/ref&gt;
Greenplum Chorus is a social network portal for data science teams.&lt;ref&gt;
{{cite web 
| url = http://www.informationweek.com/news/development/database/232602911
| title = EMC Marries Social Networking And Big Data 
| accessdate  = August 7, 2012
}}
&lt;/ref&gt;

The Greenplum Data Computing Appliance (DCA) is a physical [[computer appliance]] to integrate structured data, unstructured data, and partner applications such as [[business intelligence]].&lt;ref&gt;
{{cite web 
|url= http://www.theregister.co.uk/2011/09/21/emc_greenplum_dca_appliance_update/
|title= Greenplum appliances swing both ways: Spinning up data warehouses and Hadoop |work= The Register
|date= September 21, 2011 |author= Timothy Prickett Morgan
|accessdate = November 18, 2013
}}
&lt;/ref&gt; A special version of DCA integrated with SAS software was released in April 2011.&lt;ref&gt;
{{cite web 
|url = http://www.zdnet.com/blog/btl/emc-greenplum-inks-sas-partnership-launches-new-appliances/46900
|title = EMC Greenplum inks SAS partnership, launches new appliances |work= Between the lines blog
|date= April 5, 2011 |author= Larry Dignan |accessdate= November 18, 2013
}}
&lt;/ref&gt;

Greenplum Command Center software displays interactive dashboards to collect performance metrics and manage system health for Greenplum products. Monitored data is also stored for historical reporting.&lt;ref&gt;
{{cite web 
| url = http://www.theregister.co.uk/2012/03/01/greenplum_database_4_2/ |work= The Register
| author= Timothy Prickett Morgan |date= March 1, 2012
| title = EMC cranks Greenplum database to 4.2: Goosing Hadoop links and warehouse backups
| accessdate  = November 18, 2013 
}}
&lt;/ref&gt;
Greenplum Analytics Lab was a data science consultation service, renamed Pivotal Data Labs in 2013.&lt;ref&gt;{{Cite web |title= Pivotal Data Labs: Becoming a Predictive Enterprise |work= Company web site |url= http://www.gopivotal.com/pivotal-data-labs |accessdate= November 18, 2013 }}&lt;/ref&gt;

Greenplum Database was supported for production use on [[SUSE Linux Enterprise Server]] 10.2 (64-bit), [[Red Hat Enterprise Linux]] 5.x (64-bit), [[CentOS]] Linux 5.x (64-bit) and Sun Solaris 10U5+ (64-bit). Greenplum Database was supported on server hardware from a range of vendors including HP, Dell, Sun and IBM.&lt;ref name=&quot;manual&quot;/&gt;
Greenplum Database was supported for non-production (development and evaluation) use on Mac OSX 10.5, Red Hat Enterprise Linux 5.2 or higher (32-bit) and CentOS Linux 5.2 or higher (32-bit).&lt;ref&gt;
{{cite web 
| url = http://www.greenplum.com/products/community-edition/
| title = Greenplum Database: Community Edition
| accessdate  = August 7, 2012
}}
&lt;/ref&gt;

Greenplum had customers in [[vertical market]]s from financial services, telecommunications, Internet, retail, transportation and pharmaceuticals industries.&lt;ref&gt;
{{cite web 
| url = http://www.dbms2.com/2009/04/30/ebays-two-enormous-data-warehouses/
| title = ebay's two enormous data warehouses
| accessdate  = August 7, 2012
}}
&lt;/ref&gt;  They included [[Silver Spring Networks]], [[Zions Bancorporation]], [[Reliance Communications]], [[NYSE Euronext]], [[Orbitz]], [[Havas Digital]], [[China Unicom]], and [[Tagged]].&lt;ref&gt;
{{cite web 
| url = http://www.greenplum.com/customers/our-customers/
| title = Our Customers
| accessdate  = August 7, 2012
}}
&lt;/ref&gt;

Greenplum provides a community edition of its database,  and community forums.&lt;ref&gt;{{cite web|title=Greenplum Communities|url=http://www.greenplum.com/communities/}}&lt;/ref&gt;

Greenplum DB has a limitation on indexing: Unique index and primary key index cannot be used at same time on a table.&lt;ref&gt;{{cite web|title=Greenplum db - may not be as ready as you think|url=http://it.toolbox.com/blogs/sap-on-db2/greenplum-db-may-not-be-as-ready-as-you-think-51834}}&lt;/ref&gt;

Partnerships included [[Impetus Technologies]],&lt;ref&gt;{{Cite web |title=Technology Partners|url=http://www.impetus.com/partners}}&lt;/ref&gt; [[Cisco]], [[Brocade Communications Systems]], [[SAS (software)]], [[Factual]], [[MicroStrategy]], and [[Informatica]].&lt;ref&gt;{{cite web |url=http://www.greenplum.com/partners/greenplum-partners |title=Greenplum Partners |accessdate=October 29, 2010 }}&lt;/ref&gt;

Competitors include [[IBM]], [[Exadata|Oracle Exadata]], [[Teradata]], [[Aster Data Systems]], [[Netezza]], [[SAP AG|SAP]], and [[Vertica]].&lt;ref&gt;
{{cite web 
|url = http://www.gartner.com/technology/reprints.do?id=1-196T8S5&amp;ct=120207&amp;st=sb |deadurl=yes
|title = Magic Quadrant for Data Warehouse Database Management Systems |date= February 6, 2012
|archiveurl= http://web.archive.org/web/20120225103158/http://www.gartner.com/technology/reprints.do?id=1-196T8S5&amp;ct=120207&amp;st=sb
|archivedate= February 25, 2012 |accessdate= November 18, 2013
}}
&lt;/ref&gt;

== See also ==

* [[Column-oriented database]]
* [[Vertica]]
* [[MapReduce]]
* [[Sybase IQ]]

==References==
{{reflist}}

{{EMC}}

[[Category:Business intelligence]]
[[Category:Data warehousing products]]
[[Category:Big data]]</text>
      <sha1>cz6xbfvj2jf40fs1ce66c869z5gmwnd</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>MarkLogic</title>
    <ns>0</ns>
    <id>2741545</id>
    <revision>
      <id>598543251</id>
      <parentid>595380116</parentid>
      <timestamp>2014-03-07T13:08:33Z</timestamp>
      <contributor>
        <ip>95.128.91.188</ip>
      </contributor>
      <comment>/* History */</comment>
      <text xml:space="preserve" bytes="18604">{{Infobox company |
  name   = MarkLogic Corporation|
  logo   = [[File:Marklogic-logo.PNG|221px|MarkLogic]]
|
&lt;/gallery&gt;
|
  type   = Private |
  foundation     = [[San Mateo, California]] (2001) |
  location       = San Carlos, California |
  key_people     = Gary Bloom, CEO
  Christopher Lindblad, co-founder |
  industry       = [[Software]]|
  products       = MarkLogic Essential Enterprise; MarkLogic Global Enterprise  |
  num_employees  = 250|
  homepage       = [http://www.marklogic.com/ www.marklogic.com]
}}
'''MarkLogic''' is an [[United States|American]] software business that makes a [[NoSQL]] database.&lt;ref&gt;{{cite web|url=http://www.bizjournals.com/sanjose/news/2013/04/15/top-5-biggest-fundings-in-silicon.html?page=all|title=Top 5: MarkLogic topped last week's Silicon Valley fundings|publisher=Silicon Valley Business Journal|accessdate=4 October 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://gigaom.com/2013/04/10/marklogic-nets-25m-to-keep-up-enterprise-nosql-pitch/|title=MarkLogic nets $25M to keep up enterprise NoSQL pitch|publisher=Gigaom|accessdate=4 October 2013}}&lt;/ref&gt; The company has its headquarters in [[Silicon Valley]] with field offices in [[Washington D.C.]], [[New York]], [[London]], [[Frankfurt]], [[Tokyo]], and [[Utrecht]].

==History==
The company was founded in 2001 by Christopher Lindblad,&lt;ref&gt;{{cite web|url=http://www.crunchbase.com/person/christopher-lindblad |title=Christopher Lindblad |publisher=CrunchBase}}&lt;/ref&gt; who was the Chief Architect of the Ultraseek search engine at Infoseek, and Paul Pedersen, a professor of computer science at [[Cornell University]]  and [[UCLA]], to address the emergence of [[XML]] as a document markup standard and [[XQuery]] as the standard means for accessing collections of documents up to hundreds of terabytes in size.&lt;ref&gt;{{cite web|url=http://newsbreaks.infotoday.com/NewsBreaks/MarkLogic--Introduces-Stable-of-New-Features-for-the-XML-Server-51064.asp |title=MarkLogic 4.0 Introduces Stable of New Features for the XML Server |publisher=Information Today |date= 2008-10-09}}&lt;/ref&gt;

In May 2012 Gary Bloom joined Mark Logic as Chief Executive Officer.&lt;ref&gt;{{cite news|last=Hoge|first=Patrick|title=Reporter|url=http://www.bizjournals.com/sanfrancisco/blog/2012/05/marklogic-appoints-gary-bloom-ceo.html|accessdate=17 May 2012|newspaper=San Francisco Business Times|date=17 May 2012}}&lt;/ref&gt; Prior to joining Veritas Software as CEO in 2000, Bloom held several senior positions at Oracle and was widely considered the successor to Larry Ellison.&lt;ref&gt;{{cite web|last=Foremski|first=Tom|title=Former senior Oracle exec Gary Bloom named CEO of Mark Logic|url=http://www.zdnet.com/blog/foremski/former-senior-oracle-exec-gary-bloom-named-ceo-of-mark-logic/2264|publisher=ZDnet|accessdate=17 May 2012}}&lt;/ref&gt;

MarkLogic is privately held with investments from [[Sequoia Capital]], [[Tenaya Capital]] and Northgate Capital.&lt;ref&gt;{{cite web|last=Joyce|first=Wells|title=MarkLogic Secures New $25 Million Investment and Targets Four Primary Product Areas|url=http://www.dbta.com/Articles/Editorial/News-Flashes/MarkLogic-Secures-New-$25-Million-Investment-and-Targets-Four-Primary-Product-Areas-88961.aspx|publisher=DBTA.com|accessdate=11 April 2013}}&lt;/ref&gt;

In 2012, MarkLogic was the vendor with the largest revenue for [[Apache Hadoop|Hadoop]]/NoSQL Software or Services, with 13% of total marketshare.&lt;ref&gt;{{cite web|last=Kelly|first=Jeff|title=Hadoop-NoSQL Software and Services Market Forecast 2012–2017|url=http://wikibon.org/wiki/v/Hadoop-NoSQL_Software_and_Services_Market_Forecast_2012-2017#Market_Share|accessdate=16 September 2013}}&lt;/ref&gt;

For the 2012 London Olympics, the [[BBC]] used MarkLogic to power its Olympic Data Services, an application that had to be built in 12 months. &quot;Given the timescales, this project would not have been achievable using a SQL database, which would have pushed the design towards more complete modeling of the data.&quot;&lt;ref&gt;{{cite web|last=Rogers|first=David|title=Building the Olympic Data Services|url=http://www.bbc.co.uk/blogs/internet/posts/olympic_data_xml_latency|publisher=BBC|accessdate=1 August 2012}}&lt;/ref&gt; BBC broke all traffic records during the 2-week games, 2.8 Petabytes on peak day, including more than 100m video requests.&lt;ref&gt;{{cite web|last=ORiordan|first=Cait|title=The story of the digital Olympics: streams, browsers, most watched, four screens|url=http://www.bbc.co.uk/blogs/internet/posts/digital_olympics_reach_stream_stats|publisher=BBC|accessdate=13 August 2012}}&lt;/ref&gt;

Since 1 October 2013, MarkLogic has been used to help power the U.S. government [http://healthcare.gov healthcare.gov] site.&lt;ref&gt;{{cite web|title=Tension and Flaws Before Health Website Crash|url=http://www.nytimes.com/2013/11/23/us/politics/tension-and-woes-before-health-website-crash.html|accessdate=26 November 2013}}&lt;/ref&gt;

==Technology==
MarkLogic is a [[Document-oriented database|NoSQL document database]] that has evolved from its [[XML database]] roots to embrace the &quot;enterprise NoSQL&quot; label. In addition to the distributed, scale-out architecture expected from a [[NoSQL]] database, it has role-based security features, JSON storage, direct use of Apache Hadoop Distributed File System (HDFS), multiple indexing strategies and ACID consistency.&lt;ref&gt;{{cite book|title=Who's Who in NoSQL DBMSs|year=2013|publisher=Gartner|author=Nick Heudecker|edition=G00252015|coauthors=Merv Adrian|date=23|month=August}}&lt;/ref&gt; It is the most popular native [[XML databases|XML DBMS]] as of April 2013.&lt;ref&gt;{{cite web|url=http://db-engines.com/en/ranking/native+xml+dbms |title=DB-Engines Ranking of Native XML DBMS |publisher=DB-Engines}}&lt;/ref&gt; The product combines a database, search engine and application services together in one platform.

MarkLogic features include replication, rollback, automated failover, point-in-time recovery, backup/restore, backup to Amazon S3, JSON, can run directly on Hadoop Distributed File System, parallelized ingest, role-based security, full text search, location services, geospatial alerting, RDF triple store and SPARQL query support.&lt;ref&gt;{{cite web|last=Feinberg|first=Donald|title=Magic Quadrant for Operational Database Management Systems|url=http://www.gartner.com/technology/reprints.do?id=1-1M1BXOP&amp;ct=131021&amp;st=sb|publisher=Gartner|coauthors=Merv Adrian, Nick Heudecker|date=21 October 2013}}&lt;/ref&gt;

MarkLogic Server is available under various licensing and delivery models. These were announced in October 2013:&lt;ref&gt;{{cite web|last=MacFadden|first=Gary|title=MarkLogic 7 Leads the NoSQL Class, Adding Semantics and Other Enhancements|url=http://wikibon.org/wiki/v/MarkLogic_7_Leads_the_NoSQL_Class,_Adding_Semantics_and_Other_Enhancements|publisher=Wikibon|accessdate=30 October 2013}}&lt;/ref&gt;

'''MarkLogic Developer:''' Free, full-featured version. Included API's extend to all versions of MarkLogic. Not for production use.&lt;ref&gt;{{cite web|title=Developer License - MarkLogic Developer Community|url=http://developer.marklogic.com/free-developer}}&lt;/ref&gt; &lt;br /&gt;
'''MarkLogic Essential Enterprise''': Full-featured Enterprise NoSQL database that includes search engine, replication, backup, high availability, recovery, fine-grained security, location services, and alerting. Semantics and advanced language packs are options. Available as perpetual license, term/yearly license or hourly on AWS.&lt;ref&gt;{{cite web|title=AWS Marketplace: MarkLogic Essential Enterprise|url=https://aws.amazon.com/marketplace/pp/B00FX8CM2I/ref=sp_mpg_product_title?ie=UTF8&amp;sr=0-2}}&lt;/ref&gt;&lt;br /&gt; 
'''MarkLogic Global Enterprise:''' Version designed for use for large, globally distributed applications. Semantics, tiered storage, geospatial alerting and advanced language packs are options.

==Sample applications==
MarkLogic's Enterprise NoSQL database platform is widely used in publishing, government, finance and other sectors, with some hundreds of large-scale systems in production. Below are some of the organizations using MarkLogic.

* MarkMail is a free public mailing list archive service that emphasizes interactivity and search analytics.&lt;ref&gt;{{cite web|last=O'Brien|first=Tim|title=Interview with Jason Hunter of MarkMail.org|url=http://broadcast.oreilly.com/2008/11/interview-with-jason-hunter-of.html|publisher=O'Reilly Community|accessdate=24 November 2008}}&lt;/ref&gt; Every search result shows a histogram traffic chart of the messages matching the query, and also the top matching lists and senders. MarkMail started in November 2007 with approximately four million email messages. As of 24 November 2013, the service claims inclusion of 66,058,071 messages across 8,761 lists, of which 2,975 were active lists.&lt;ref&gt;{{cite web|url=http://markmail.org/browse |title=MarkMail list of lists}}&lt;/ref&gt; The archive includes complete list histories for [[Apache HTTP Server|Apache]], [[FreeBSD]], [[GNOME]], [[Extensible Messaging and Presence Protocol|Jabber]], [[Java.net]], [[KDE]], [[Mozilla]], [[MySQL]], [[OpenOffice.org]], [[Perl|Perl.org]], [[PostgreSQL]], [[Python (programming language)|Python]], [[Red Hat]], [[Ruby (programming language)|Ruby]], [[W3C]], and [[Xen]], among others.
* [[American Lawyer Media]]—content management, processing, publication and repurposing&lt;ref&gt;{{cite web|title=Testimonial video: Editorial Content Consolidation and Delivery in a Digital First World|url=http://www.youtube.com/watch?v=PPWQAa5Pl_o|publisher=MarkLogic}}&lt;/ref&gt;
* [[American Psychological Association]]—semantic search, improved search speed, and accelerated delivery of new content&lt;ref&gt;{{cite web|title=APA PsycNET® Is Now Powered by MarkLogic|url=http://www.apa.org/pubs/databases/news/2012/03/marklogic.aspx}}&lt;/ref&gt;
* [[BBC]]—supports the site that reported the 2012 London Olympics&lt;ref&gt;{{cite web|title=BC and Press Association select MarkLogic to handle Olympics data|url=http://2012tech.v3.co.uk/live/MarkLogic—to—handle—Olympics—data.htm}}&lt;/ref&gt;
* [[Boeing]]—national security and intelligence applications&lt;ref&gt;{{cite web|last=Rudin|first=Rob|title=Boeing Develops National Security App on MarkLogic|url=http://www.youtube.com/watch?v=uqrnbAc_6cs}}&lt;/ref&gt;
* [[Centers for Medicare and Medicaid Services]]—powers the US [[Patient Protection and Affordable Care Act]] services including the [[Federal Data Services Hub]] and parts of Federally Facilitated Marketplace (healthcare.gov)&lt;ref&gt;{{cite web|last=Lipton, et al.|first=Eric|title=Tension and Flaws Before Health Website Crash|url=http://www.nytimes.com/2013/11/23/us/politics/tension—and—woes—before—health—website—crash.html?_r=0|accessdate=23 November 2013}}&lt;/ref&gt;
* [[The Church of Jesus Christ of Latter-day Saints]]—various websites with millions of monthly visitors&lt;ref&gt;{{cite web|last=MacFadden|first=Gary|title=MarkLogic 7 Leads the NoSQL Class, Adding Semantics and Other Enhancements|url=http://wikibon.org/wiki/v/MarkLogic_7_Leads_the_NoSQL_Class,_Adding_Semantics_and_Other_Enhancements|publisher=Wikibon|accessdate=30 October 2013}}&lt;/ref&gt;
* CQ Roll Call—a variety of applications designed to help search and discover US legistlation &lt;ref&gt;{{cite web|title=CQ Roll Call Takes Access to Unstructured Information to a New Level with MarkLogic|url=http://www.reuters.com/article/2011/02/15/idUS113970+15—Feb—2011+BW20110215|publisher=reuters|accessdate=15 February 2011}}&lt;/ref&gt;
* [[Dow Jones &amp; Company|Dow Jones]]—[[Factiva]] is planned to be the first of three services, followed by [[WSJ.com]] and [[Dow Jones Financial Services]]&lt;ref&gt;{{cite web|last=Quint|first=Barbara|title=Dow Jones Moving to MarkLogic Platform; Factiva First |url=http://newsbreaks.infotoday.com/NewsBreaks/Dow—Jones—Moving—to—MarkLogic—Platform—Factiva—First—86705.asp}}&lt;/ref&gt;
* [[Fairfax County, Virginia]]—property records management application&lt;ref&gt;{{cite web|title=Fairfax County land—use solution puts big data on the map|url=http://gcn.com/articles/2012/11/06/fairfax—county—land—use—big—data.aspx|accessdate=6 November 2012}}&lt;/ref&gt;
* [[Federal Aviation Administration]]—[[Emergency Operations Network]]&lt;ref&gt;{{cite web|last=Lawrence|first=Will|title=Interview: FAA, Will Lawrence|url=http://www.youtube.com/watch?v=lFlYBxYyOWI}}&lt;/ref&gt;
* MBS Digital Direct—digital publishing platform&lt;ref&gt;{{cite web|title=Power Through Reading Analytics—The Future of Publishing with MarkLogic|url=http://www.youtube.com/watch?v=Ac41zdYcnMM&amp;list=PL035BDAAC8BC4481F&amp;index=27}}&lt;/ref&gt;
* [[Informatics Corporation of America]]—enable healthcare providers to securely find and analyze patient information&lt;ref&gt;{{cite web|title=Informatics Corporation of America Revolutionizes Healthcare|url=http://www.youtube.com/watch?v=IzOWA7Maai4}}&lt;/ref&gt;
*Lagardère Active—&quot;Content Factory&quot; &lt;ref&gt;{{cite web|title=Content Factory: The Example of Lagardere Active|url=http://www.4dconcept.fr/en/content—factory|publisher=4dconcepts}}&lt;/ref&gt;
* [[LexisNexis]]—next generation legal research products&lt;ref&gt;{{cite web|last=Darrow|first=Barb|title=LexisNexis puts MarkLogic to work in big data makeover|url=http://gigaom.com/2011/12/08/lexisnexis—puts—marklogic—to—work—in—big—data—makeover/|publisher=gigaom|accessdate=8 December 2011}}&lt;/ref&gt;
* Lex Paradigm—Xquire component content management system&lt;ref&gt;{{cite web|title=XQuire: Advantages of MarkLogic 6|url=http://xquire.com/resources/advantages—of—marklogic—6/}}&lt;/ref&gt;
* [[Library of Congress]]—The largest library in the world and responsible for making its contents available for Congress and the American public uses MarkLogic to search, retrieve and display video, data and digitized documents from the Library’s collections.&lt;ref&gt;{{cite web|title=Government Agencies Discuss the Importance of Managing Unstructured Data at MarkLogic Government Summit|url=http://www.reuters.com/article/2010/11/11/idUS127503+11-Nov-2010+BW20101111}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Unstructured Data contained by MarkLogic|url=http://www.librarianwithdesign.com/blog/2010/11/unstructured-data-contained-by-marklogic/}}&lt;/ref&gt;
* McGraw Hill—revenue generating applications, and a  prototyping program that creates new market opportunities in as little as two weeks&lt;ref&gt;{{cite web|title=McGraw—Hill Builds Revenue and Future on MarkLogic|url=http://www.youtube.com/watch?v=6TZXfSFxUx0}}&lt;/ref&gt;
* [[Mitchell 1]]—auto information application&lt;ref&gt;{{cite web|title=Journey from Print to Online|url=http://www.youtube.com/watch?v=4mpN4XHSetY}}&lt;/ref&gt;
* [[Press Association]]—content management and publishing platform&lt;ref&gt;{{cite web|last=Heath|first=Nick|title=How PA cleared the big data hurdle at the London Olympics|url=http://www.techrepublic.com/blog/european—technology/how—pa—cleared—the—big—data—hurdle—at—the—london—olympics/1485/?s_cid=e019&amp;tag=nl.e019|publisher=European Technology}}&lt;/ref&gt;
*Really Strategies—RSuite content management system&lt;ref&gt;{{cite web|title=RSuite CMS + MarkLogic Server: Perfect together|url=http://www.youtube.com/watch?v=bceoQjYYYTM}}&lt;/ref&gt;
*[[Royal Society of Chemistry]]—manage and publish content for its RSC Publishing site, Learn Chemistry site, and the [[Merck Index]]&lt;ref&gt;{{cite web|last=Leonard|first=John|title=A positive reaction: big data technology at the Royal Society of Chemistry|url=http://www.computing.co.uk/ctg/analysis/2305957/a—positive—reaction—big—data—technology—at—the—royal—society—of—chemistry|publisher=Computing}}&lt;/ref&gt;
*[[Springer Science+Business Media|Springer]]—content platform (built in less than a year) and migrating tens of thousands of institutional customers (and tens of millions of pageviews per month) 
*[[US Patent Office]]—speed patent application process&lt;ref&gt;{{cite web|title=US patent office embraces big data|url=http://civsourceonline.com/2013/04/25/us—patent—office—embraces—big—data/}}&lt;/ref&gt;
*Warner Bros.—TOPS digital supply chain, managing metadata across the organization&lt;ref&gt;{{cite web|title=Warner Bros TOPS|url=http://www.hollywooditsociety.com/malibu—2013/files/2012/10/02—Room—A—The—New—Digital—Supply—Chain.mp3}}&lt;/ref&gt;
*[[Wiley Publishing|Wiley]]—Strategic publishing application &lt;ref&gt;{{cite web|title=Wiley Uses Mark Logic for Custom Publishing Application|url=http://www.econtentmag.com/Articles/News/News—Item/Wiley—Uses—Mark—Logic—for—Custom—Publishing—Application—53526.htm|accessdate=21 April 2009}}&lt;/ref&gt;
*[[Zynx Health]]—evidence—based, clinical decision support solutions for healthcare&lt;ref&gt;{{cite web|title=Complex Data is a Totally Different Beast|url=http://www.youtube.com/watch?v=EmHMsuq6oZQ}}&lt;/ref&gt;

==US Affordable Care Act==

The Centers for Medicaid and Medicare, responsible for implementation of the [[Patient Protection and Affordable Care Act]] (ACA) uses MarkLogic to power its database, including the [[Federal Data Services Hub]] and parts of Federally Facilitated Marketplace.

According to the ''New York Times'', the main contractor for ACA originally objected to using MarkLogic. &quot;CGI officials argued that it would slow work because it was too unfamiliar. Government officials disagreed, and its configuration remains a serious problem.&quot;&lt;ref&gt;{{cite web|last=Lipton, et al.|first=Eric|title=Tension and Flaws Before Health Website Crash|url=http://www.nytimes.com/2013/11/23/us/politics/tension-and-woes-before-health-website-crash.html?_r=0|accessdate=23 November 2013}}&lt;/ref&gt; The Times story did not say that MarkLogic's software is bad but stated, &quot;CGI officials argued that it would slow work because it was too unfamiliar.&quot;&lt;ref&gt;[http://developers.slashdot.org/story/13/11/24/1437203/nyt-healthcaregov-project-chaos-due-partly-to-unorthodox-database-choice NYT: Healthcare.gov Project Chaos Due Partly To Unorthodox Database Choice], Posted by timothy, Slashdot, 24 November 2013&lt;/ref&gt;

==Further reading==

* McCreary, Dan, and Ann Kelly. ''Making Sense of NoSQL''. Manning Publications Co. August 2012. ISBN 9781617291074. 
* Zhang, Andy. ''Beginning Mark Logic with XQuery and MarkLogic Server''. Champion Writers, Inc. 24 June 2009. ISBN 1608300153.

==See also==
*[[Unstructured data|Unstructured Data]]

==References==
{{Reflist|2}}

==External links==
* [http://www.marklogic.com/ MarkLogic home page]
* [http://markmail.org MarkMail home page]

{{Use dmy dates|date=February 2011}}

[[Category:NoSQL]]
[[Category:Software companies based in California]]
[[Category:Computer companies of the United States]]
[[Category:Companies established in 2001]]
[[Category:Companies based in San Carlos, California]]
[[Category:XML databases]]
[[Category:Big data]]</text>
      <sha1>rl9y8oweg04b8a63xhk3t0f4reav12r</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hopper (company)</title>
    <ns>0</ns>
    <id>39014446</id>
    <revision>
      <id>592860390</id>
      <parentid>592858012</parentid>
      <timestamp>2014-01-28T21:53:07Z</timestamp>
      <contributor>
        <ip>69.51.218.40</ip>
      </contributor>
      <text xml:space="preserve" bytes="6095">{{Infobox company
| name             = Hopper
| logo             = [[File:Logo for Hopper.png]]
| caption          =
| trading_name     = &lt;!-- d/b/a/, doing business as - if different from legal name above --&gt;
| native_name      = &lt;!-- Company's name in home country language --&gt;
| native_name_lang = &lt;!-- Use ISO 639-2 code, e.g. &quot;fr&quot; for French. If there is more than one native name, in different languages, enter those names using {{tl|lang}}, instead. --&gt;
| romanized        =
| former type      = 
| type             = 
| traded_as        = 
| industry         = [[Big Data]], [[Travel]]
| genre            = &lt;!-- Only used with media and publishing companies --&gt;
| fate             = 
| predecessor      = 
| successor        = 
| foundation       = 2007
| founder          = Frédéric Lalonde ([[CEO]]), Joost Ouwerkerk ([[Chief technology officer|CTO]]), Sébastien Rainville (Development)
| defunct          = &lt;!-- {{End date|YYYY|MM|DD}} --&gt;
| location_city    = [[Cambridge, Massachusetts]]
| location_country = 
| locations        = &lt;!-- Number of locations, stores, offices &amp;c. --&gt;
| area_served      = North America
| key_people       =  
| products         = 
| production       = 
| services         = 
| revenue          = 
| operating_income = 
| net_income       = 
| aum              = &lt;!-- Only used with financial services companies --&gt;
| assets           = 
| equity           = 
| owner            = 
| num_employees    = 
| parent           = 
| divisions        = 
| subsid           = 
| homepage         = [http://www.hopper.com/ Hopper.com]
| footnotes        = 
| intl             =
| bodystyle        =
}}
'''Hopper''' is an in-development travel search and recommendation engine, powered by the world's largest structured database of travel information.&lt;ref&gt;{{cite web|title=Hopper's About page|url=http://hopper.com/corp/about.html|accessdate=22 March 2013}}&lt;/ref&gt;

==History==
Hopper is a big-data start-up&lt;ref&gt;{{cite web|last=Alspach|first=Kyle|title=Hopper raises $12M to pioneer data-powered travel planning|url=http://www.bizjournals.com/boston/blog/startups/2012/08/hopper-travel-site-raises-12-million.html|publisher=Boston Business Journal|accessdate=4 April 2013}}&lt;/ref&gt;  that was founded in [[Montreal, Canada]], in 2007 by CEO Frédéric Lalonde, CTO Joost Ouwerkerk, and developer Sébastien Rainville with the goal of becoming &quot;the [[Google]] of travel.&quot;&lt;ref&gt;{{cite web|last=Kirsner|first=Scott|title=Website hopes to add retro to travel planning|url=http://www.boston.com/business/technology/articles/2011/08/08/site_takes_retro_approach_to_travel_planning/|publisher=Boston.com|accessdate=26 March 2013}}&lt;/ref&gt;

The engine's search results will build custom pages of travel information from its index of one billion web sites&lt;ref&gt;{{cite web|title=Hopper's Facebook page|url=https://www.facebook.com/155688707787821/timeline/story?ut=32&amp;wstart=1349074800&amp;wend=1351753199&amp;hash=466271513396204&amp;pagefilter=3&amp;ustart=1|accessdate=29 March 2013}}&lt;/ref&gt; and 10 million physical things, like hotels and restaurants, scraped from user-generated content from all around the web.&lt;ref&gt;{{cite web|last=Alspach|first=Kyle|title=Music, travel businesses tune in to ‘big data’|url=http://www.bizjournals.com/boston/print-edition/2012/06/01/music-travel-businesses-tune-in-to.html?page=2|publisher=Boston Business Journal|accessdate=29 March 2013}}&lt;/ref&gt; 

The company is currently headquartered in [[Cambridge, Massachusetts]], in the [[Kendall Boiler and Tank Company]] building, and also maintains an office in Montreal.&lt;ref&gt;{{cite web|title=Hopper's &quot;About&quot; page|url=http://www.hopper.com/corp/about.html|accessdate=29 March 2013}}&lt;/ref&gt;

===Funding===
In 2008, the company secured $2 million in funding. In 2011, a second round of funding secured $8 million from [[Atlas Venture]] and [[Brightspark Ventures]].&lt;ref&gt;{{cite web|title=Hopper Raises $8M to Reinvent Travel Search|url=http://finance.yahoo.com/news/Hopper-Raises-8M-Reinvent-iw-718821752.html|publisher=Yahoo Finance|accessdate=22 March 2013}}&lt;/ref&gt; The next year, Hopper secured $12 million from [[OMERS Ventures]].&lt;ref&gt;{{cite web|last=Ogg|first=Erica|title=Hopper gets another $12M to organize the web’s travel data|url=http://gigaom.com/2012/08/15/hopper-gets-another-12m-to-organize-the-webs-travel-data/|publisher=GigaOM|accessdate=22 March 2013}}&lt;/ref&gt;

==Hack/reduce==
In May 2012, Hopper CEO Frédéric Lalonde and entrepreneur [[Christopher P. Lynch]] founded [[Hack/reduce]], a non-profit hacker space designed to attract and foster big-data talent in the Boston area.&lt;ref&gt;{{cite web|last=Latamore|first=Bert|title=Hopper Brings Big-Data-as-a-Service to Online Travel Planning|url=http://servicesangle.com/blog/2012/06/28/hopper-brings-big-data-as-a-service-to-online-travel-planning/|publisher=Services Angle|accessdate=22 March 2013}}&lt;/ref&gt; Hack/reduce, which is located in the same building as Hopper's offices, has partnered with [[MIT]], [[CSAIL]], and [[Harvard]],&lt;ref&gt;{{cite web|last=Rousmaniere|first=Dana|title=Harvard Business Review interview with Lelonde|url=http://blogs.hbr.org/cs/2012/10/what_could_you_accomplish_with.html|accessdate=22 March 2013}}&lt;/ref&gt; and has received a grant from Governor [[Deval Patrick]] as part of his Massachusetts Big Data Initiative.&lt;ref&gt;{{cite web|title=Governor Patrick Announces New Initiative To Strengthen Massachusetts’ Position As A World Leader In Big Data|url=http://www.mass.gov/governor/pressoffice/pressreleases/2012/2012530-governor-announces-big-data-initiative.html|work=The Official Website of the Governor of Massachusetts|accessdate=27 March 2013}}&lt;/ref&gt;

==External links==
* [http://www.hopper.com/ Official website]
* [https://www.facebook.com/pages/Hopper/155688707787821 Hopper on Facebook]
* [https://twitter.com/hoppertravel Hopper on Twitter]

==References==
{{Reflist}}

[[Category:Travel websites]]
[[Category:Travel technology]]
[[Category:Travel ticket search engines]]
[[Category:Consumer guides]]
[[Category:Big data]]
[[Category:Online companies]]
[[Category:Companies based in Boston, Massachusetts]]</text>
      <sha1>jjllb3l4cytt2lk1vb1d4rw5j5hgnpd</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hack/reduce</title>
    <ns>0</ns>
    <id>37579425</id>
    <revision>
      <id>594901636</id>
      <parentid>589640796</parentid>
      <timestamp>2014-02-10T23:47:59Z</timestamp>
      <contributor>
        <username>Ktr101</username>
        <id>5495886</id>
      </contributor>
      <comment>/* References */ added coordinates</comment>
      <text xml:space="preserve" bytes="3639">{{Infobox Non-profit
| Non-profit_name   = hack/reduce
| Non-profit_logo   = [[Image:Logo_of_hack-reduce.png|200px]]
| Non-profit_type   = [[501(c)#501(c)(3)|501(c)(3)]]
| founded_date      = 2012
| location          = [[Cambridge, Massachusetts]], USA 
| origins           = 
| key_people        = 
| area_served       = 
| product           =
| focus             = [[Big Data]]
| method            = 
| revenue           = 
| endowment         = 
| num_volunteers    = 
| num_employees     = 
| num_members       = 
| subsib            = 
| owner             = 
| Non-profit_slogan = Code Big or Go Home!
| homepage          = {{URL|www.hackreduce.org}}
| dissolved         = 
| footnotes         = 
}}

[[File:Kendall Boiler and Tank Company building.jpg|thumb|right|The [[Kendall Boiler and Tank Company]] building where hack/reduce is housed]]
'''hack/reduce''' is a 501(c)(3) non-profit created to cultivate a community of [[Big Data]] experts in [[Boston, Massachusetts|Boston]].&lt;ref name=&quot;About&quot;&gt;[http://www.hackreduce.org/about/ About]. hack/reduce {{accessdate|2012-11-08}}&lt;/ref&gt;  It is located in the historic [[Kendall Boiler and Tank Company]] building at 275 Third Street in [[Kendall Square]] in [[Cambridge, Massachusetts]].

It was founded by serial entrepreneurs [[Christopher P. Lynch]] and Frederic Lalonde in May 2012.&lt;ref name=&quot;boston1&quot;&gt;[http://www.boston.com/business/innovation/2012/11/07/big-data-center-opening-cambridge/Suutqva9ORpZ1j2n1URkMP/story.html Big data center opening in Cambridge - Business]. Boston.com {{accessdate|2012-11-08}}&lt;/ref&gt; At its founding, hack/reduce raised more than $500,000 from [[Atlas Venture|local venture capital firms]] and [[Samuel Madden (computer scientist)|industry technology leaders]].&lt;ref name=&quot;boston1&quot;/&gt; hack/reduce is a “community hacker space” for Big Data that provides high performance compute resources, large data sets and subject matter expertise to help  identify and implement Big Data projects.&lt;ref name=&quot;About&quot;/&gt; It has partnerships with [[MIT]], [[CSAIL]], [[Bentley University]]. and [[Harvard]].&lt;ref&gt;[http://www.xconomy.com/boston/2012/11/07/hackreduce-to-open-thursday-as-lynch-fires-back-at-big-data-knuckleheads/ Hack/Reduce to Open Thursday as Lynch Fires Back at Big Data &quot;Knuckleheads&quot;]. Xconomy. Retrieved on 2012-11-08.&lt;/ref&gt; Sponsors include [[Microsoft]], [[IBM]], [[GoGrid]], Massachusetts Technology Collaborative, [[Dell]], [[Atlas Venture]], [[Bessemer Venture Partners]], [[Hopper (company)|Hopper]], Bright Spark Ventures, [[Google]], and others.&lt;ref&gt;[http://www.bizjournals.com/boston/blog/startups/2012/11/big-data-center-hackreduce-cambridge.html 'Big data' center hack/reduce getting big buzz - Boston Business Journal]. Bizjournals.com {{accessdate|2012-11-08}}&lt;/ref&gt;
 
In June 2012, Massachusetts Governor Deval Patrick announced the [[Massachusetts Big Data Initiative]] which comprised corporate, academic, and government programs supporting Big Data as well as a $50,000 grant and the State's support for hack/reduce.&lt;ref&gt;[http://bostinno.com/2012/11/07/no-hacks-allowed-hackreduce-launches-thursday-to-help-boston-step-up-its-big-data-game/ hack/reduce: Boston Steps Up Its Big Data Game]. BostInno {{accessdate|2012-11-08}}&lt;/ref&gt;

==References==
{{Reflist}}

{{Coord|42|21|58|N|71|5|2|W|display=title}}

{{DEFAULTSORT:Hack reduce}}
[[Category:501(c)(3) nonprofit organizations]]
[[Category:Big data]]
[[Category:Organizations established in 2012]]
[[Category:2012 establishments in Massachusetts]]
[[Category:Economy of Boston, Massachusetts]]
[[Category:Organizations based in Cambridge, Massachusetts]]

{{nongov-org-stub}}</text>
      <sha1>sieixfntpfbzunvdi5fzpfl223eusc6</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Oracle Big Data Appliance</title>
    <ns>0</ns>
    <id>34229379</id>
    <revision>
      <id>589664095</id>
      <parentid>576985212</parentid>
      <timestamp>2014-01-07T21:37:53Z</timestamp>
      <contributor>
        <ip>192.84.89.132</ip>
      </contributor>
      <comment>Removed link to Exalogic that appeared after Exalytics. The two are completely different.</comment>
      <text xml:space="preserve" bytes="7351">{{Advert|date=March 2012}}

The '''Oracle Big Data Appliance''' consists of hardware and software from [[Oracle Corporation]] designed to integrate enterprise data, both structured and unstructured.  It includes the [[Oracle Exadata]] Database Machine and the Oracle Exalytics Business Intelligence Machine, used for obtaining, consolidating and loading unstructured data into Oracle Database 11g.&lt;ref name=&quot;OBA&quot; /&gt; The product also includes an open source distribution of [[Apache Hadoop]], [[Oracle NoSQL Database]], [[Oracle Data Integrator]] with Application Adapter for Hadoop, Oracle Loader for Hadoop, an open source distribution of [[R (programming language)|R]], [[Oracle Linux]], and [[HotSpot|Oracle Java Hotspot]] Virtual Machine.&lt;ref name=&quot;BDE&quot; /&gt;

== History ==
Oracle announced the Oracle Big Data Appliance Mon, October 3 at [[Oracle OpenWorld]] 2011.&lt;br /&gt;
Oracle is notorious for acquiring other companies' software / hardware  and making its own software compatible.{{Citation needed|date=March 2012}} They have maximized their position in the field of Big Data by maximizing their platform capabilities by acquiring the web tier, the middleware, the database software, the database tier and the storage tier. This allows them to offer what they consider the total package for Big Data.

== The Challenge ==
The purpose of Oracle’s Big Data appliance is to integrate all enterprise data, structured and unstructured using a combination of hardware and software. This integration includes capturing the mountains of data from department silos, from weblogs, social media feeds, smart meters, sensors and other devices that generate massive volumes of data that are found in most enterprises. This maneuvering of data will change how business users perceive data and use it.

== Hardware ==
The major hardware components of the Big Data appliance consist of: a full rack configuration with 864GB of main memory and 432 TB of raw storage (without any redundancy protection). A full rack consists of 18 servers nodes that include a Sun server which is made up of: 2 CPUs (6-core Intel processors), 48 GB memory per node (upgradable to 96 GB or 144 GB), 12 x 2TB disks per node, InfiniBand Networking and 10 GbE connectivity.
&lt;ref name=&quot;BBOR&quot; /&gt;

== Software ==
* '''Oracle NoSQL Database''' is a distributed, scalable, key-value database based on Oracle’s Berkeley DB Java Edition High Availability storage engine. It is reputed to have predictable levels of throughput and latency and requires minimal administrative interaction. NoSQL database will be available in both open-source and commercial versions.&lt;ref name=&quot;OND&quot; /&gt;
* '''[[Apache Hadoop]]''' is a framework that allows for the dispersed processing of large data sets across groups of computers using a simple programming model.
* '''[[Oracle Data Integrator]] with Application Adapter for Hadoop'''
* '''Oracle Loader for Hadoop (OLH)''' enables users to use Hadoop MapReduce processing to create enhanced data sets for efficient loading and analysis in Oracle Database 11g. The difference between this loader and others is that it generates Oracle internal formats to load data faster and use less database system resources.
* '''Oracle R Enterprise''' tool is the combining of the open source distribution of R, a programming language and software environment for statistical computing and publication-quality graphics (Winter, 2011) with Oracle Database 11g. Oracle R Enterprise uses the approach that the models will run in-database and process large data sets, using the Oracle Database 11g and Exadata.
* '''Oracle Linux''' is an enterprise-class Linux distribution supported by Oracle.
* '''Oracle Java Hotspot Virtual Machine''' is a core component of the Java SE platform. It implements the Java Virtual Machine Specification, and is delivered as a shared library in the Java Runtime Environment.&lt;ref name=&quot;JavaSEHotSpot&quot; /&gt;

The software available will also be sold separately, to allow customers to define their own configurations with their existing Big Data infrastructure as well as a component in the data appliance.&lt;ref name=&quot;ORR&quot; /&gt;

== How it Works ==
A simplistic view is an organization would use the Oracle Big Data Appliance (Hadoop and NSQL) to capture the data, then use Big Data Connectors to a data warehouse where they can use Oracle Enterprise R or any other  data mining techniques to analyze the data further...

== Support ==
In partnership with Cloudera, the Hadoop software and services provider, Oracle will provide first-line support, Tier 1,  for the appliance and all software (including the Hadoop distribution and Cloudera Manager) through its issue-tracking support infrastructure ([[Metalink|My Oracle Support]]). Cloudera will handle Tier 2 and 3 support as well as training and consulting engagements.&lt;ref name=CLO /&gt;

== Additional Information ==
[http://www.idevnews.com/stories/4858/Oracle-Digs-in-On-NoSQL-Hadoop-End-to-End-Big-Data?print=1 Oracle Digs in On NoSQL, Hadoop, End-to-End Big Data ]
&lt;br /&gt;
[http://www.oracle.com/us/technologies/big-data/index.html Oracle Big Data Appliance Overview]

== References ==
&lt;references&gt;
&lt;ref name=&quot;OBA&quot;&gt;{{cite web|last=Darrow|first=Barb|title=Oracle BigData Appliance stakes big claim|url=http://gigaom.com/2011/10/03/oracle-big-data-appliance-stakes-big-claim/|accessdate=30 December 2011|date=2011-10-03|month=October}}&lt;/ref&gt;
&lt;ref name=&quot;CLO&quot;&gt;{{cite web|last=Henschen|first=Doug|title=Oracle Makes Big Data Appliance Move With Cloudera. |url=http://www.informationweek.com/news/software/info_management/232400021|publisher=Information Week|accessdate=24 January 2012|date=10 Jan 2011|month=January|year=2011}}&lt;/ref&gt;
&lt;ref name=&quot;BDE&quot;&gt;{{cite web|last=Dijcks|first=Jean-Pierre|title=Oracle: Big Data for the Enterprise|url=http://resources.idgenterprise.com/original/AST-0054994_DW_US_EN_WP_BigData.pdf|publisher=Oracle Corporation|accessdate=30 December 2011}}&lt;/ref&gt;
&lt;ref name=&quot;ORR&quot;&gt;{{cite web|last=Kanaracus|first=Chris|title=Oracle Rolls Out 'Big Data' Appliance|url=http://www.cio.com/article/690884/Oracle_Rolls_Out_Big_Data_Appliance?page=1&amp;taxonomyId=3007|publisher=CIO|accessdate=30 December 2011|date=3 Oct 2011|year=2011}}&lt;/ref&gt;
&lt;ref name=&quot;OND&quot;&gt;{{cite web|last=Oracle Corporation|title=ORACLE NOSQL DATABASE 11G|publisher=Oracle Corporation|accessdate=30 December 2011}}&lt;/ref&gt;
&lt;ref=&quot;IBD&quot;&gt;{{cite web|last=Powell|first=Ron|title=Incorporating Big Data into an Enterprise Information Architecture - A Q&amp;A Spotlight with Oracle's George Lumpkin|url=http://www.b-eye-network.com/view/15480|publisher=BeyeNETWORK|accessdate=30 December 2011|date=24 Oct 2011|year=2011}}&lt;/ref&gt;
&lt;ref name=&quot;BBOR&quot;&gt;{{cite web|last=Winter|first=Richard|title=Big Data :Business Opportunities, Requirements and Oracle's Approach|url=http://www.oracle.com/us/corporate/analystreports/infrastructure/winter-big-data-1438533.pdf|publisher=Winter Corporation|accessdate=30 December 2011|date=December 2011|year=2011}}&lt;/ref&gt;
&lt;ref name=&quot;JavaSEHotSpot&quot;&gt;{{cite web|last=Oracle Corporation|title=Java SE HotSpot at a Glance|url=http://www.oracle.com/technetwork/java/javase/tech/index-jsp-136373.html|publisher=Oracle Technology Network|accessdate=4 January 2012|date=1 Jan 2011|month=January|year=2011}}&lt;/ref&gt;
&lt;/references&gt;

[[Category:Oracle Corporation]]
[[Category:Business intelligence]]
[[Category:Big data]]</text>
      <sha1>hsbe9nj3tx791o66svgo1gfwt7c14qb</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Palantir Technologies</title>
    <ns>0</ns>
    <id>27197818</id>
    <revision>
      <id>600437161</id>
      <parentid>596439121</parentid>
      <timestamp>2014-03-20T11:40:08Z</timestamp>
      <contributor>
        <ip>24.46.31.198</ip>
      </contributor>
      <comment>Removed a reference to the expired Palantir Metropolis Joyride demo (https://joyride.palantir.com/welcome/ - the joyride demo is no longer available)</comment>
      <text xml:space="preserve" bytes="16373">{{Infobox company
| name             = Palantir Technologies
| logo             = [[File:Palantir Technologies company logo.jpg|200px]]
| type             = [[Privately held company|Private]]
| foundation       = 2004
| founder          = [[Peter Thiel]], [[Joe Lonsdale]], Alex Karp, [[Stephen Cohen (entrepreneur)|Stephen Cohen]], Nathan Gettings
| location_city    = [[Palo Alto, California]]
| products         = Palantir Gotham, Palantir Metropolis
| num_employees    = 1,200&lt;ref&gt;{{cite web |title = Big Data, Big Bucks: Palantir Valued at $9 Billion |publisher = Wall Street Journal |url = http://blogs.wsj.com/digits/2013/12/05/big-data-big-bucks-palantir-valued-at-9-billion/ |accessdate = 2013-12-05}}&lt;/ref&gt; 
| homepage         = {{URL |www.palantir.com}}
}}
'''Palantir Technologies, Inc.''' is an American computer software and services company, specializing in US government customers, and since 2010, financial customers.

== History ==
Palantir was founded in 2004 by [[Peter Thiel]], Alex Karp,&lt;ref&gt;{{cite web |url= http://media.palantirtech.com/videos/charlierose.html |title=charlierose |publisher= Media.palantirtech.com |date= |accessdate=2012-01-30}}&lt;/ref&gt; [[Joe Lonsdale]],&lt;ref&gt;{{cite web |url= http://www.crunchbase.com/company/palantir-technologies |title=Palantir Technologies &amp;#124; CrunchBase Profile |publisher=Crunchbase.com |date= |accessdate=2012-01-30}}&lt;/ref&gt; [[Stephen Cohen (entrepreneur)|Stephen Cohen]], and Nathan Gettings. Early investments were $2 million from the US [[Central Intelligence Agency]] venture arm [[In-Q-Tel]], and $30 million from Thiel and his firm, [[Founders Fund]].&lt;ref name=&quot;bare_url&quot;&gt;{{cite news| url=http://online.wsj.com/article/SB125200842406984303.html | work=The Wall Street Journal | title=How Team of Geeks Cracked Spy Trade | first=Siobhan | last=Gorman | date=September 4, 2009}}&lt;/ref&gt;&lt;ref name=&quot;npr&quot;&gt;{{cite web|url=http://www.npr.org/templates/story/story.php?storyId=106479613 |title=A Tech Fix For Illegal Government Snooping? |publisher=NPR |date= |accessdate=2012-01-30}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.foundersfund.com/palantir.php |title=Palantir « Founders Fund |publisher= Foundersfund.com |date= |accessdate=2012-01-30}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Evelyn Rusli |url=http://techcrunch.com/2010/06/25/palantir-the-next-billion-dollar-company-raises-90-million/ |title=Palantir: The Next Billion-Dollar Company Raises $90 Million |publisher=TechCrunch |date=2010-06-25 |accessdate=2012-01-30}}&lt;/ref&gt; Alex Karp is Palantir’s CEO.&lt;ref name=&quot;charlierose&quot;&gt;{{cite web|url=http://www.charlierose.com/guest/view/6717 |title=Alexander Karp |publisher=Charlie Rose |date=2009-08-11 |accessdate=2012-01-30}}&lt;/ref&gt; Palantir’s name comes from the &quot;[[Palantír|seeing stones]]&quot; in J. R. R. Tolkien's fantasy epic ''[[The Lord of the Rings]].'' Headquartered in [[Palo Alto, California]], the company has four international offices and four in the United States.&lt;ref&gt; {{cite web |url=http://www.palantir.com/contact |title=contact information}}&lt;/ref&gt;

Palantir developed its technology by computer scientists and analysts from intelligence agencies over three years, through pilots facilitated by In-Q-Tel.&lt;ref&gt;{{cite web| author=Jeff Widman |url=http://entrepreneur.venturebeat.com/2009/06/05/palantir-keeps-it-lean-and-mean-on-five-year-journey-from-zero-to-150-employees/ |title=Palantir keeps it lean and mean on five-year journey from zero to 150 employees | publisher=VentureBeat |date=2009-06-05 |accessdate=2012-01-30}}&lt;/ref&gt; The software concept grew out of technology developed at [[PayPal]] to detect fraudulent activity, much of it conducted by Russian organized crime syndicates.&lt;ref name=&quot;bare_url&quot; /&gt; 
The company said computers alone using [[artificial intelligence]] could not defeat an adaptive adversary. 
Palantir  proposed using human analysts to explore data from many sources, called [[Intelligence Amplification|intelligence augmentation]].&lt;ref&gt;{{cite web|author=Ari Gesher |url= http://blog.palantirtech.com/2010/03/08/friction-in-human-computer-symbiosis-kasparov-on-chess/ |title=Palantir Technologies » Blog Archive » Friction in Human-Computer Symbiosis: Kasparov on Chess |publisher=Blog.palantirtech.com |date=2010-03-08 |accessdate=2012-01-30}}&lt;/ref&gt;

In April 2010, Palantir announced a partnership with [[Thomson Reuters]] to sell the Palantir Metropolis product as QA Studio.&lt;ref&gt;{{cite news| title=Press release: Thomson Reuters and Palantir Technologies enter exclusive agreement to create next-generation analytics platform for financial clients |url=http://thomsonreuters.com/content/press_room/financial/2010_04_12_palantir_technologies_agreement |date=2010-04-12 |publisher=Thomson Reuters}}&lt;/ref&gt;
On June 18, 2010, [[Vice President of the United States|Vice President]] [[Joe Biden]] and [[Office of Management and Budget]] Director [[Peter Orszag]] held a press conference at the White House announcing the success of fighting fraud in the stimulus by the [[Recovery Accountability and Transparency Board]] (RATB).  Biden credited the success to the software, Palantir, being deployed by the federal government.&lt;ref&gt;{{cite web|author=Tim Kauffman | url=http://www.federaltimes.com/article/20100627/AGENCY05/6270306/ |title=The new high-tech weapons against fraud |publisher= Federal Times |date=2010-06-27 |accessdate=2012-01-30}}&lt;/ref&gt;  He announced that the capability will be deployed at other government agencies, starting with [[Medicare (United States)|Medicare]] and [[Medicaid]].&lt;ref&gt;{{cite news| url=http://content.usatoday.com/communities/theoval/post/2010/06/obama-administration-to-create-do-not-pay-list-to-bar-shady-contractors/1 | newspaper=USA Today | title=Obama administration to create 'do not pay' list to bar shady contractors | date=2010-06-18 | first=Kathy | last=Kiely}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Peter Orszag, Director |url=http://www.whitehouse.gov/omb/blog/10/06/18/Do-Not-Pay-Do-Read-This-Post/?mkt_tok=3RkMMJWWfF9wsRonu63NZKXonjHpfsX66%2BgtWaOg38431UFwdcjKPmjr1YICTQ%3D%3D |title=Do Not Pay? Do Read This Post |publisher=Whitehouse.gov |date=2010-06-18 |accessdate=2012-01-30}}&lt;/ref&gt;&lt;ref name=&quot;bare_url_b&quot;&gt;{{cite news| url=http://politicalticker.blogs.cnn.com/2010/06/01/companies-capitalize-on-open-government/?fbid=ykqVZByQGPM |work= Political Ticker blog |author= Eric Kuhn |date= June 1, 2010 | publisher=CNN | title=Companies capitalize on 'open government' |accessdate= September 28, 2013 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.palantir.com/2010/05/govcon5-videos-now-available/ |title=Using Palantir with Open Source Data: Finding and Preventing Fraud in Stimulus Spending |publisher=Palantir Technologies |date=2010-05-04}}&lt;/ref&gt;
Estimates were $250 million in revenues in 2011.&lt;ref&gt;{{Cite news |title= Palantir, the War on Terror's Secret Weapon: A Silicon Valley startup that collates threats has quietly become indispensable to the U.S. intelligence community |work= Business Week Magazine |date= November 22, 2011 |authors=  Ashlee Vance and Brad Stone |url= http://www.businessweek.com/magazine/palantir-the-vanguard-of-cyberterror-security-11222011.html |accessdate= September 28, 2013 }}&lt;/ref&gt;

In September 2013, Palantir disclosed over $196 million in funding according to a US [[Securities and Exchange Commission]] filing.&lt;ref&gt; {{cite web | url = http://www.sec.gov/Archives/edgar/data/1321655/000132165513000002/xslFormDX01/primary_doc.xml | title = Notice of Exempt Offering of Securities |date= September 27, 2013 | accessdate = September 28, 2013 | publisher = The United States Securities and Exchange Commission}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Cutler|first=Kim-Mai|title=Palantir Is Raising $197M In Growth Capital, SEC Filing Shows|url=http://techcrunch.com/2013/09/27/palantir-197m-sec-filing/|publisher=TechCrunch}}&lt;/ref&gt; It was estimated that the company would likely close almost $1 billion in contracts in 2014.&lt;ref&gt; {{cite news |authors = Andy Greenberg, Ryan Mac | title = How A 'Deviant' Philosopher Built Palantir, A CIA-Funded Data-Mining Juggernaut | date = September 2, 2013 | url = http://www.forbes.com/sites/andygreenberg/2013/08/14/agent-of-intelligence-how-a-deviant-philosopher-built-palantir-a-cia-funded-data-mining-juggernaut/ | work = Forbes | accessdate = September 28, 2013 }}&lt;/ref&gt;

== Products ==
=== Palantir Gotham ===
Palantir Gotham (formerly known as Palantir Government) integrates structured and [[unstructured data]], provides advanced search and discovery capabilities, enables knowledge management, and facilitates secure collaboration. The Palantir platform includes the privacy and civil liberties protections mandated by legal requirements such as those in the 9/11 Commission Implementation Act.  Palantir’s privacy controls keep investigations focused, as opposed to the expansive [[data mining]] techniques that have drawn criticism from privacy advocates concerned about civil liberties protection. Palantir maintains security tags at a granular level.&lt;ref name=&quot;npr&quot; /&gt;&lt;ref name=&quot;charlierose&quot; /&gt;

Palantir runs the site AnalyzeThe.US,&lt;ref&gt;{{cite web|url=http://analyzethe.us |title= AnalyzeThe.US — Home |work= Web site |publisher= Palantir  |accessdate= September 28, 2013 }}&lt;/ref&gt; which allows Palantir customers and affiliates to use Palantir Gotham to perform analysis on publicly available data from data.gov, usaspending.gov, the Center for Responsive Politics’ Open Secrets Database, and Community Health Data from HHS.gov.&lt;ref&gt;{{cite web|url=http://www.fiercehealthcare.com/press-releases/palantir-technologies-showcase-analysis-community-health-data-initiative-forum-harnes |title=Palantir Technologies to Showcase Analysis at the Community Health Data Initiative Forum: Harnessing the Power of Information to |publisher=FierceHealthcare |date=2010-06-02 |accessdate=2012-01-30}}&lt;/ref&gt;

=== Palantir Metropolis ===
Palantir Metropolis (formerly known as Palantir Finance) is software for data integration, information management and quantitative [[analytics]].  The software connects to commercial, proprietary and public data sets and discovers trends, relationships and anomalies, including [[predictive analytics]].

== Customers ==
Palantir Gotham is used by counter-terrorism analysts at offices in the [[United States Intelligence Community]] and [[United States Department of Defense]], fraud investigators at the [[Recovery Accountability and Transparency Board]], and cyber analysts at [[Information Warfare Monitor]] (responsible for the [[GhostNet]] and the Shadow Network investigation).  Palantir Metropolis is used by hedge funds, banks, and financial services firms.&lt;ref name=&quot;bare_url&quot; /&gt;&lt;ref name=&quot;npr&quot; /&gt;&lt;ref&gt;{{cite web|url=http://www.gov2expo.com/gov2expo2010/public/schedule/detail/13996 |title=A Human Driven Data-centric Approach to Accountability: Analyzing Data to Prevent Fraud, Waste and Abuse in Stimulus Spending: Gov 2.0 Expo 2010 - Co-produced by UBM TechWeb &amp; O'Reilly Conferences, May 25 - 27, 2010, Washington, DC |publisher=Gov2expo.com |date= |accessdate= 2012-01-30}}&lt;/ref&gt;&lt;ref name=&quot;bare_url_a&quot;&gt;{{cite web| title=PayPal-Based Technology Helped Bust India's And The Dalai Lama's Cyberspies |url=http://www.forbes.com/sites/firewall/2010/04/30/paypal-based-technology-helped-bust-indias-and-the-dalai-lamas-cyberspies/ | publisher=Forbes | first= Oliver | last=Chiang |date=2010-04-30}}&lt;/ref&gt;

U.S. [[military intelligence]] used the Palantir product to improve their ability to predict locations of [[improvised explosive devices]] in its [[war in Afghanistan (2001–present)|war in Afghanistan]]. A small number of practitioners reported it to be more useful than the U.S. Army's program of record, the [[Distributed Common Ground System]] (DCGS-A). California Congressman [[Duncan D. Hunter]] complained of [[United States Department of Defense|US DoD]] obstacles to its wider use in 2012.&lt;ref&gt;{{cite news |newspaper= [[The Washington Times]]  |title=Military has to fight to purchase lauded IED buster |date= July 16, 2012 |url= http://p.washingtontimes.com/news/2012/jul/16/military-has-to-fight-to-purchase-lauded-ied-buste/ |authorlink=Rowan Scarborough |first=Rowan |last =Scarborough |accessdate= September 29, 2013 }}&lt;/ref&gt;

=== Infowar Monitor ===
Palantir partner [[Information Warfare Monitor]] used Palantir software to uncover both the [[Ghostnet]] and the Shadow Network. The Ghostnet was a China-based cyber espionage network targeting  1,295 computers in 103 countries, including the [[Dalai Lama]]’s office, a NATO computer and  embassies.&lt;ref&gt;{{cite news| url=http://www.cnn.com/video/#/video/tech/2009/03/30/vause.china.cyber.espionage.cnn | work=CNN | title=CNN.com Video}}&lt;/ref&gt; The Shadow Network  was also a China-based espionage operation that hacked into the Indian security and defense apparatus.  Cyber spies stole documents related to Indian security, embassies abroad, and NATO troop activity in Afghanistan.&lt;ref name=&quot;bare_url_a&quot; /&gt;&lt;ref&gt;{{cite news| url=http://www.nytimes.com/2009/03/29/technology/29spy.html | work=The New York Times | title=Vast Spy System Loots Computers in 103 Countries | first=John | last=Markoff | date=March 29, 2009}}&lt;/ref&gt;

=== Recovery Accountability and Transparency Board ===
Palantir’s software is used by the [[Recovery Accountability and Transparency Board]] to detect and investigate fraud and abuse in the American Recovery and Reinvestment Act.  Specifically, the Recovery Operations Center (ROC) used Palantir to integrate transactional data with open-source and private data sets that describe the entities receiving Stimulus funds.&lt;ref name=&quot;bare_url_b&quot; /&gt;

== Palantir Night Live ==
Palantir hosts Palantir Night Live at Palantir’s McLean and Palo Alto offices. The event brings speakers from the intelligence community and technology space to discuss topics of common interest. Past speakers include [[Garry Kasparov]], [[Nart Villeneuve]] from Information Warfare Monitor, [[Andrew McAfee]], author of Enterprise 2.0, and [[Michael Chertoff]].&lt;ref&gt;{{cite web |url= http://www.washingtonlife.com/2010/04/09/society-2-0-tenet-chertoff-and-beer-oh-my/ |title=Society 2.0: Tenet, Chertoff and Beer, Oh My! &amp;#124; Washington Life Magazine |publisher=Washingtonlife.com |date=2010-04-09 |accessdate=2012-01-30}}&lt;/ref&gt;

== WikiLeaks proposals ==
In 2010 [[Hunton &amp; Williams]] LLP allegedly asked [[Berico Technologies]], Palantir, and [[HBGary Federal]] to draft a response plan to “the [[WikiLeaks]] Threat.” In early 2011 [[Anonymous (group)|Anonymous]] publicly released HBGary-internal documents, including the plan. The plan proposed Palantir software would “serve as the foundation for all the data collection, integration, analysis, and production efforts.”&lt;ref name=&quot;Harris&quot;&gt;{{cite web|last=Harris|first=Shane|title=Killer App|url=http://www.washingtonian.com/articles/people/killer-app/|work=Washingtonian|date=31 January 2012|accessdate=14 March 2012}}&lt;/ref&gt;  The plan also included slides, allegedly authored by HBGary CEO Aaron Barr, which suggested “[spreading] disinformation” and “disrupting” [[Glenn Greenwald]]’s support for WikiLeaks.&lt;ref&gt;{{cite web|author=James Wray and Ulf Stabe |url=http://www.thetechherald.com/article.php/201106/6798/Data-intelligence-firms-proposed-a-systematic-attack-against-WikiLeaks?page=2 |title=Data intelligence firms proposed a systematic attack against WikiLeaks |publisher=Thetechherald.com |date= |accessdate=2012-01-30}}&lt;/ref&gt;

Palantir CEO Karp ended all ties to HBGary and issued a statement apologizing to “progressive organizations… and Greenwald … for any involvement that we may have had in these matters.&quot; Palantir placed an employee on leave pending a review by a third-party law firm. The employee was later reinstated.&lt;ref name=&quot;Harris&quot;/&gt;

== See also ==
* [[Recorded Future]]

== References ==
{{reflist|30em}}

== External links ==
* [http://www.palantir.com/ Palantir Technologies]
* [http://www.forbes.com/sites/andygreenberg/2013/06/07/startup-palantir-denies-its-prism-software-is-the-nsas-prism-surveillance-system/ Startup Palantir Denies Its 'Prism' Software Is The NSA's 'PRISM' Surveillance System] by Andy Greenberg, Forbes, 2013-06-07, accessed 2013-06-07.

[[Category:Business software companies]]
[[Category:Big data]]
[[Category:Companies established in 2004]]
[[Category:Software companies based in the San Francisco Bay Area]]</text>
      <sha1>qrnkq7h31bj8l35zdgnygbdj3g94rek</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Hibari (database)</title>
    <ns>0</ns>
    <id>28886888</id>
    <revision>
      <id>577292426</id>
      <parentid>572184170</parentid>
      <timestamp>2013-10-15T15:15:37Z</timestamp>
      <contributor>
        <username>Mortlaken</username>
        <id>19934545</id>
      </contributor>
      <minor/>
      <comment>removed unneeded bolding to linking</comment>
      <text xml:space="preserve" bytes="2486">{{Infobox software
| name                   = Hibari
| logo                   =
| screenshot             =
| caption                =
| author                 =
| developer              = Hibari developers
| released               = 2010
| status                 = Active
| latest release version = v0.1.10
| latest release date    = {{release date|2013|02|04}}
| latest preview version = 
| latest preview date    = 
| frequently updated     = yes
| programming language   = [[Erlang (programming language)|Erlang]]
| operating system       = [[Cross-platform]]
| language               = English, Japanese
| genre                  = [[Key-value pair|Key-value]] store
| license                = [[Apache License]] 2.0
| website                = https://github.com/hibari/hibari
}}
{{Portal|Free software}}
'''Hibari''' is a strongly consistent, highly available, distributed, key-value [[Big Data]] store. ([[NoSQL]] database) &lt;ref&gt;[https://github.com/hibari/hibari Hibari project homepage]&lt;/ref&gt; It was developed by [[Cloudian, Inc.]], formerly Gemini Mobile Technologies to support its mobile messaging and email services and released as [[open source]] on July 27, 2010.

Hibari, a Japanese name meaning &quot;Cloud Bird&quot;, can be used in [[cloud computing]] with services&amp;mdash;such as [[social networking]]&amp;mdash;requiring the daily storage of potentially [[terabyte]]s or [[petabyte]]s of new data.

==Distinctive Features==
Hibari uses chain replication for strong consistency, high-availability, and durability. Unlike many other [[NoSQL]] variants, Hibari support micro-transaction, which is ACID transaction within a certain range of keys.

Hibari has excellent performance especially for read and large value (around 200KB) operations.

==Interfaces==
Hibari supports APIs such as [[Amazon S3]], [[JSON-RPC]] and Universal Binary Protocol; plans have been announced for support of [[Apache Thrift]]; in addition to [[Erlang (programming language)|Erlang]], the language it was developed in. Hibari supports [[language binding]]s such as [[Java (software platform)|Java]], [[C (programming language)|C]], [[C++]], [[Python (programming language)|Python]], and [[Ruby (programming language)|Ruby]].

==References==
{{Reflist}}

{{DEFAULTSORT:Hibari (Database)}}
[[Category:Free database management systems]]
[[Category:2010 software]]
[[Category:Erlang programming language]]
[[Category:Cloud storage]]
[[Category:Distributed data storage]]
[[Category:NoSQL]]
[[Category:Big data]]</text>
      <sha1>i5ufkh3uw5efpu8mmdoh9g21v3umhge</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>SAP HANA</title>
    <ns>0</ns>
    <id>40383082</id>
    <revision>
      <id>599904809</id>
      <parentid>599904719</parentid>
      <timestamp>2014-03-16T19:46:53Z</timestamp>
      <contributor>
        <username>Saida777</username>
        <id>20990583</id>
      </contributor>
      <text xml:space="preserve" bytes="7453">{{Infobox software

| name = SAP HANA
| logo = &lt;!-- logo removed since it was for SAP the company not this product --&gt;
| screenshot =
| caption =
| developer = [[SAP AG]]
| released =
| latest_release_version =  1.0 SPS7
| latest_release_date = {{start date and age|2013|12|01}}
| latest_preview_version =
| latest_preview_date =
| release_location =
| genre = [[in-memory database|In-memory]] [[RDBMS]]
| status = Active
| programming language = [[C (programming language)|C]], [[C++]]
| language = Multi-lingual&lt;!-- Should state the number of languages. --&gt;
| license = [[proprietary software|Proprietary]]
| website = {{URL|www.saphana.com}}&lt;br/&gt;
[http://scn.sap.com/community/hana-in-memory SAP Community Network]
}}

'''SAP HANA''', short for &quot;'''H'''igh-Performance '''An'''alytic '''A'''ppliance&quot; is an [[In memory database|in-memory]], [[Column-oriented DBMS|column-oriented]],  [[relational database management system]] developed and marketed by [[SAP AG]].&lt;ref&gt;{{Cite web |title= Primer on SAP HANA |author= Jeff Kelly |date= July 12, 2013 |accessdate= October 9, 2013 |work= Wikibon |url=  http://wikibon.org/wiki/v/Primer_on_SAP_HANA }}&lt;/ref&gt;&lt;ref&gt;{{YouTube |id=adv25iZmgQc |title=  SAP HANA - The Column Oriented (Based) Database }} (December 8, 2012)&lt;/ref&gt;

==History==
SAP HANA originates from developed or acquired technologies, including [[TREX search engine]], an [[In-memory database|in-memory]] [[column-oriented DBMS|column-oriented search engine]], P*TIME, an in-memory [[OLTP]] database acquired by SAP in 2005, and [[MaxDB]] with its in-memory liveCache engine.&lt;ref&gt;{{cite web | url= http://www.redbooks.ibm.com/redpapers/pdfs/redp4814.pdf | title= SAP In-Memory Computing on IBM eX5 Systems | first1= Gereon | last1= Vey | first2= Ilya | last2= Krutov | date= January 2012 | accessdate= 22 October 2013}}&lt;/ref&gt;&lt;ref name=&quot;line&quot;&gt;{{Cite web |title= SAP HANA Timeline |date= June 17, 2012 |author= SAP AG |publisher= SlideShare |url= http://www.slideshare.net/SAPTechnology/sap-hana-timeline |accessdate= October 9, 2013 }}&lt;/ref&gt;
In 2008, teams from [[SAP AG]] working with [[Hasso Plattner Institute]] and [[Stanford University]] demonstrated an application architecture for real-time analytics and aggregation, mentioned as &quot;Hasso's New Architecture&quot; in SAP executive [[Vishal Sikka]]'s blog. Before the name HANA settled in, people referred to this product as New Database.&lt;ref&gt;{{Cite web |work= Gucons web site |year= 2011 |title= What is SAP HANA Database |url= http://www.gucons.com/what-is-sap-hana-database/ |accessdate= October 9, 2013 }}&lt;/ref&gt;

The product was officially announced in May 2010. In November 2010, SAP AG announced the release of SAP HANA 1.0, an in-memory appliance for [[business application]]s and [[business Intelligence]] allowing real-time response.&lt;ref name=&quot;set&quot;&gt;{{Cite news |title= SAP's in-memory analytics boxes set for November release |work= Info World |date= October 19, 2010 |author= Chris Kanaracus |url= http://www.infoworld.com/d/applications/saps-in-memory-analytics-boxes-set-november-release-117 |accessdate= October 9, 2013 }}&lt;/ref&gt;
The first product shipped in late November 2010.&lt;ref name=&quot;line&quot; /&gt;&lt;ref&gt;{{Cite news |title= SAP launches HANA for in-memory analytics: The in-memory analytic appliance will compete with next-generation data-processing platforms such as Oracle's Exadata machines |author= Chris Kanaracus |date= December 1, 2010 |work= Info World |url= http://www.infoworld.com/d/data-management/sap-launches-hana-in-memory-analytics-300 |accessdate= September 24, 2013 }}&lt;/ref&gt;
By mid-2011, the technology had attracted interest but the conservative business customers still considered it &quot;in early days&quot;.&lt;ref&gt;{{Cite news |title= SAP's HANA is hot, but still in early days |work= Network World |date= September 15, 2011 |author= Chris Kanaracus |url= http://www.networkworld.com/news/2011/091511-saps-hana-is-hot-but-250942.html |accessdate= October 15, 2013 }}&lt;/ref&gt;
HANA support for [[SAP NetWeaver Business Warehouse]] was announced in September 2011 for availability by November.&lt;ref&gt;{{Cite news |title= SAP Begins BW on HANA Ramp-Up, First Big Test for the HANA Database |author= Courtney Bjorlin |work= ASUG News |date= November 9, 2011 |url= http://www.asugnews.com/article/sap-begins-bw-on-hana-ramp-up-first-big-test-for-the-hana-database |accessdate= October 15, 2013  }}&lt;/ref&gt;

In 2012, SAP promoted aspects of [[cloud computing]].&lt;ref&gt;{{Cite news |title= SAP Headed For $71 On Cloud, Mobile And HANA Growth |author= Trevis Team |work= Forbes |date= April 30, 2012 |url= http://www.forbes.com/sites/greatspeculations/2012/04/30/sap-headed-for-71-on-cloud-mobile-and-hana-growth/ |accessdate= October 9, 2013 }}&lt;/ref&gt;
In October 2012, SAP announced a variant called HANA One that used a smaller amount of memory on [[Amazon Web Services]] for an hourly fee.&lt;ref&gt;{{Cite news |title= SAP Launches Cloud Platform Built On Hana |author= Doug Henschen |date= October 17, 2012 |work= Information Week |url= http://www.informationweek.com/software/enterprise-applications/sap-launches-cloud-platform-built-on-han/240009198&lt;!-- pay site  --&gt; |deadurl=no |archiveurl= http://web.archive.org/web/20121019234617/http://www.informationweek.com/software/enterprise-applications/sap-launches-cloud-platform-built-on-han/240009198 |archivedate= October 19, 2012 |accessdate= October 15, 2013 }}&lt;/ref&gt;

In January 2013, [[SAP ERP|SAP enterprise resource planning]] software from its [[SAP Business Suite|Business Suite]] was announced for HANA, and became available by May.&lt;ref&gt;{{Cite news |title= SAP puts Business Suite on HANA, joins transactional to analytical |work= Computer Weekly |author= Brian McKenna |date= January 11, 2013 |url= http://www.computerweekly.com/news/2240175913/SAP-puts-Business-Suite-on-HANA-joins-transactional-to-analytical |accessdate= October 15, 2013 }}&lt;/ref&gt;&lt;ref&gt;{{Cite news |title= Sapphire 2013: Business Suite on HANA goes to general availability |work= Computer Weekly |date= May 15, 2013 |url= http://www.computerweekly.com/news/2240184187/Sapphire-2013-Business-Suite-on-HANA-goes-to-general-availability |accessdate= October 15, 2013 }}&lt;/ref&gt;
In May 2013, a [[software as a service]] offering called the HANA Enterprise Cloud service was announced.&lt;ref&gt;{{Cite news |title= 
SAP unveils HANA Enterprise Cloud service: Customers will be able to run their applications on the HANA-powered cloud |work= Network World |date= May 7, 2013 |author=  Chris Kanaracus |url= http://www.networkworld.com/news/2013/050713-sap-unveils-hana-enterprise-cloud-269505.html |accessdate= October 15, 2013 }}&lt;/ref&gt;

Rather than [[software versioning|versioning]], the software utilizes [[service pack]]s.&lt;ref name=&quot;faq&quot;&gt;{{Cite web |title= Update III: The SAP HANA FAQ - answering key SAP In-Memory questions |date= May 28, 2012 |date= John Appleby |accessdate= October 9, 2013 |publisher=  Bluefin Solutions |url= http://www.bluefinsolutions.com/insights/blog/the_sap_hana_faq_answering_key_sap_in_memory_questions/ }}&lt;/ref&gt;&lt;ref name=&quot;cheat&quot;&gt;{{Cite web |title= Your SAP HANA Cheat Sheet: Milestones, Terms and More |author= Mellisa Tolentino |work= Silicon Angle blog |date= May 14, 2012 |url= http://siliconangle.com/blog/2012/05/14/your-sap-hana-cheat-sheet-milestones-terms-and-more/ |accessdate= October 9, 2013 }}&lt;/ref&gt;

==References==
{{Reflist|2}}

[[Category:2010 software]]
[[Category:Proprietary database management systems]]
[[Category:Big data]]</text>
      <sha1>j78n1fu7xqyxvimocbex0upi3gdp12b</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Sense Networks</title>
    <ns>0</ns>
    <id>27370710</id>
    <revision>
      <id>573715795</id>
      <parentid>572184270</parentid>
      <timestamp>2013-09-20T00:01:18Z</timestamp>
      <contributor>
        <ip>24.60.67.237</ip>
      </contributor>
      <comment>added newsweek cover article</comment>
      <text xml:space="preserve" bytes="7470">{{Infobox Dotcom company
| company_name         = Sense Networks, Inc.
| company_slogan       = Indexing the real world using location data for predictive analytics
| owner                =
| company_logo         = [[File:Sensenetworks logo.png|240px]]
| caption              =
| company_type         = [[Private company|Private (venture backed)]]
| foundation           = [[New York City]], [[New York]], USA
| founder              = [[Greg Skibiski]]
| area_served          = [[World]]wide
| location_city        = [[New York City, New York]]
| location_country     = [[United States]]
| key_people           = [[Alex Pentland]]&lt;small&gt; ([[Chief privacy officer|CPO]])&lt;/small&gt;&lt;br /&gt;[[Tony Jebara]]&lt;small&gt; (Chief Scientist)&lt;/small&gt;&lt;br /&gt;[[Christine Lemke]]&lt;small&gt; ([[Chief operating officer|COO]])&lt;/small&gt; &lt;br/&gt;[[Mikki Nasch]]&lt;small&gt; (EVP BD)&lt;/small&gt;
| revenue              =
| homepage             = [http://www.sensenetworks.com]
| screenshot           =
| caption              =
| num_employees        =
| url                  =
| registration         =
| launch_date          = February 2006
| current_status       = Active
| language             =
| advertising          =
| alexa                =
| website_type         =
}}
'''Sense Networks''' is a New York City based company with a focus on applications that analyze [[big data]] from [[mobile phone]]s, [[carrier network]]s, and [[taxicabs]], particularly by using [[machine learning]] technology to make sense of large amounts of location (latitude/longitude) data.&lt;ref&gt;Fitzgerald, Michael. [http://www.nytimes.com/2008/06/22/technology/22proto.html?_r=2&amp;scp=1&amp;sq=sense+networks&amp;st=nyt&amp;oref=slogin &quot;Predicting Where You’ll Go and What You’ll Like&quot;], ''[[The New York Times]]'', New York, 22 June 2008. Retrieved on 2010-05-12.&lt;/ref&gt;&lt;ref&gt;Sheridan, Barrett. [http://www.newsweek.com/id/186970/page/1 &quot;A Trillion Points of Data&quot;], ''[[Newsweek]]'', New York, 28 Feb 2009. Retrieved on 2010-05-12.&lt;/ref&gt;&lt;ref&gt;Baker, Stephen. [http://www.businessweek.com/magazine/content/09_10/b4122042889229.htm &quot;Mapping a New, Mobile Internet&quot;], ''[[Bloomberg Businessweek]]'', New York, 26 February 2009. Retrieved on 2010-05-12.&lt;/ref&gt;&lt;ref&gt;Markoff, John. [http://www.nytimes.com/2008/11/30/business/30privacy.html?scp=2&amp;sq=Greg+Skibiski&amp;st=nyt &quot;You're Leaving a Digital Trail. What About Privacy?&quot;], ''[[The New York Times]]'', New York, 29 November 2008. Retrieved on 2010-05-12.&lt;/ref&gt;

In 2009, Sense was named one of &quot;The 25 Most Intriguing Startups in the World&quot; by [[Bloomberg Businessweek]]&lt;ref&gt;Ante, Spencer. [http://images.businessweek.com/ss/09/11/1112_most_intriguing_companies/index.htm &quot;The World's Most Intriguing Startups&quot;], ''[[Bloomberg BusinessWeek]]'', New York, 12 November 2009. Retrieved on 2010-05-12.&lt;/ref&gt; and was called &quot;The Next Google&quot; on the cover of [[Newsweek]].&lt;ref&gt;[http://hd.media.mit.edu/newsweek2_03.09.09.pdf]&lt;/ref&gt;

==History==
Sense Networks was founded by [[Greg Skibiski]] in February 2006 near his home in [[Northampton, Massachusetts]]. After establishing an office in [[NoHo]], [[New York City]] near [[Silicon Alley]], Skibiski recruited [[Alex Pentland]], Director of Human Dynamics Research and former Academic Head of the [[MIT Media Lab]], [[Tony Jebara]], Associate Professor and Head of the Machine Learning Laboratory at [[Columbia University]], and [[Christine Lemke]], who would later become co-founders.&lt;ref&gt;Junietz, Erika. [http://docs.google.com/viewer?a=v&amp;q=cache:hKKbZBAfYHYJ:www.sensenetworks.com/press/mit-tech-insider.pdf+skibiski+idea+created&amp;hl=en&amp;gl=us&amp;pid=bl&amp;srcid=ADGEESheMj8iT2tzutepbEVsOwFl3_EAsOKF-hZSF2RwUWQzE12nAX8N7-iUegcn_ooA1VchAzO_42c7qaDJcXIsQ1qBTV8H-1JUKdHyI1tj68FgQ5A3dw5o5MIxiH7ldPMLHfIEUCVz&amp;sig=AHIEtbT2N32A-ERj0s16J8Fx0LSD30FxZg &quot;A Sense of Place&quot;], ''MIT Technology Insider'', Boston, August 2008. Retrieved on 2010-05-14.&lt;/ref&gt;

Sense Networks investors include [[Intel Capital]], [http://javelinvp.com Javelin Venture Partners], and [[Kenan Altunis]].&lt;ref&gt;[http://www.intel.com/capital/news/releases/090630.htm &quot;Press Release: Sense Networks Secures Series B Funding for Location Analytics, Led by Intel Capital&quot;], ''[[Intel Capital]]'', New York, 30 June 2009. Retrieved on 2010-05-13.&lt;/ref&gt;

Founder [[Greg Skibiski]] was pushed out by [[lead investor]] [[Intel Capital]]&lt;ref&gt;Baker, Stephen. [http://thenumerati.net/index.cfm?postID=489 &quot;Data correlation: Used-car customers drop cell-phone service?&quot;], ''[[The Numerati]]'', New York, 8 January 2010. Retrieved on 2010-05-14.&lt;/ref&gt; in November 2009 following the company's B round of financing, the same week company won the Emerging Communications Conference &quot;Company to Watch&quot; Award.&lt;ref&gt;[http://www.sensenetworks.com/pr_11022009.php &quot;Sense Networks Wins the Emerging Communications Conference &amp; Awards Inaugural &quot;Company to Watch&quot; Award&quot;], ''[[Press Release]]'', New York, 2 November 2009. Retrieved on 2010-05-16.&lt;/ref&gt;

The company has three published [[patent applications]] for analyzing [[sensor]] data streams, System and Method of Performing Location Analytics (US 20090307263), Comparing Spatial-Temporal Trails in Location Analytics (US 20100079336), and Anomaly Detection in Sensor Analytics (US 20100082301).&lt;ref&gt;[http://appft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=/netahtml/PTO/search-bool.html&amp;r=0&amp;f=S&amp;l=50&amp;TERM1=skibiski&amp;FIELD1=&amp;co1=AND&amp;TERM2=&amp;FIELD2=&amp;d=PG01 US Patent &amp; Trademark Office]. Retrieved on 2010-05-12.&lt;/ref&gt;

==Products and services==
The Citysense consumer application, that shows hotspots of human activity in real-time from mobile phone location and taxicab GPS data,&lt;ref&gt;Silver, James. [http://www.wired.co.uk/wired-magazine/archive/2009/06/features/the-hidden-persuaders-mining-your-mobile-phone-log.aspx &quot;The Hidden Persuaders&quot;], ''[[Wired Magazine]]'', London, 22 June 2009. Retrieved on 2010-05-12.&lt;/ref&gt;    was named by [[ReadWriteWeb]] (in The [[New York Times]]) as &quot;Top 10 Internet of Things Products of 2009&quot;.&lt;ref&gt;Macmanus, Richard. [http://www.nytimes.com/external/readwriteweb/2009/12/08/08readwriteweb-top-10-internet-of-things-products-of-2009-74048.html &quot;Top 10 Internet of Things Products of 2009&quot;], ''[[The New York Times]]'', New York, 8 December 2009. Retrieved on 2010-05-13.&lt;/ref&gt;

The Cabsense consumer application, that shows the best place to catch a New York City taxicab based GPS data from the vehicles, launched in March 2010.&lt;ref&gt;Grynbaum, Michael. [http://www.nytimes.com/2010/04/03/nyregion/03icab.html &quot;Need a Cab? New Analysis Shows Where to Find One&quot;], ''[[The New York Times]]'', New York, 2 April 2010. Retrieved on 2010-05-16.&lt;/ref&gt;

The Macrosense platform is for mobile application providers and mobile phone carriers to analyze billions of customer location data points for predictive analytics in advertising and churn management applications.

==Privacy &amp; Data Ownership==

The company's privacy and data ownership policies are based on The New Deal on Data, as advocated by Alex &quot;Sandy&quot; Pentland, head of the Human Dynamics group at MIT.

==See also==
*[[Geosocial Networking]]
*[[Location-based service]]
*[[Location awareness]]

==References==
{{reflist}}

==External links==
* [http://www.sensenetworks.com Sense Networks website]
* [http://www.cabsense.com CabSense website]
* [http://www.citysense.com CitySense website]

[[Category:Android (operating system) software]]
[[Category:IOS software]]
[[Category:Mobile software]]
[[Category:Applied machine learning]]
[[Category:Big data]]</text>
      <sha1>jqjery690auz7j5y783h8pmcmtqno28</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Sumo Logic</title>
    <ns>0</ns>
    <id>35061359</id>
    <revision>
      <id>600153706</id>
      <parentid>597775636</parentid>
      <timestamp>2014-03-18T13:24:22Z</timestamp>
      <contributor>
        <username>Mean as custard</username>
        <id>10962546</id>
      </contributor>
      <comment>tone down promotional material</comment>
      <text xml:space="preserve" bytes="6320">{{advert|date=January 2013}}
{{Infobox company
| name       = Sumo Logic
| type       = [[Privately held company]]
| logo       = 
| industry   = [[Computer software]], security management, [[Enterprise software]]
| foundation = 2010
| founder    = Kumar Saurabh, Christian Beedgen
| location   = [[Redwood City, California]], [[United States|USA]]
| key_people = {{unbulleted list
 | Vance Loiselle {{small|(CEO)}}
 | Kumar Saurabh {{small|(Co-Founder &amp; Vice President of Analytics)}}
 | Christian Beedgen {{small|(CTO, Co-founder)}}
 | Bruno Kurtic {{small|(Founding Vice President of Product and Strategy)}}
}}
| num_employees = 
| revenue       = 
| net_income    = 
| products      = 
| homepage      = [http://www.sumologic.com// sumologic.com]
| footnotes     = 
}}

'''Sumo Logic''' is a cloud-based [[log management]] and analytics service that leverages [[Big Data]] to deliver real-time IT insights, headquartered in [[Redwood City, California|Redwood City]], [[California]].&lt;ref&gt;http://www.eweek.com/c/a/Application-Development/Cloud-Analytics-Startup-Sumo-Logic-Exits-Stealth-Raises-15M-574795/&lt;/ref&gt;Sumo Logic was founded in April 2010 by Kumar Saurabh and Christian Beedgen, and received funding from [[Accel Partners]], [[Greylock Partners]], [[Sutter Hill Ventures]] and angel investor Shlomo Kramer.&lt;ref&gt;http://www.marketwire.com/press-release/sumo-logic-raises-15m-series-b-round-next-generation-log-management-analytics-1612992.htm&lt;/ref&gt; The round of series C funding announced in November 2012 brings the company’s total venture capital backing to $50.5 million.&lt;ref&gt;http://www.marketwire.com/press-release/Sumo-Logic-Raises-30-Million-Series-C-Funding-Help-Enterprises-Drive-Actionable-1730970.htm/&lt;/ref&gt;

==Technology==
Sumo Logic’s architecture features an elastic petabyte scale platform that collects, manages and analyzes enterprise log data, reducing millions of log lines into operational info in real time.&lt;ref&gt;http://techcrunch.com/2012/01/31/log-data-management-and-analytics-startup-sumo-logic-raises-15m-from-greylock-and-others/&lt;/ref&gt; Their cloud-based approach overcomes the inherent problems of premise-based [[solution]]s, including limits on scalability, inefficient or haphazard analysis, and uncontrolled costs.&lt;ref&gt;http://www.dailydisruption.com/2012/02/disruptor-of-the-day-sumo-logic-harnessing-the-power-of-big-data-for-real-time-it/&lt;/ref&gt; 

Sumo Logic is built around a globally distributed data retention architecture that keeps all log data available for instant analysis, eliminating the need for an enterprise to manage the cost and complexity of data archiving, backups and restoration.&lt;ref&gt;http://www.dailydisruption.com/2012/02/disruptor-of-the-day-sumo-logic-harnessing-the-power-of-big-data-for-real-time-it/&lt;/ref&gt;

The service  is entirely cloud-based and is maintenance free.&lt;ref&gt;http://venturebeat.com/2012/01/31/sumo-logic-raises-15m-drops-its-stealthy-status/&lt;/ref&gt;

Sumo Logic modeled its approach on that of [[Google]], according to Christian Beedgen, the company’s CTO and one of its Cofounders. Sumo Logic uses advanced machine learning algorithms to whittle down mountains of log file data into common groupings, much the way Google News categorizes new stories distributed across the web, making it easier for administrators to synthesize the information.&lt;ref&gt;http://servicesangle.com/blog/2012/01/31/sumo-logic-emerges-from-stealth-to-take-on-splunk-log-data-analytics/&lt;/ref&gt;

In June 2012, Sumo Logic announced Sumo Logic Free, a [[freemium]] full functionality edition of its analytical solution that is deployed on [[Amazon Web Services]],&lt;ref&gt;http://siliconangle.com/blog/2012/06/26/sumo-logic-goes-freemium-with-data-analysis/&lt;/ref&gt; and in August 2012, the company announced Sumo Logic for VMware, which allows enterprises to search, visualize and analyze all [[VMware]] logs in real time so they can monitor and detect events within VMware virtual environments.&lt;ref&gt;http://www.talkincloud.com/sumo-logic-for-vmware-digs-deep-into-real-time-insights/&lt;/ref&gt;

==Leadership==
Sumo Logic was founded in 2010 by a technical leadership team with expertise in log management, scalable systems, Big Data and security, including:
* Kumar Saurabh, Acting CEO, Co-Founder &amp; Vice President of Analytics, formerly of [[ArcSight]]
* Christian Beedgen, Co-Founder &amp; CTO, formerly of [[ArcSight]]
* Bruno Kurtic, Founding Vice President of Product and Strategy, formerly of [[Sensage]]

Vance Loiselle, formerly of [[BladeLogic]], joined Sumo Logic as Chief Executive Officer in May 2012.&lt;ref&gt;http://www.marketwire.com/press-release/sumo-logic-appoints-vance-loiselle-as-chief-executive-officer-1660745.htm&lt;/ref&gt;

In April 2012, Sumo Logic formed a new advisory board, bringing in three [[Silicon Valley]] veterans: DJ Patil of Greylock Partners, and security experts Gerhard Eschelbeck of [[Sophos]] and Nir Zuk of [[Palo Alto Networks]].&lt;ref&gt;http://siliconangle.com/blog/2012/04/12/sumo-logic-brings-data-rock-stars-to-advisory-board/&lt;/ref&gt;

==Patents==
Sumo Logic’s service is powered by patent-pending Elastic Log Processing and LogReduce technologies.&lt;ref&gt;http://www.eweek.com/c/a/Application-Development/Cloud-Analytics-Startup-Sumo-Logic-Exits-Stealth-Raises-15M-574795/&lt;/ref&gt;

==Awards and Recognition==
In January, 2012, RSA named Sumo Logic one of its top 10 finalists for the Most Innovative Company at RSA.&lt;ref&gt;http://www.businesswire.com/news/home/20120105005264/en/Top-10-Finalists-Announced-Innovative-Company-RSA®&lt;/ref&gt; At the April 2012 Under the Radar Conference, Sumo Logic received the Judge's Choice and the Audience Choice Awards for Performance Monitoring.&lt;ref&gt;http://www.undertheradarblog.com/?id=11&lt;/ref&gt; In May 2012, Sumo Logic was named a Red Herring Americas 2012 Top 100 Winner.&lt;ref&gt;http://www.redherring.com/red-herring-americas/americas-2012-finalists/&lt;/ref&gt;

==See also==
* [[Accelops]]
* [[ArcSight]]
* [[LogLogic]]
* [[Sematext]]
* [[Splunk]]

==References==
{{reflist}}

[[Category:System administration]]
[[Category:Data security]]
[[Category:Computer security companies]]
[[Category:Information technology companies of the United States]]
[[Category:Privately held companies based in California]]
[[Category:Companies established in 2010]]
[[Category:Big data]]
[[Category:Companies based in Redwood City, California]]</text>
      <sha1>jup0sk0p3yjf8ixj8sbmizc9styc70n</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Talend</title>
    <ns>0</ns>
    <id>24143094</id>
    <revision>
      <id>600223446</id>
      <parentid>599109430</parentid>
      <timestamp>2014-03-18T22:19:16Z</timestamp>
      <contributor>
        <ip>67.101.124.14</ip>
      </contributor>
      <comment>/* Competitors */</comment>
      <text xml:space="preserve" bytes="13618">{{Infobox company
| name             = Talend
| logo             = [[File:Talend logo.png|170px]]
| type             = Private
| genre            =
| fate             =
| predecessor      =
| successor        =
| foundation       = 2006
| founder          =
| defunct          =
| location_city    = [[Suresnes]], France &amp; [[Los Altos, California]], US
| location_country =
| location         =
| locations        =
| area_served      = Worldwide
| key_people       = Mike Tuchen, CEO / Bertrand Diard, co-founder &amp; CSO / Fabrice Bonan, co-founder &amp; CPO
| industry         = Computer Software
| products         = Big Data, Data Integration, Data Quality, MDM, ESB, BPM
| production       =
| services         =
| revenue          =
| operating_income =
| net_income       =
| aum              =
| assets           =
| equity           =
| owner            =
| num_employees    = 400
| parent           =
| divisions        =
| subsid           =
| homepage         = http://www.talend.com/
| footnotes        =
| intl             =
}}
'''Talend''' is an [[open source]] software vendor that provides [[data integration]], [[data management]], [[enterprise application integration]] and [[big data]] software and services. Headquartered in Suresnes, France and Los Altos, California, Talend has offices in North America, Europe and Asia, and an international network of technical and services partners.  Customers include [[eBay]], [[Virgin Mobile]], [[Sony Online Entertainment]], [[Deutsche Post]] and [[Allianz]].{{citation needed|date=October 2013}} It has 400 employees in 14 offices in 7 countries.{{citation needed|date=October 2013}}

== History ==
Talend was founded in 2005 by Bertrand Diard and Fabrice Bonan. It was the first [[commercial open source software|commercial open source]] vendor of data integration software. Other vendors have since entered this market, including [[Apatar]], [[Jitterbit]], [[Pentaho]], and [[SnapLogic]].  Non open source data integration vendors include [[Software AG]], [[Ab Initio (company)|Ab Initio]], [[SAS Institute]], [[Pervasive Software]], [[IBM]], [[Informatica]], and [[SAP AG|SAP]].

The company's first product, [[Talend Open Studio for Data Integration]],&lt;ref&gt;[http://www.networkworld.com/news/2007/042407-talend-data-integration.html?page=1 Open source start-up upgrades data integration software]. Networkworld.com (2007-04-24). Retrieved on 2013-01-10.&lt;/ref&gt; was launched in October 2006, under its previous name: ''Talend Open Studio''. In January 2008, it had been downloaded over 1 million times. In January 2012, the product totaled 20 million downloads and the company had over 3500 clients around the world.&lt;ref&gt;[http://apache.ulitzer.com/node/2294767 Talend’s Enterprise Manageability Capabilities Streamline Hadoop Deployments]. Apache.ulitzer.com. Retrieved on 2013-01-10.&lt;/ref&gt;

Talend is backed by six [[venture capital]] firms. The first two rounds of funding were provided by AGF Private Equity and Galileo Partners. In January 2009, [[Bernard Liautaud]], the founder of Business Objects, led a $12 million round C&lt;ref&gt;[http://www.latribune.fr/journal/archives/edition-du-2301/126471/bernard-liautaud-finance-lediteur-de-logiciels-talend.html Bernard Liautaud finance l'éditeur de logiciels Talend]. LaTribune.fr. Retrieved on 2013-01-10.&lt;/ref&gt;&lt;ref&gt;[http://www.techcrunch.com/2009/01/26/more-validation-for-open-source-software-talend-raises-12-million/ More Validation For Open Source Software: Talend Raises $12 Million]. TechCrunch (2009-01-26). Retrieved on 2013-01-10.&lt;/ref&gt;&lt;ref&gt;[http://www.itbusinessedge.com/cm/community/news/inf/blog/talends-fundraising-validates-open-source-data-integration/?cs=30028 Talend's Fundraising Validates Open Source Data Integration]. Itbusinessedge.com (2009-01-26). Retrieved on 2013-01-10.&lt;/ref&gt;&lt;ref&gt;[http://www.sdtimes.com/DATA_INTEGRATOR_TALEND_SCORES_12M_IN_FUNDING/About_DATAINTEGRATION_and_OPENSOURCE_and_TALEND/33208 Data integrator Talend scores $12M in funding]. Sdtimes.com. Retrieved on 2013-01-10.&lt;/ref&gt; for his firm [[Benchmark Capital|Balderton Capital]]. In November 2010, [[Silver Lake Partners|Silver Lake Sumeru]] led a $34 million round and Talend announced at the same time the acquisition of Sopera, a strategic member of the [[Eclipse Foundation]].&lt;ref&gt;Flinn, Ryan. (2010-11-10) [http://www.bloomberg.com/news/2010-11-10/silver-lake-sumeru-invests-in-data-management-company-talend.html Silver Lake Sumeru Invests in Data Management Company Talend]. Bloomberg.com. Retrieved on 2013-01-10.&lt;/ref&gt;&lt;ref&gt;[http://techcrunch.com/2010/11/10/open-source-software-company-talend-raises-34m-acquires-sopera/ Open Source Software Company Talend Raises $34M; Acquires Sopera]. Techcrunch.com (2010-11-10). Retrieved on 2013-01-10.&lt;/ref&gt;&lt;ref&gt;[http://www.france24.com/en/20101113-bertrand-diard-ceo-talend-internet-data-softwar-open-source Bertrand DIARD, CEO and Co-founder of Talend]. France24.com. Retrieved on 2013-01-10.&lt;/ref&gt;&lt;ref&gt;[http://www.latribune.fr/blogs/blog-initie/20101110trib000571525/nouvelle-etape-de-croissance-pour-talend.html Nouvelle étape de croissance pour Talend]. Latribune.fr. Retrieved on 2013-01-10.&lt;/ref&gt; In May 2011 Talend introduced its Unified Integration Platform – an important milestone subsequent to the acquisition.  In June 2011 the company also released Talend Cloud to support all forms of Cloud-based computing. In February 2012, Talend Open Studio for Big Data, an integration application for big data, was released. In December 2013, Talend raised $40 million from Bpifrance, [[Iris Capital]] and [[Silver Lake Partners|Silver Lake Sumeru]].&lt;ref&gt;[http://techcrunch.com/2013/12/11/talend-raises-40m-to-more-aggressively-extend-into-big-data-market-sets-sights-on-ipo/ Talend Raises $40M To More Aggressively Extend Into Big Data Market, Sets Sights On IPO]&lt;/ref&gt;

In 2011 the company was listed on the Momentum Index of venture-backed companies with an open source business model.&lt;ref&gt;http://momentumindex.com/96-open-source-start-ups-ranked-by-momentum-index-3/&lt;/ref&gt;

Today, Talend has more than 4000 paying customers including eBay, Virgin Mobile, Sony Online Entertainment, Deutsche Post and Allianz.{{citation needed|date=October 2013}}

== Products ==

=== Data management ===
[[File:Talend Open Studio.jpg|thumb|[[software]] Talend Open Studio for Data Integration Screenshot]]
* ''Talend Open Studio for Big Data'': combining big data technologies into a unified open source environment simplifying the loading, extraction, transformation and processing of large and diverse data sets
* ''Talend Enterprise Big Data'': a big data integration solution that extends Talend Open Studio for Big Data with teamwork and management features
* ''Talend Platform for Big Data'': a powerful and versatile big data integration and data quality solution that simplifies the loading, extraction and processing of large and diverse data sets so you can make more informed and timely decisions

* ''[[Talend Open Studio for Data Integration]]'': an open source application for data integration job design with a graphical development environment
* ''Talend Enterprise Data Integration'': extends Talend Open Studio for Data Integration with technical support and additional features
* ''Talend Platform for Data Management'': turn disparate, duplicate sources of data into trusted stores of consolidated information
* ''Talend Platform for Data Services'': a comprehensive unified data, application and service integration solution that lessens the impact of changing data structures while making the management of data across domains easier.

* ''Talend Open Studio for MDM'': a set of functions for master data management that provides functionality for integration, quality, governance, mastering and collaborating on enterprise data
* ''Talend Platform for Master Data Management'': turn disparate, inconsistent information across a business into a single, reliable “version of the truth”, providing increased confidence in decisions made
* ''[[Talend Open Studio for Data Quality]]'': an open source data profiling tool that examines the content, structure and quality of complex data structures

=== Application integration ===
[[File:Talend Open Profiler.gif|thumb|[[software]] Talend Open Studio for Data Quality Screenshot]]
* ''Talend ESB Standard Edition'': an Apache-based open source [[enterprise service bus]]
* ''Talend Open Studio for [[Enterprise service bus|ESB]]'': an [[enterprise service bus]] and a standards-based connectivity layer used to integrate distributed systems across functional, enterprise and geographic boundaries. It is powered by the [[Apache CXF]], [[Apache Camel]] and [[Apache ActiveMQ]] open source integration projects
* ''Talend Enterprise ESB'': extends Talend Open Studio for ESB with advanced deployment and management functions
* ''Talend Platform for Enterprise Integration'': unifies business process management, application integration and data management allowing firms to increase business productivity, deliver projects faster, and lower operating costs

=== Cloud based integration ===
[[File:Talend MDM.gif|thumb|[[software]] Talend Open Studio for MDM Screenshot]]
* ''Talend Platform for Hybrid Cloud'': Talend Cloud is a comprehensive and unified set of application integration and data management tools that enable to rapidly connect, mediate and manage on-premise, cloud-based and SaaS applications resulting in a more responsive business.&lt;ref&gt;{{cite web|last=Gardner|first=Dana|title=Talend open-source approach provides holistic integration capability across, data, devices, services|url=http://www.zdnet.com/blog/gardner/talend-open-source-approach-provides-holistic-integration-capability-across-data-devices-services/4245|work=ZD Net|publisher=ZD Net|accessdate=June 29, 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Dubray|first=Jean-Jacques|title=Talend Releases a Cloud Version of its Unified Integration Platform|url=http://www.infoq.com/news/2011/06/talend-cloud-uip|work=InfoQ|publisher=InfoQ|accessdate=June 13, 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Vaughan|first=Jack|title=Integration platform set for cloud, on-premise and hybrid setups|url=http://searchsoa.techtarget.com/news/2240036932/Unified-integration-platform-set-for-cloud-on-premise-and-hybrid-computing|work=SearchSOA.com|publisher=SearchSOA.com|accessdate=June 14, 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Kanaracus|first=Chris|title=Options for cloud, SaaS data integration grow|url=http://www.networkworld.com/news/2011/060811-options-for-cloud-saas-data.html?hpg1=bn|work=Network World|publisher=Network World|accessdate=June 8, 2011}}&lt;/ref&gt;

== Community ==
Talend is an [[Apache Software Foundation]] sponsor and many of its engineers are contributors to Apache including [[CXF]], [[Apache Camel|Camel]], [[ServiceMix]], Karaf, Santuario, and [[ActiveMQ]].{{citation needed|date=October 2013}} The company is also a member of the [[Java Community Process]] (JCP) and is a Strategic Developer Member of the [[Eclipse Foundation]], a Corporate Member of [[OW2]], and has active involvement in the [[OASIS (organization)|OASIS]] organization.{{citation needed|date=October 2013}}

Talend publishes the code of its core modules under the [[GNU Public License]] or the [[Apache License]]. Talend is also a contributor to key open source projects{{which|date=October 2013}}.

[[Java (programming language)|Java]] is the development language of Talend’s products and services.

Its commercial partners include [[Bonitasoft]], [[Cloudera]], [[Greenplum]], [[Google]], [[Hortonworks]], [[Impetus Technologies]],&lt;ref&gt;http://www.talend.com/ecosystem/company/impetus&lt;/ref&gt; [[Jaspersoft]], [[Netezza]], [[Teradata]] and [[Vertica]].  Uniserv&lt;ref&gt;http://www.uniserv.com/en/news-data-quality/news2011/aktuelles_040.php&lt;/ref&gt;

==Talendforge==
Talendforge.org is Talend’s technical community site. Sections available for users include a support [[Internet forum|forum]], a [[wiki]], [[Bugtracker]], an exchange, components, tutorials and the translation tool Babili.&lt;ref&gt;http://www.talendforge.org/ Talendforge&lt;/ref&gt;

== Competitors ==
Talend's competitors include:
* Application Integration: [[IBM]],[[MS BI]], [[Oracle Corporation|Oracle]], [[Software AG]], [[TIBCO]], [[Progress Software]], [[SnapLogic]]
* Data Management: [[Informatica]], [[IBM]], [[Oracle Corporation|Oracle]], Semarchy, [[Red Hat]]
* ESB: [[MuleSoft]], [[Fusesource]], [[WSO2]]

== License ==
Talend uses the [[open core]] [[business model]].&lt;ref&gt;Aslett,Matthew (2009-07-08) [http://web.archive.org/web/20100801004103/http://blogs.the451group.com/opensource/2009/07/08/what-is-open-core-licensing-and-what-isnt/ What is Open Core Licensing (and what isn’t)]. blogs.the451group.com&lt;/ref&gt; Talend publishes the code of its core modules under open source licenses including the [[GNU Public License]], LGPL and the [[Apache License]], and value added features and services under a commercial subscription license.

The commercial subscription license includes:
* Access to value added features (such as teamwork, [[Load balancing (computing)|load balancing]], [[Network monitoring|monitoring]])
* Technical support
* IP indemnification (legal protection)

== See also ==
* [[Talendforge]]

== References ==
{{reflist|35em}}

==External links==
{{Commons|Talend}}
* {{official website|http://www.talend.com/}}
* Talend Help Center:https://help.talend.com/

[[Category:Enterprise application integration]]
[[Category:Business intelligence]]
[[Category:Software companies based in California]]
[[Category:Data warehousing products]]
[[Category:Talend|*]]
[[Category:Big data]]</text>
      <sha1>d5qq0qirayw1euw1wu1m7oodfnb82kl</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Teradata</title>
    <ns>0</ns>
    <id>168680</id>
    <revision>
      <id>601424052</id>
      <parentid>599412790</parentid>
      <timestamp>2014-03-26T22:52:30Z</timestamp>
      <contributor>
        <ip>24.4.122.81</ip>
      </contributor>
      <comment>/* Partners */</comment>
      <text xml:space="preserve" bytes="32251">{{newsrelease |date=September 2013}}{{advert|date=September 2013}}{{Original research|date=September 2013}} {{Use mdy dates|date=January 2013}}
{{Infobox company
| company_name   = Teradata Corporation
| company_logo   = [[Image:TeradataLogoVertical.gif|200px]]
| company_type   = [[Public company|Public]]
| traded_as      = {{nyse|TDC}}&lt;br /&gt;[[S&amp;P 500|S&amp;P 500 Component]]
| company_slogan = Raising Intelligence
| foundation     = 1979
| location       = [[Miami Township, Montgomery County, Ohio|Miami Township]], [[Ohio]]
| key_people     = [[Michael F. Koehler]], President and CEO
| num_employees  = 10,200 (January 2014)&lt;ref name=macroaxis&gt;{{cite web |url=http://www.macroaxis.com/invest/ratio/TDC--Number_of_Employees |title=Teradata Number of Employees TDC NYSE |accessdate=2014-01-12}}&lt;/ref&gt;
| industry       = [[Data warehousing]]
| products       = Integrated Data Warehouse Hardware and Software, Professional Services, Customer Services
| revenue        = $2.692 billion [[United States dollar|USD]] (2013)
| net_income     = $303 million USD (2012)
| homepage       = {{URL|http://www.teradata.com}}
}}

'''Teradata Corporation''' is an American computer company that sells analytic data platforms, applications and related services. Its products are meant to consolidate data from different sources and make the data available for analysis. Formerly a division of [[NCR Corporation]], Teradata was incorporated in 1979 and separated from NCR in October 2007.&lt;ref&gt;{{cite web |url=http://www.ncr.com/about_ncr/media_information/news_releases/2007/october/100107a.jsp |title=NCR Completes Teradata Spin Off |date=October 1, 2007}}&lt;/ref&gt; Michael Koehler became president and CEO of Teradata after its 2007 spin off from NCR.&lt;ref&gt;^ Hallett, Tony. &quot;Q&amp;A: Teradata CEO Mike Koehler&quot;. Interview 10/22/07. Silicon.com. Retrieved 20 December 2011.&lt;/ref&gt;  Teradata's headquarters is located in [[Miamisburg, Ohio|Miamisburg]], [[Ohio]].

==Corporate Overview==

Teradata is an [[enterprise software]] company that develops and sells a [[relational database management system]] (RDBMS) with the same name. Teradata is publicly traded on the New York Stock Exchange (NYSE) NYSE  under the stock symbol TDC. &lt;ref name=NCRspinoff&gt;{{cite press release |url=http://www.ncr.com/about_ncr/media_information/news_releases/2007/october/100107a.jsp |publisher=NCR |date=October 1, 2007 |title=NCR Completes Teradata Spin Off}}&lt;/ref&gt;

The Teradata product is referred to as a &quot;data warehouse system&quot; and stores and manages data. The data warehouses use a &quot;shared nothing architecture which means that each server node has its own memory and processing power.&lt;ref&gt;http://www.teradatatech.com/?p=103&lt;/ref&gt;  Adding more [[server (computing)|server]]s and nodes increases the amount of data that can be stored. The [[database software]] sits on top of the servers and spreads the workload among them.&lt;ref name=&quot;Lawson&quot;&gt;{{cite web|last=Lawson|first=Loraine|title=How Business Logic Modeling Helps Data Warehouse Integration|url=http://www.itbusinessedge.com/cm/community/features/interviews/blog/how-business-logic-modeling-helps-data-warehouse-integration/?cs=43396|work=IT Business Edge|accessdate=December 13, 2011}}&lt;/ref&gt; Teradata sells applications and software to process different types of data. In 2010, Teradata added [[text analytics]] to track [[unstructured data]], such as word processor documents, and semi-structured data, such as spreadsheets.&lt;ref&gt;{{cite web|last=Sambandaraksa|first=Don|title=Teradata launches new text analytics|url=http://www.bangkokpost.com/tech/computer/210208/teradata-launches-new-text-analytics|date=August 12, 2010|work=Bangkok Post|accessdate=December 13, 2011}}&lt;/ref&gt;

Teradata's product can be used for business analysis. Data warehouses can track company data, such as sales, customer preferences, product placement, etc.&lt;ref name=&quot;Lawson&quot;/&gt;

Teradata has a supplier diversity program that designates a minimum of 3 to 5% of spending on minority, women, veteran, or small business vendors.&lt;ref&gt;{{cite news|title=Teradata Targets Diverse Suppliers|url=http://www.bizjournals.com/dayton/stories/2009/11/02/story1.html |work=Dayton Business Journal|accessdate=December 13, 2011|first=Joe|last=Cogliano|date=November 2, 2009}}&lt;/ref&gt;

In 2013, the Ethisphere Institute named Teradata as one of the &quot;World's Most Ethical Companies.&quot;&lt;ref&gt;http://ethisphere.com/worlds-most-ethical/wme-honorees/&lt;/ref&gt;

==History==
Timeline information taken from Teradata company history unless otherwise cited.&lt;ref&gt;{{cite web|title=History|url=http://www.teradata.com/history/|publisher=Teradata company website|accessdate=December 13, 2011}}&lt;/ref&gt;

*1976–1979: concept of Teradata grows from research at [[California Institute of Technology]] (Caltech) and from the discussions of [[Citibank]]'s advanced technology group.&lt;ref name=&quot;tdhist&quot;&gt;[http://www.teradata.com/t/go.aspx/page.html?id=42649 Teradata Milestones]&lt;/ref&gt;
*Incorporated in 1979 in [[Brentwood, CA]] by Dr. Jack E. Shemer, Dr. Philip M. Neches, Walter E. Muir, Jerold R. Modes, William P. Worth, and Carroll Reed.
*1980: Enough funding for a research and development team.
*1983: First beta system shipped to [[Wells Fargo Bank]].
*1984: Teradata releases the world's first parallel data warehouses and data marts.&lt;ref&gt;{{cite web|last=Pereira|first=Brian|title=Marrying Strategic Intelligence with Operational Intelligence|url=http://www.informationweek.in/Archive/10-01-01/Marrying_Strategic_Intelligence_with_Operational_Intelligence.aspx|date= January 1, 2010|work=InformationWeek|accessdate=December 13, 2011}}&lt;/ref&gt;
*1986: [[Fortune Magazine]] names Teradata &quot;Product of the Year.&quot;
*1987: Teradata [[initial public offering]] in August
*1989: Teradata partners with NCR to build new database computers.
*September 1991: [[AT&amp;T Corporation]] acquires [[NCR Corporation|NCR]].&lt;ref name=&quot;Andrews&quot;&gt;{{cite news|last=Andrews|first=Edmund|title=AT&amp;T Acquisition, Soon to be Spun Off, Regains NCR Name|url=http://www.nytimes.com/1996/01/11/business/at-t-acquisition-soon-to-be-spun-off-regains-ncr-name.html |work=The New York Times|accessdate=December 13, 2011|date=January 11, 1996}}&lt;/ref&gt;
*December 1991: NCR announces acquisition of Teradata.&lt;ref&gt;{{cite news|title=NCR, Teradata to split up|url=http://money.cnn.com/2007/01/08/news/companies/ncr/index.htm|publisher=CNN Money|accessdate=December 13, 2011|date=January 8, 2007}}&lt;/ref&gt;
*1992: Teradata creates the first system over 1 [[terabyte]], which goes live at [[Wal-Mart]].&lt;ref&gt;{{cite web|last=Hubler|first=David|title=Teradata sees revenue growth in data consolidation|url=http://washingtontechnology.com/articles/2010/12/13/teradata-data-consolidation.aspx |publisher=Washington Technology|accessdate=December 13, 2011}}&lt;/ref&gt;
*1994: Gartner names Teradata the &quot;Leader in Commercial Parallel Processing.&quot;
*1995: IDC consulting group names Teradata number one in massively parallel processing in Computerworld Magazine.
*1996: A Teradata database becomes the world's largest database at 11 terabytes.
*1997: NCR becomes independent from AT&amp;T.&lt;ref name=&quot;Andrews&quot;/&gt;
*1997: Teradata customer creates world's largest production database at 24 terabytes.
*1998: Teradata ported to [[Microsoft Windows]] NT.
*1999: Teradata customer has world's largest database with 130 terabytes.On November 4, 1999, Survey.com announced that NCR's Teradata product received the highest ranking overall in a survey of data warehouse implementers.&lt;ref&gt;{{cite web |url=https://www.lexis.com/research/retrieve?_m=9d74a0e30c35d1d3f77134bff7404b82&amp;docnum=9&amp;_fmtstr=FULL&amp;_startdoc=1&amp;wchp=dGLzVzB-zSkAz&amp;_md5=04d9a9984aa7f21c7035f2de4543447c |title= NCR's Teradata Product Receives Highest Data Warehouse Customer Satisfaction Rating in DBS III North American Survey
}}&lt;/ref&gt;
*2000: NCR acquires Ceres Integrated Solutions and reconfigures their customer relationship management software into Teradata CRM.&lt;ref&gt;{{cite news|title=Company News; NCR Acquires Ceres Integrated in a $90 Million Deal.|url=http://www.nytimes.com/2000/04/12/business/company-news-ncr-acquires-ceres-integrated-in-a-90-million-deal.html|work=The New York Times|accessdate=December 13, 2011 |date= April 12, 2000}}&lt;/ref&gt;
*2000: NCR acquired Stirling Douglas Group and its demand chain management software.&lt;ref name=&quot;stirling&quot;&gt;{{cite news |work= Press release |title=NCR Completes Acquision of Sterling Douglas Group |publisher= NCR |url= http://investor.ncr.com/phoenix.zhtml?c=83840&amp;p=irol-newsArticle&amp;ID=104395 |date=July 14, 2000|accessdate= August 16, 2013  }}&lt;/ref&gt;
*2002: Teradata Warehouse 7.0 was announced.
*2003: More than 120 companies migrate from [[Oracle Corporation]] to Teradata after Oracle-to-Teradata migration program.&lt;ref&gt;{{cite web|title=Teradata Expands Successful Offer to Migrate Oracle Users|url=http://www.computerweekly.com/Articles/2009/04/27/235806/Users-welcome-integration-of-Netweaver-and-Teradata.htm|date= April 27, 2009|publisher=Information Management Online|accessdate=December 13, 2011}}&lt;/ref&gt;
*2003: Teradata University is created. Nearly 170 universities in 27 countries included in network.
*2004: Teradata creates partnerships with SAP&lt;ref&gt;{{cite web|last=Grant|first=Ian|title=Users welcome integration of Netweaver and Teradata|url=http://www.computerweekly.com/Articles/2009/04/27/235806/Users-welcome-integration-of-Netweaver-and-Teradata.htm|date= April 27, 2009|work=Computer Weekly}}&lt;/ref&gt;  and Siebel Systems, Inc.&lt;ref&gt;{{cite web|last=Campanelli|first=Melissa|title=Siebel Systems, Teradata Partner|url=http://www.dmnews.com/siebel-systems-teradata-partner/article/83975/|date=April 21, 2004|publisher=Direct Marketing News|accessdate=December 13, 2011}}&lt;/ref&gt;
*2005: Teradata launches Teradata Warehouse 8.1.
*2005: Teradata acquires DecisionPoint software&lt;ref&gt;{{cite web|title=Teradata Acquires DecisionPoint Software|url=http://www.information-management.com/news/1043424-1.html|date= December 5, 2005|publisher=Information Management Online}}&lt;/ref&gt; and rebrands it as Teradata Decision Experts.
*2005: Teradata adds [[Linux]] as an [[operating system]] choice for enterprise-class data warehouses.
*2006: Teradata launches Enterprise Master Data Management Solution.
*2006: [[Microsoft]] and Teradata collaborate on business intelligence application.&lt;ref&gt;{{cite press release |title=Microsoft and Teradata Collaborate to Offer Business Intelligence Solutions for Mutual Customers|url=http://www.microsoft.com/presspass/press/2007/jan07/01-15MSTeradataPR.mspx|publisher=Microsoft |date=January 15, 2007|accessdate=December 13, 2011}}&lt;/ref&gt;
*2007: NCR announces that NCR and Teradata will separate into two independent businesses.&lt;ref&gt;{{cite news|title=NCR, Teradata to split up|url=http://money.cnn.com/2007/01/08/news/companies/ncr/index.htm|date= January 8, 2007|publisher=CNN Money|accessdate=December 13, 2011}}&lt;/ref&gt;
*2007: Intelligent Enterprise magazine names Teradata the best global data warehouse-business intelligence appliance vendor.
*2007: Teradata partners with DFA Capital Management, Inc.&lt;ref&gt;{{cite press release|title=Teradata and DFA Capital Management Announce Partnership|url=https://www.dfa.com/us/News-Events/Press-Releases/2007/Teradata-and-DFA-Capital-Management-Inc.-Announce-Partnership|publisher=DFA Capital Management |date= May 8, 2007|accessdate=December 13, 2011}}&lt;/ref&gt;
*2007: [[Agilent Technologies]] and Teradata established the first partnership between a data warehouse company and an instrument measurement company in order to integrate network and customer data for telecommunications industry.&lt;ref&gt;{{cite press release |title=Agilent Technologies and Teradata Establish First-of-Its-Kind Partnership to Integrate Network, Customer Data for Telecommunications Industry|url=http://www.agilent.com/about/newsroom/presrel/2007/21may-em07092.html|publisher=Agilent Technologies |date=May 21, 2007|accessdate=December 13, 2011}}&lt;/ref&gt;
*2007: Teradata University network consists of 850 universities in 70 countries.
*2007: On Oct 1, Teradata completes spin-off from NCR and is traded as its own stock.  Mike Koehler becomes the CEO of Teradata.&lt;ref&gt;{{cite news|last=Hagerty|first=John|title=NCR to Spinoff Teradata|url=http://www.forbes.com/2007/01/09/amr-teradata-ncr-split-biz-logistics-cx_jh_0109tera.html|date=January 9, 2007|work=Forbes|accessdate=December 13, 2011}}&lt;/ref&gt;
*2007: Teradata launches Teradata 12
*2007: Teradata announces partnership with [[SAS (software)|SAS]] involving further technical integration of their respective products and coordinated marketing, sales, and services activities.&lt;ref&gt;{{cite web|title=SAS, Teradata Expand In-database Analytics Options.|url=http://www.efytimes.com/e1/38111/SAS,%20Teradata%20Expand%20In-database%20Analytics%20Options.htm|date=October 24, 2009|publisher=EFYTimes.com|accessdate=December 13, 2011}}&lt;/ref&gt;
*2008: Teradata acquires Claraview.&lt;ref&gt;{{cite web|last=Grimes|first=Seth|title=Teradata has Acquired BI/DW Firm Claraview|url=http://www.informationweek.com/blog/228900615|date=March 20, 2008|work=InformationWeek|accessdate=December 13, 2011}}&lt;/ref&gt;
*2008: Teradata Purpose Built Platform Family launches.
*2008: Teradata Petabyte Power Players announced: a group of five Teradata customers with data warehouse environments exceeding one [[petabyte]].&lt;ref&gt;{{cite web|last=Lai|first=Eric|title=Teradata creates elite club for petabyte-plus data warehouse customers|url=http://www.computerworld.com/s/article/9117159/Teradata_creates_elite_club_for_petabyte_plus_data_warehouse_customers|date=October 14, 2008|work=Computerworld}}&lt;/ref&gt;
*2008: Teradata Labs becomes the first to unveil a working prototype demonstrating the innovative use of solid state disk (SSD) drives in a data warehouse environment at the 2008 Teradata PARTNERS User Group Conference &amp; Expo.&lt;ref&gt;{{cite web|last=Jai|first=C.S.|title=Teradata Labs Unveils SSD Data Warehouse Solution.&quot; InfoTech Spotlight|url=http://it.tmcnet.com/topics/it/articles/42729-teradata-labs-unveils-ssd-data-warehouse-solution.htm|date=October 14, 2008|accessdate=December 13, 2011}}&lt;/ref&gt;
*2008: Teradata Accelerate launches
*2009: [[Forrester Research]] ranks Teradata as number 1 amongst enterprise data warehousing.
*2009: Teradata announces that database administrators can now rewind Teradata Viewpoint monitoring applications and portlets to check database activity or performance at a specific point in time.
*2009: Teradata ranked among BusinessWeek’s “InfoTech 100,&quot;&lt;ref&gt;{{cite web|title=The Infotech 100|url=http://www.businessweek.com/interactive_reports/it100_2009.html|year= 2009|publisher=Bloomberg Business Week|accessdate=December 13, 2011}}&lt;/ref&gt; the world’s best-performing tech companies” and Fortune’s 1000.&lt;ref&gt;{{cite news|title=Fortune 1000|url=http://money.cnn.com/magazines/fortune/fortune500/2009/companies/T.html|date= May 4, 2009|publisher=CNNMoney.com|accessdate=December 13, 2011}}&lt;/ref&gt;
*2009: Teradata Database 13 releases
*2010: Teradata introduces Teradata Extreme Data Appliance 1600.
*2010: Teradata is named “one of the world’s most ethical companies” by The Ethisphere Institute.&lt;ref&gt;{{cite news|last=Coster|first=Helen|title=The World’s Most Ethical Companies|url=http://archive.is/20130123134224/http://www.forbes.com/2010/03/22/ethisphere-ethical-companies-leadership-citizenship-100.html?partner=daily_newsletter|date=March 22, 2010|work=Forbes|accessdate=December 13, 2011|date=March 22, 2010}}&lt;/ref&gt;
*2011: Teradata acquires [[Aprimo]]&lt;ref&gt;{{cite web|last=Morgan|first=Timothy Prickett|title=Teradata eats Aprimo for $550m.|url=http://www.channelregister.co.uk/2010/12/22/teradata_buys_aprimo/|date=December 22, 2010|work=The Register|accessdate=December 13, 2011}}&lt;/ref&gt; and [[Aster Data Systems]].&lt;ref&gt;{{cite web|last=Morgan|first=Timothy Prickett|title=Teradata snaps up Aster Data for $263m|url=http://www.theregister.co.uk/2011/03/03/teradata_buys_aster_data/|date=March 3, 2011|work=The Register|accessdate=December 13, 2011}}&lt;/ref&gt;
*2011: Gartner names Teradata as the global leader in data warehousing databases.&lt;ref&gt;Henschen, 2011. http://www.informationweek.com/news/software/info_management/229215658?pgno=1&lt;/ref&gt;
*2011: Teradata adds Aprimo Real-Time Interaction Manager to its Marketing Solution suite.
*2012: Teradata acquires [[eCircle]], a direct marketing company with focus on email
*2013: Teradata is named a leader in Gartner's Data Warehouse DBMS Magic Quadrant in February 2013 &lt;ref&gt;{{cite web|title=Dataware house DBMS magic quadrant|url=http://e01.ams11.com/OutboundMessage.aspx?M=1790.5bbdb4c0-98ab-4244-819f-24f5388a725b|date=February 2, 2013|publisher=Dashboard Insight|accessdate=February 2, 2013}}&lt;/ref&gt;
*2013: Teradata Teradata Applications and ADAM Software Announce Strategic Global Partnership

==Technology and products==
Teradata is a [[massive parallelism (computing)|massively parallel processing]] system running a [[shared nothing architecture]].&lt;ref&gt;http://searchdatamanagement.techtarget.com/answer/How-to-select-an-MPP-database-DB2-vs-Teradata &lt;/ref&gt;  Its technology consists of [[computer hardware|hardware]], [[software]], database, and consulting. The system moves data to a [[data warehouse]] where it can be recalled and analyzed.&lt;ref&gt;{{cite web|last=Pereira|first=Brian|title=Marrying Strategic Intelligence with Operational Intelligence|url=http://www.informationweek.in/Archive/10-01-01/Marrying_Strategic_Intelligence_with_Operational_Intelligence.aspx|date= January 1, 2010|work=InformationWeek|accessdate=December 13, 2011}}&lt;/ref&gt;

The  systems can be used as back-up for one another during downtime, and in normal operation balance the work load across themselves.&lt;ref&gt;{{cite web|last=Howard|first=Philip|title=Dual Loading for Teradata|url=http://www.businesscomputingworld.co.uk/dual-loading-for-teradata/|date=September 8, 2010|publisher=BCW IT Leadership|accessdate=December 13, 2011}}&lt;/ref&gt;

In 2009, Forrester Research issued a report, &quot;The Forrester Wave: Enterprise Data Warehouse Platform,&quot; by James Kobielus,&lt;ref name=&quot;Kobielus&quot;&gt;[http://www.teradata.com/t/WorkArea/DownloadAsset.aspx?id=10115 &quot;The Forrester Wave: Enterprise Data Warehouse Platform,&quot;] by James Kobielus, February 6, 2009.&lt;/ref&gt; rating Teradata the industry's number one enterprise data warehouse platform in the &quot;Current Offering&quot; category.

Marketing research company [[Gartner Group]] placed Teradata in the &quot;leaders quadrant&quot; in its 2009, 2010, and 2012 reports, &quot;Magic Quadrant for Data Warehouse Database Management Systems&quot;.&lt;ref name=&quot;Gartner&quot;&gt;{{Cite news |title= Magic Quadrant for Data Warehouse Database Management Systems |author= Donald Feinberg, Mark A. Beyer |publisher= Gartner Group |date= January 28, 2011 |url= http://www.gartner.com/technology/media-products/reprints/teradata/vol3/article1/article1.html |accessdate= October 25, 2011 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Magic Quadrant for Data Warehouse Database Management Systems|url=http://www.gartner.com/technology/reprints.do?id=1-196T8S5&amp;ct=120207&amp;st=sb|date=February 6, 2012|publisher=Gartner Group|accessdate=February 29, 2012}}&lt;/ref&gt;

Teradata is the most popular data warehouse DBMS in the DB-Engines database ranking.&lt;ref&gt;{{cite web|title=DB-Engines Ranking of database management systems|url=http://db-engines.com/en/ranking|accessdate=April 11, 2013}}&lt;/ref&gt; 

In 2010, Teradata was listed in [[Fortune Magazine|Fortune’s]] annual list of Most Admired Companies.&lt;ref&gt;{{cite news |url=http://money.cnn.com/magazines/fortune/mostadmired/2010/snapshots/11465.html |title=WORLD'S MOST ADMIRED COMPANIES |work=Fortune |accessdate=October 27, 2010}}&lt;/ref&gt;

===Active enterprise data warehouse===
Teradata Active Enterprise Data Warehouse is the platform that runs the Teradata Database, with added data management tools and [[data mining]] software.

The data warehouse differentiates between “hot and cold” data – meaning that the warehouse puts data that is not often used in a slower storage section.&lt;ref&gt;{{cite web|last=Whitehorn|first=Mark|title=What your database needs is a good thermometer.|url=http://www.theregister.co.uk/2009/09/14/hot_and_cold_data/|date=September 14, 2009|work=The Register|accessdate=December 13, 2011}}&lt;/ref&gt; As of October 2010, Teradata uses Xeon 5600 processors for the server nodes.&lt;ref&gt;{{cite web|last=Morgan|first=Timothy Prickett|title=Teradata pumps data warehouses with six-core Xeons|url=http://www.theregister.co.uk/2010/10/25/teradata_appliance_refresh/|date=October 25, 2010|work=The Register|accessdate=December 13, 2011}}&lt;/ref&gt;

Teradata Database 13.10 was announced in 2010 as the company’s database software for storing and processing data.&lt;ref&gt;Dignan, Larry. “Teradata rolls out latest database, pushes time aware analysis.” ZDnet. October 25, 2010. http://www.zdnet.com/blog/btl/teradata-rolls-out-latest-database-pushes-time-aware-analysis/40865&lt;/ref&gt;&lt;ref&gt;Vizard, Mike. [http://www.ctoedge.com/content/teradata-extends-analytics-engine “Teradata Extends Analytics Engine.”] CTOEdge. October 25, 2010. &lt;/ref&gt;

Teradata Database 14 was sold as the upgrade to 13.10 in 2011 and runs multiple data warehouse workloads at the same time.&lt;ref&gt;Russom, Philip. [http://tdwi.org/blogs/philip-russom/2011/09/big-data-analytics-news-from-teradata.aspx “Big Data Analytics:  The News from Teradata.”] TDWI blog. September 22, 2011. &lt;/ref&gt; It includes column-store analyses.&lt;ref&gt;Henschen, Doug. [http://www.informationweek.com/news/software/bi/231601992?pgno=1 “Teradata Upgrades Break Down Database Barriers.”] ''InformationWeek''. September 22, 2011. &lt;/ref&gt;

Teradata Integrated Analytics is a set of tools for data analysis that resides inside the data warehouse.&lt;ref&gt;Winter, Richard. InformationWeek. September 18, 2010. [http://www.informationweek.com/news/software/info_management/227500132  “Research: State of Enterprise Databases.”]&lt;/ref&gt;

===Backup, archive, and restore===
BAR is Teradata’s backup and recovery system.&lt;ref&gt;Teradata brochure. [http://www.teradata.com/brochures/Teradata-Backup-Archive-Restore-eb5737/ “Teradata Backup Archive Restore.”]&lt;/ref&gt;

The Teradata Disaster Recovery Solution is automation and tools for data recovery and archiving.  Customer data can be stored in an offsite recovery center.&lt;ref&gt;Fratto, Mike. [http://www.networkcomputing.com/backup-recovery/229503239?pgno=1 “Teradata Disaster Recovery Solution Helps Reduce the Panic of Catastrophic Events.”] Network Computing. June 29, 2009.&lt;/ref&gt;

===Platform family ===
Teradata Platform Family is a set of products that include the Teradata Data Warehouse, Database, and a set of analytic tools. The platform family is marketed as smaller and less expensive than the other Teradata solutions.&lt;ref&gt;IT Reseller Magazine. [http://www.itrportal.com/absolutenm/templates/article-storage.aspx?articleid=4911&amp;zoneid=21 “Teradata announces new family of powerful analytic platforms.”]&lt;/ref&gt;

===Acquisitions===
{| class=&quot;wikitable&quot;
|-
! Acquisition date
! Company
! Valuation in millions USD
! Purpose
! References
|-
| May 2, 2012
| [[eCircle]]
|
| Marketing Tools
|-
|-
| March 3, 2011
| [[Aster Data Systems]]
| 263
| MapReduce, Big Data
|&lt;ref&gt;{{cite web|last=Morgan|first=Timothy Prickett|title=Teradata Snaps Up Aster Data for $263m|url=http://www.theregister.co.uk/2011/03/03/teradata_buys_aster_data/|date=March 3, 2011|publisher=Channel Register|accessdate=December 13, 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Vizard|first=Mike|title=Diversity Comes to Data Management|url=http://www.itbusinessedge.com/cm/blogs/vizard/diversity-comes-to-data-management/?cs=48670|date=September 22, 2011|publisher=IT Business Edge|accessdate=December 13, 2011}}&lt;/ref&gt;
|-
| December 22, 2010
| [[Aprimo]]
| 550
| Marketing tools
|&lt;ref&gt;{{cite web|last=Morgan|first=Timothy Prickett|title=Teradata Eats Aprimo for $550m|url=http://www.channelregister.co.uk/2010/12/22/teradata_buys_aprimo/ |date= December 22, 2010 |publisher= Channel Register |accessdate= December 13, 2011 }}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Henschen|first=Doug|title=IT and Marketing: How Digital Media’s Changing the Relationship |url= http://www.informationweek.com/news/software/bi/229400641?pgno=1 |date= April 9, 2011|work=InformationWeek|accessdate=December 13, 2011}}&lt;/ref&gt;
|-
| August 10, 2010
| [[Kickfire]]
|
|
|&lt;ref&gt;{{cite web|last=Kanaracus|first=Chris|title=Teradata buys analytics vendor Kickfire|url=http://www.infoworld.com/d/the-industry-standard/teradata-buys-analytics-vendor-kickfire-496|date=August 10, 2010|publisher=Info World|accessdate=December 13, 2011}}&lt;/ref&gt;
|-
| March 20, 2008
| Claraview
|
|
|&lt;ref&gt;{{cite web|last=Grimes|first=Seth|title=Teradata Has Acquired BI/DW Firm Claraview|url=http://www.informationweek.com/blog/228900615|date=March 20, 2008|work=InformationWeek|accessdate=December 13, 2011}}&lt;/ref&gt;
|-
| November 30, 2005 under NCR
| DecisionPoint
|
| Sourcing and integration of data
|&lt;ref&gt;{{cite news |title=Teradata Acquires DecisionPoint(R) Software; Acquisition Extends Teradata's Financial Management Portfolio |work= Press release |url= http://investor.ncr.com/phoenix.zhtml?c=83840&amp;p=irol-newsArticle&amp;ID=791311 |date= November 30, 2005 |accessdate= December 13, 2011}}&lt;/ref&gt;
|-
| July 14, 2000 under NCR
| Stirling Douglas Group
|
| Demand chain management
|&lt;ref name=&quot;stirling&quot; /&gt;
|}

==Partners==
Below is a partial list of Teradata partners.

* [[Capgemini]]
*[[Cloudera]]&lt;ref&gt;{{cite web|title=Browse Our Partners|url=http://www.teradata.com/templates/Partners/BrowsePartners.aspx|publisher=Teradata |accessdate=April 26, 2013}}&lt;/ref&gt;
*[[IBM]]
*[[Informatica]]: Dual Load solution&lt;ref&gt;{{cite web|last=Howard|first=Philip|title=Dual Loading for Teradata|url=http://www.businesscomputingworld.co.uk/dual-loading-for-teradata/|date=September 8, 2010|publisher=BCW|accessdate=December 13, 2011}}&lt;/ref&gt;
*[[Intel]]
*[[Kalido]]: Teradata and Kalido Accelerate product&lt;ref&gt;{{cite press release |title=Kalido and Teradata Team to Deliver an Agile Data Warehouse for the Mid-Market|url=http://www.kalido.com/7d0a8eed-dab8-4c38-a6b7-e657276c9b91/news-and-events-press-center-press-releases-detail.htm|publisher=Kalido |date=October 25, 2010|accessdate=December 13, 2011}}&lt;/ref&gt;
*[[MapR]]
*[[Microsoft]]
*[[MicroStrategy]]:  joint business intelligence products&lt;ref&gt;{{cite web|title=Solutions: MicroStrategy and Teradata|url=http://www.microstrategy.com/bi-applications/bydatasource/teradata/index.asp|work=MicroStrategy website|accessdate=December 13, 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite press release|title=MicroStrategy Announces Teradata Support for Business Intelligence Cloud|date=January 24, 2012|url=http://www.microstrategy.com/about-us/press/release/?ctry=167&amp;id=2402|publisher=MicroStrategy}}&lt;/ref&gt;
*[[NetApp]]
*[[Oracle Corporation|Oracle]]
*[[SAP AG|SAP]]: [[Netweaver]] datawarehouse&lt;ref&gt;{{cite web|last=Grant|first=Ian|title=Users welcome integration of Netweaver and Teradata|url=http://www.computerweekly.com/Articles/2009/04/27/235806/Users-welcome-integration-of-Netweaver-and-Teradata.htm|date=April 27, 2009|work=Computer Weekly|accessdate=December 13, 2011}}&lt;/ref&gt;
*[[SAS (software)|SAS]]
*[[Symantec]]&lt;ref name=&quot;:0&quot;&gt;{{cite web|title=Browse Our Partners|url=http://www.teradata.com/templates/Partners/BrowsePartners.aspx|publisher=Teradata |accessdate=December 13, 2011}}&lt;/ref&gt;
* [[Tableau Software]]: interactive data visualization products focused on business intelligence&lt;ref name=&quot;:0&quot; /&gt;
* [[Attunity]] &lt;ref&gt;{{cite web|title=Attunity|url=http://www.attunity.com}}&lt;/ref&gt; 
* [[WhereScape]]: data warehouse development and management tools

===Teradata Partners Conference===
Teradata holds an annual user group conference and [[Trade fair|expo]] known as ''Teradata PARTNERS'' with keynote industry speakers, educational sessions led by customers and other vendors.&lt;ref&gt;{{cite web|url=http://www.teradata-partners.com/|title=Teradata PARTNERS User Group}}&lt;/ref&gt; The Teradata Partners Conference has been an annual event since 1985.&lt;ref&gt;{{cite web|title=Learning how small fish can eat big fish|url=http://english.peopledaily.com.cn/90001/90778/90858/90864/7199036.html|date=November 15, 2010|work=China Daily|accessdate=December 13, 2011}}&lt;/ref&gt; The conference involves lectures and speeches on technical and business topics and announcements about new products.&lt;ref&gt;{{cite web|last=Wise|first=Lyndsay|title=Wrapping Up at Teradata Partners User Group Conference|url=http://www.dashboardinsight.com/news/news-articles/wrapping-up-at-teradata-partners-user-group-conference.aspx|date=October 22, 2009|publisher=Dashboard Insight|accessdate=December 13, 2011}}&lt;/ref&gt;

==Teradata and Big Data==
Teradata began to associate itself with the term, “[[Big Data]]” in 2010.  CTO, Stephen Brobst, attributes the rise of big data to “new media sources, such as [[social media]].”&lt;ref&gt;{{cite web|last=Salah-Ahmed|first=Amira|title=One-on-One with Teradata’s CTO Stephen Brobst|url=http://thedailynewsegypt.com/it-a-telecom/one-on-one-with-teradatas-cio-stephen-brobst.html|date=May 4, 2011|work=The Daily Egypt News|accessdate=December 13, 2011}}&lt;/ref&gt; The increase in semi-structured and unstructured data gathered from online interactions prompted Teradata to form the “Petabyte club” in 2011 for its heaviest big data users.&lt;ref name=&quot;Grant&quot;&gt;{{cite web|last=Grant|first=Ian|title=Big data boosts Teradata growth|url=http://www.computerweekly.com/Articles/2011/04/11/246287/Big-data-boosts-Teradata-growth.htm|date=April 11, 2011|work=Computer Weekly|accessdate=December 13, 2011}}&lt;/ref&gt;

The rise of big data resulted in many traditional data warehousing companies updating their products and technology.&lt;ref&gt;{{cite web|last=Miniman|first=Stuart|title=The Emerging Big Data Vendor Ecosystem|url=http://wikibon.org/wiki/v/The_Emerging_Big_Data_Vendor_Ecosystem|date=March 28, 2011|publisher=Wikibon|accessdate=December 13, 2011}}&lt;/ref&gt;  For Teradata, big data prompted the acquisition of [[Aster Data Systems]] in 2011 for the company’s [[MapReduce]] capabilities and ability to store and analyze semi-structured data.&lt;ref&gt;{{cite web|last=Kanaracus|first=Chris|title=Teradata Buys Aster Data, Boosts &quot;Big Data&quot; Wares|url=http://www.cio.com/article/672663/Teradata_Buys_Aster_Data_Boosts_big_Data_Wares|date=March 3, 2011|publisher=CIO|accessdate=December 13, 2011}}&lt;/ref&gt;

Public interest in big data resulted in a 13% increase in Teradata’s global sales.&lt;ref name=&quot;Grant&quot;/&gt;

==Competition==
Teradata's main competitors are similar products from vendors such as [[Oracle database|Oracle]], [[IBM DB2|IBM]], [[Microsoft SQL Server|Microsoft]] and [[Sybase IQ]].  Also, competitors include data warehouse appliance vendors such as [[Netezza]]&lt;ref&gt;[http://www.dbms2.com/2008/09/15/teradata-data-warehouse-appliance/ Teradata decides to compete head-on as a data warehouse appliance vendor]&lt;/ref&gt; (acquired in November 2010 by [[IBM]]), [[DATAllegro]] (acquired in August 2008 by [[Microsoft]]), [[ParAccel]], [[Greenplum]] (acquired in July 2010 by [[EMC Corporation|EMC]]), and [[Vertica|Vertica Systems]] (acquired in February 2011 by [[HP]]), and from packaged data warehouse applications such as [[SAP BW|SAP]] and [[Kalido]].

==Philanthropy==
In 2012, over 4500 employees completed over 15,000 hours of volunteerism. Since the launch of Teradata Cares in May 2008 more than over 46,000 hours of volunteerism have been completed.&lt;ref&gt;http://www.teradata.com/corporate-social-responsibility/?ICID=Acsr &lt;/ref&gt; Teradata Cares supports data philanthropy, youth science and technology education programs and established charities such as  Junior Achievement and United Way.&lt;ref&gt;&quot;Teradata Cares&quot;. Teradata. Retrieved December 13, 2011.&lt;/ref&gt; 

==References==
{{reflist|colwidth=30em}}

==External links==
* [http://www.teradata.com/ Company Home page]
* [http://www.teradata.com/Darryl Teradata CMO [[Darryl McDonald]]'s Blog]
* [http://www.teradata-partners.com/ Teradata PARTNERS Conference] – Teradata user group conference and expo

{{NCR Corp}}
{{AT&amp;T Spinoffs}}

[[Category:Companies listed on the New York Stock Exchange]]
[[Category:Software companies based in Ohio]]
[[Category:Data warehousing products]]
[[Category:Teradata]]
[[Category:Companies based in Dayton, Ohio]]
[[Category:NCR Corporation]]
[[Category:Big data]]</text>
      <sha1>att7wuvt8a6sdtp570eqlw6qzwbq759</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Business analytics</title>
    <ns>0</ns>
    <id>10147369</id>
    <revision>
      <id>601683024</id>
      <parentid>593443349</parentid>
      <timestamp>2014-03-28T17:40:49Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>/* Further reading */Task 1c: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated date parameter errors]]</comment>
      <text xml:space="preserve" bytes="10777">{{Distinguish|Business analysis}}
{{Refimprove|date=October 2010}}
'''Business analytics''' ('''BA''') refers to the skills, technologies, applications and practices for continuous iterative exploration and investigation of past business performance to gain insight and drive business planning.&lt;ref&gt;{{cite web 
 |title= Next Generation Business Analytics
 |url= http://www.docstoc.com/docs/7486045/Next-Generation-Business-Analytics-Presentation 
 |last= Beller
 |first= Michael J.
 |coauthors= Alan Barnett
 |date= 2009-06-18 
 |publisher= Lightship Partners LLC 
 |accessdate=2009-06-20
}}&lt;/ref&gt;  Business analytics focuses on developing new insights and understanding of business performance based on [[data]] and [[Statistics|statistical methods]]. In contrast, [[business intelligence]] traditionally focuses on using a consistent set of metrics to both measure past performance and guide business planning, which is also based on data and statistical methods.

Business [[analytics]] makes extensive use of data, [[Statistics|statistical]] and [[Quantitative research|quantitative]] analysis, explanatory and [[predictive modeling]],&lt;ref&gt;{{cite web| url=http://www.citi.uconn.edu/cist07/5c.pdf |title = Predictive vs. Explanatory Modeling in IS Research| author = Galit Schmueli and Otto Koppius }}&lt;/ref&gt; and fact-based management to drive [[decision making]]. It is therefore closely related to [[management science]]. Analytics may be used as input for human decisions or may drive fully automated decisions. Business intelligence is [[Information retrieval|querying]], [[report]]ing, [[OLAP]], and &quot;alerts.&quot;

In other words, querying, reporting, OLAP, and alert tools can answer questions such as what happened, how many, how often, where the problem is, and what actions are needed. Business analytics can answer questions like why is this happening, what if these trends continue, what will happen next (that is, predict), what is the best that can happen (that is, optimize).&lt;ref name=Davenport2007&gt;{{Cite book
 | last1 = Davenport | first1 = Thomas H.
 | last2 = Harris | first2 = Jeanne G.
 | year = 2007
 | title = Competing on analytics : the new science of winning
 | isbn = 978-1-4221-0332-6
 | publisher = Harvard Business School Press
 | location = Boston, Mass.
}}&lt;/ref&gt;

==Examples of application==
Banks, such as [[Capital One]], use [[data analysis]] (or ''[[analytics]]'', as it is also called in the business setting),  to differentiate among customers based on [[credit risk]], usage and other characteristics and then to match customer characteristics with appropriate product offerings. [[Harrah's Entertainment|Harrah’s]], the gaming firm, uses analytics in its [[customer loyalty]] programs. [[E &amp; J Gallo Winery]] quantitatively analyzes and predicts the appeal of its wines. Between 2002 and 2005, [[Deere &amp; Company]] saved more than $1 billion by employing a new analytical tool to better optimize inventory.&lt;ref name=&quot;Davenport2007&quot;/&gt;

==Types of analytics==
&lt;!-- Need to expand descriptions in this section --&gt;
* Descriptive Analytics: Gain insight from historical data with [[reporting (disambiguation)|reporting]], scorecards, [[Cluster analysis|clustering]] etc.
* [[Predictive analytics]] (predictive modeling using statistical and machine learning techniques)
* [[Prescriptive analytics]] recommend decisions using optimization, simulation etc.
* Decisive analytics: supports human decisions with visual analytics the user models to  reflect reasoning.

==Basic domains within analytics==
&lt;!-- Need to expand descriptions in this section --&gt;
* [[Retail sales]] analytics
* [[Financial service]]s analytics
* [[credit risk|Risk &amp; Credit]] analytics
* Talent analytics
* [[Marketing]] analytics
* [[Behavioral analytics]]
* [[Cohort Analysis]]
* Collections analytics
* [[Fraud]] analytics
* [[Pricing]] analytics 
* [[Telecommunications]]
* [[Supply Chain]] analytics
* [[Transportation]] analytics
* Contextual data modeling - supports the human reasoning that occurs after viewing &quot;executive dashboards&quot; or any other visual analytics

==History==
Analytics have been used  in business since the time management exercises that were initiated by [[Frederick Winslow Taylor]] in the late 19th century. [[Henry Ford]] measured pacing of assembly line. But analytics began to command more attention in the late 1960s when computers were used in [[decision support systems]]. Since then, analytics have evolved with the development of [[enterprise resource planning]] (ERP) systems, [[data warehouses]], and a wide variety of other hardware and software tools and applications.&lt;ref name=&quot;Davenport2007&quot;/&gt;

With the recent explosion of [[big data]] and intuitive BI tools, data is more accessible to business professionals and managers than ever before. Thus there is a big opportunity to make better decisions using that data to drive incremental revenue, decrease cost and loss by building better products, improving customer experience, catching fraud before it happens, improving customer engagement through targeting and customization- all with the power of data. More and more companies are now equipping their employees with the know-how of Business Analytics to drive efficiency in day-to-day decision making.&lt;ref name=&quot;forbes&quot;/&gt;

==Challenges==
Business analytics depends on sufficient volumes of high quality data. The difficulty in ensuring data quality is integrating and reconciling data across different systems, and then deciding what subsets of data to make available.&lt;ref name=&quot;Davenport2007&quot;/&gt;

Previously, analytics was considered a type of after-the-fact method of [[forecasting]] [[consumer behavior]] by examining the number of units sold in the last quarter or the last year. This type of data warehousing required a lot more storage space than it did speed. Now business analytics is becoming a tool that can influence the outcome of customer interactions.&lt;ref&gt;{{cite web|url= http://content.dell.com/us/en/enterprise/d/large-business/best-storage-business-analytic.aspx |title= Choosing the Best Storage for Business Analytics|publisher=Dell.com | accessdate=2012-06-25}}&lt;/ref&gt; When a specific customer type is considering a purchase, an analytics-enabled enterprise can modify the sales pitch to appeal to that consumer. This means the storage space for all that data must react extremely fast to provide the necessary data in real-time.

==Competing on analytics==
Davenport argues that businesses can optimize a distinct business capability via analytics and thus better compete. He identifies these characteristics of an organization that are apt to compete on analytics:&lt;ref name=&quot;Davenport2007&quot;/&gt;
* One or more senior executives who strongly advocate fact-based decision making and, specifically, analytics
* Widespread use of not only [[descriptive statistics]], but also predictive modeling and complex [[optimization (mathematics)|optimization]] techniques
* Substantial use of analytics across multiple business functions or processes
* Movement toward an enterprise level approach to managing analytical tools, data, and organizational skills and capabilities

== See also ==
*[[Data mining]]
*[[Analytics]]
*[[Business Intelligence]]
*[[Test and Learn]] 
*[[Business Process Discovery]]
*[[OLAP]]
*[[Statistics]]
*[[Customer dynamics]]
* cf. [[business analysis]]

== References ==
{{reflist|refs=
&lt;ref name=forbes&gt;{{cite web|last=Jain|first=Piyanka|title=Analytics is Fast Becoming a Core Competency for Business Professionals|url=http://www.forbes.com/sites/piyankajain/2013/03/27/analytics-is-a-core-competency-for-business-professionals/|work=Forbes|publisher=Forbes|accessdate=10 May 2013}}&lt;/ref&gt;
}}

== Further reading ==

* {{cite book |last=Bartlett|first=Randy |title=A Practitioner’s Guide To Business Analytics: Using Data Analysis Tools to Improve Your Organization’s Decision Making and Strategy |date=February 2013|publisher=McGraw-Hill |location= |isbn=978-0071807593}}
* {{cite book |last=Davenport |first=Thomas H. |authorlink=Thomas H. Davenport |coauthors=Jeanne G. Harris |title=Competing on Analytics: The New Science of Winning |date=March 2007 |publisher=Harvard Business School Press |location= |isbn= }}
* {{cite book |last=McDonald |first=Mark |coauthors=Tina Nunno |title=Creating Enterprise Leverage: The 2007 CIO Agenda |date=February 2007  |publisher=Gartner, Inc. |location=Stamford, CT |isbn= }}
* {{cite book |last=Stubbs |first=Evan |title=The Value of Business Analytics |date=July 2011 |publisher=John Wiley &amp; Sons |location= |isbn=}}
* {{cite book |last=Ranadive |first=Vivek |title=The Power to Predict: How Real Time Businesses Anticipate Customer Needs, Create Opportunities, and Beat the Competition |date=2006-01-26 |publisher=McGraw-Hill |location= |isbn= }}
* {{cite book |last=Zabin |first=Jeffrey |coauthors=Gresh Brebach |title=Precision Marketing |date=February 2004 |publisher=John Wiley |location= |isbn= }}
* {{cite journal |last=Baker |first=Stephen |date=January 23, 2006 |title=Math Will Rock Your World |journal=BusinessWeek |volume= |issue= |pages= |id= |url=http://www.businessweek.com/print/magazine/content/06_04/b3968001.htm?chan=gl |accessdate=2007-09-19 |quote= }}
* {{cite journal |last=Davenport |first=Thomas H. |date=January 1, 2006 |title=Competing on Analytics |journal=Harvard Business Review |volume= |issue= |pages= |id= |url= |accessdate= |quote= }}
* {{cite journal |last=Pfeffer |first=Jeffrey |authorlink=Jeffrey Pfeffer |coauthors=[[Robert I. Sutton]]|date=January 2006 |title=Evidence-Based Management |journal=Harvard Business Review |volume= |issue= |pages= |id= |url= |accessdate= |quote= }}
* {{cite journal |last=Davenport |first=Thomas H. |coauthors=Jeanne G. Harris |date=Summer 2005 |title=Automated Decision Making Comes of Age |journal=MIT Sloan Management Review |volume= |issue= |pages= |id= |url= |accessdate= |quote= }}
* {{cite book |last=Lewis |first=Michael |title=Moneyball: The Art of Winning an Unfair Game |date=April 2004 |publisher=W.W. Norton &amp; Co. |location= |isbn= }}
* {{cite journal |last=Bonabeau |first=Eric |date=May 2003 |title=Don’t Trust Your Gut |journal=Harvard Business Review |volume= |issue= |pages= |id= |url= |accessdate= |quote= }}
* {{cite journal |last=Davenport |first=Thomas H. |coauthors=Jeanne G. Harris, David W. De Long, Alvin L. Jacobson |title=Data to Knowledge to Results: Building an Analytic Capability |journal=California Management Review |volume=43 |issue=2 |pages=117–138 |id= |url= |accessdate= |quote= }}

{{DEFAULTSORT:Business Analytics}}
[[Category:Business terms]]
[[Category:Data warehousing]]
[[Category:Applied data mining]]
[[Category:Business intelligence]]
[[Category:Management science]]
[[Category:Big data|analytics]]

[[de:Business Analytics]]</text>
      <sha1>22wo7cpwxc6t1trn0d7kifhfwh3o4e1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Manuel Aparicio</title>
    <ns>0</ns>
    <id>35815630</id>
    <revision>
      <id>576937393</id>
      <parentid>572177935</parentid>
      <timestamp>2013-10-13T02:49:53Z</timestamp>
      <contributor>
        <username>Afasmit</username>
        <id>560336</id>
      </contributor>
      <minor/>
      <comment>defaultsort</comment>
      <text xml:space="preserve" bytes="1285">Dr. '''Manuel Aparicio''' is a recognized expert in the field of Associative Memories and [[big data analytics]], holding several patents for [[associative memory base]] technology {{US patent|7,908,438}}

Dr Aparicio co-founded [[Saffron Technology, Inc.]] a company specializing in [[Associative Memory Base]] Technology and he serves as its Chief Executive Officer.&lt;ref&gt;http://investing.businessweek.com/research/stocks/private/person.asp?personId=627899&amp;privcapId=99166&amp;previousCapId=99166&amp;previousTitle=Saffron%20Technology,%20Inc.&lt;/ref&gt; 

Dr. Aparicio served as Chief Scientist of IBM Knowledge Management and Intelligent Agent Center. 

He serves as Member of the Advisory Board at Sociocast Networks LLC.

==Footnotes==
{{Reflist}}

==External links==
* [http://www.saffrontech.com/ Saffron Technology]
* [http://www.sociocast.com/ Sociocast Networks LLC]

{{Persondata
| NAME              = Aparicio, Manuel
| ALTERNATIVE NAMES =
| SHORT DESCRIPTION = American businessman
| DATE OF BIRTH     =
| PLACE OF BIRTH    =
| DATE OF DEATH     =
| PLACE OF DEATH    =
}}
{{DEFAULTSORT:Aparicio, Manuel}}
[[Category:Living people]]
[[Category:American chief executives]]
[[Category:American computer scientists]]
[[Category:Big data]]
[[Category:American technology company founders]]</text>
      <sha1>o0tv30xyezrwtsgqhagem5ov6ikvlpt</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Flutura Decision Sciences and Analytics</title>
    <ns>0</ns>
    <id>40765199</id>
    <revision>
      <id>597917446</id>
      <parentid>582803340</parentid>
      <timestamp>2014-03-03T05:35:07Z</timestamp>
      <contributor>
        <username>Shyamsunder</username>
        <id>800815</id>
      </contributor>
      <comment>−[[Category:Companies by city in India]]; −[[Category:Companies based in Karnataka]] using [[WP:HC|HotCat]]</comment>
      <text xml:space="preserve" bytes="3112">{{Orphan|date=October 2013}}

{{Infobox company
| name             = Flutura Decision Sciences and Analytics    
| logo             =[[Image:Flutura-Logo.png|150px]]
| type             =[[Private company|Private Ltd.]]  
| location_city    ={{Unbulleted list|[[Bangalore]], [[Karnataka]], [[India]]|[[San Jose, California|San Jose]], [[California]], [[USA]]}}
 
| founder          ={{Unbulleted list|Krishnan Raman|Derick Jose|Srikanth Muralidhara}}   
| foundation       =February 2012  
| area_served      =Worldwide
| industry         =[[Machine to Machine]](M2M), [[Internet of Things]], [[Big Data]] [[Analytics]], [[Decision science]]s
| products = {{Unbulleted list|Cerebra}}
| homepage         ={{URL|flutura.com/}}   
| intl             =yes
}}

'''Flutura Decision Sciences and Analytics''' is a Decision Sciences Company that focuses on M2M (machine to machine) and Big Data Analytics. Its main offices are located in Bengaluru, India, and San Jose, USA. California based technology magazine ''CIO Review'' has recognized Flutura as one of the Top 20 Most Promising Big Data Companies Globally. Flutura has also been recognized by TechSparks2013 as one of the Top 3 startups out of India.

== History ==
Flutura was co-founded by Krishnan Raman, Srikanth Muralidhara and Derick Jose, in February 2012. Flutura is a butterfly in Albanian. It is symbol of aspiration to transform decisioning processes within organisations and enable disruptive outcomes. The flutura team has been strategic advisors to several Fortune 500 organizations worldwide.

== Recognition ==
* In October 2013, CIOReview Magazine rated Flutura as one of top 20 promising Big data companies of the world&lt;ref&gt;http://www.cioreview.com/magazine/Flutura-Solutions-Help-See-Previously-Unseen-Patterns-OUJL830948901.html &quot;CIOReview&quot;, October 2013&lt;/ref&gt;
* In September 2013,  Flutura was spotted in CIOReview magazine`s top 10 Big data start-ups of India list&lt;ref&gt;http://www.cioreview.com/crtech-india/10-most-promising-big-data-companies.html &quot;CIOReview&quot;, September 2013&lt;/ref&gt;
* In 2013, TechSparks ranked Flutura as one of the Top 3 Startups out of India&lt;ref&gt;http://yourstory.in/2013/10/top-30-quotes-for-startups-and-innovators-from-techsparks-2013/ &quot;Techspark 2013&quot;&lt;/ref&gt; &lt;ref&gt;https://twitter.com/techsparks2013/status/386491414143848448&lt;/ref&gt;

== Products ==
To derive meaning out of vast amounts of data generated by machines, Flutura developed a patent pending platform called '''Cerebra'''. Cerebra does not only harvest signals from machines in real time, it gives the product users absolute visibility of their operations and empowers them with the ability to choose how they want to act in different scenarios. The unique ability of Cerebra to capture important signals from complex machine generated logs and harness previously untapped data makes it the future generation intelligent Big Data platform.

== References ==
{{reflist}}

== External links ==
*{{official website|http://www.flutura.com}}

[[Category:Analytics]]
[[Category:Big data]]
[[Category:Machine to Machine]]
[[Category:Organisations based in Bangalore]]</text>
      <sha1>0b2f8brgxaja4s39wif7mx5pkudu6z2</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Category:Machine to Machine</title>
    <ns>14</ns>
    <id>40797529</id>
    <revision>
      <id>577241302</id>
      <timestamp>2013-10-15T06:48:54Z</timestamp>
      <contributor>
        <username>Swapnil.Sinha</username>
        <id>19907786</id>
      </contributor>
      <comment>[[WP:AES|←]]Created page with '[[Category:Big data]]'</comment>
      <text xml:space="preserve" bytes="21">[[Category:Big data]]</text>
      <sha1>sznkzbu23pa4lfpicy3fim9mfpqdhpu</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Rocket Fuel Inc.</title>
    <ns>0</ns>
    <id>41309184</id>
    <revision>
      <id>598909619</id>
      <parentid>595603710</parentid>
      <timestamp>2014-03-09T23:58:44Z</timestamp>
      <contributor>
        <ip>97.79.129.70</ip>
      </contributor>
      <comment>fr</comment>
      <text xml:space="preserve" bytes="9051">{{Infobox company
| name             = Rocket Fuel Inc.
| logo             = [[File:Rocket Fuel Inc. logo.png|220px]]
| traded_as        = {{NASDAQ link|FUEL}}
| industry         = Marketing and Advertising
| foundation       = {{start date|2008}}
| founders          = George John, Richard Frankel, Abhinav Gupta
| location_city    = [[Redwood_City,_California|Redwood City, CA]]
| location_country = US
| locations        = 20
| area_served      = North America, Europe, and Japan
| key_people       = George John, CEO&lt;br&gt; Richard Frankel, President&lt;br&gt; Abhinav Gupta, Senior Vice President of Engineering. 
| num_employees    = 640
| homepage         = {{URL|http://rocketfuel.com/}}
}}

'''Rocket Fuel''' ({{NASDAQ|FUEL}}) is a provider of a programmatic media-buying platform that utilizes [[artificial intelligence]] (AI), [[big data]], and predictive modeling to autonomously real-time bid (RTB) on digital media ad impressions across web, video, mobile, and social media.&lt;ref name=&quot;Seeking Alpha&quot;&gt;{{cite web|url=http://seekingalpha.com/article/1756932-rocket-fuels-ipo-soars|title=Rocket Fuel's IPO Soars|publisher=Seeking Alpha|author=Sramana Mitra|date=21 October 2013|accessdate=3 December 2013}}&lt;/ref&gt;&lt;ref name=&quot;All Things D&quot;&gt;{{cite web|url=http://allthingsd.com/20130920/rocket-fuels-ipo-may-boost-ad-tech/|title=Rocket Fuel’s IPO May Boost Ad Tech|publisher=All Things D|author=Peter Kafka|date=20 September 2013|accessdate=3 December 2013}}&lt;/ref&gt;&lt;ref name=DealBook&gt;{{cite web|url=http://dealbook.nytimes.com/2013/09/20/rocket-fuel-and-fireeye-more-than-double-in-market-debut/|title=Rocket Fuel and FireEye More Than Double in Market Debut|publisher=New York Times DealBook|author=Michael J. De La Merced|author2=William Alden|date=20 September 2013|accessdate=3 December 2013}}&lt;/ref&gt;&lt;ref name=&quot;New York Times&quot;&gt;{{cite web|url=http://www.nytimes.com/2012/09/09/technology/data-driven-discovery-is-techs-new-wave-unboxed.html?_r=2&amp;&amp;gwh=2D2836E35351BF56C037BBB65A653C1E|title=Tech’s New Wave, Driven by Data|publisher=New York Times DealBook|author=Steve Lohr|date=8 September 2012|accessdate=3 December 2013}}&lt;/ref&gt;&lt;ref name=Forbes3&gt;{{cite web|url=http://www.forbes.com/sites/davidleinweber/2012/06/28/bye-bye-wall-street-new-flavor-of-big-data-may-be-more-lucrative-for-quants/|title=Bye-Bye, Wall Street: New Flavor Of Big Data May Be More Lucrative For Quants|publisher=Forbes|author=David Leinweber|date=28 June 2012|accessdate=3 December 2013}}&lt;/ref&gt; Rocket Fuel's platform evaluates millions of data points (such as shopping behavior, loyalty data, weather, and demographics) to score and predict the likelihood that consumers surfing the web, tablet, or mobile devices will respond to advertising.&lt;ref name=&quot;All Things D2&quot;&gt;{{cite web|url=http://allthingsd.com/20121214/beyond-marketing-clouds-the-age-of-machine-learning/|title=Beyond Marketing Clouds — The Age of Machine Learning|publisher=All Things D|author=Raj De Datta|date=14 December 2012|accessdate=3 December 2013}}&lt;/ref&gt;&lt;ref name=&quot;San Francisco Business Times&quot;&gt;{{cite web|url=http://www.bizjournals.com/sanfrancisco/print-edition/2012/05/11/rocket-fuel-predicts-shoppers-habits.html?page=all|title=Rocket Fuel predicts shoppers’ habits in milliseconds|publisher=San Francisco Business Times|author=Krystal Peak|date=11 May 2012|accessdate=3 December 2013}}&lt;/ref&gt;&lt;ref name=&quot;Wall Street Journal&quot;&gt;{{cite web|url=http://blogs.wsj.com/venturecapital/2012/06/29/the-daily-start-up-rocket-fuel-blasts-off-with-50m-for-ad-tech/|title=The Daily Start-Up: Rocket Fuel Blasts Off With $50M for Ad Tech|publisher=Wall Street Journal|author=|date=29 June 2012|accessdate=3 December 2013}}&lt;/ref&gt;&lt;ref name=&quot;Luxury Deluxe&quot;&gt;{{cite web|url=http://www.luxurydaily.com/personalized-mobile-banner-ads-does-less-mean-more/|title=Personalized mobile banner ads: Does less mean more?|publisher=Luxury Deluxe|author=Rachel Lamb|date=5 October 2012|accessdate=3 December 2013}}&lt;/ref&gt;&lt;ref name=&quot;Automotive News&quot;&gt;{{cite web|url=http://www.autonews.com/apps/pbcs.dll/article?AID=/20121223/RETAIL03/121229955/data-companies-help-automakers-reduce-digital-guessing-games#axzz2joJrHQUg|title=Data companies help automakers reduce digital guessing games|publisher=Automotive News|author=Vince Bond Jr.|date=23 December 2012|accessdate=3 December 2013}}&lt;/ref&gt; It then bids on the impressions to reach those consumers.&lt;ref name=&quot;All Things D2&quot;/&gt;&lt;ref name=&quot;San Francisco Business Times&quot;/&gt;&lt;ref name=&quot;Wall Street Journal&quot;/&gt;&lt;ref name=&quot;Luxury Deluxe&quot;/&gt;&lt;ref name=&quot;Automotive News&quot;/&gt; To tailor results, the software learns and improves with each transaction by using machine-learning algorithms.&lt;ref name=&quot;San Francisco Business Times&quot;/&gt;&lt;ref name=&quot;Wall Street Journal&quot;/&gt;&lt;ref name=Forbes2&gt;{{cite web|url=http://www.forbes.com/sites/davidleinweber/2012/09/28/getting-up-close-with-big-data-part-one/|title=Getting Up Close with Big Data: Part One|publisher=Forbes|author=David Leinweber|date=28 September 2012|accessdate=3 December 2013}}&lt;/ref&gt;

The company has three founders: George John, Richard Frankel and Abhinav Gupta. John serves as Rocket Fuel’s CEO and Frankel is the company’s president.&lt;ref name=DealBook/&gt;&lt;ref name=Bloomberg&gt;{{cite web|url=http://www.bloomberg.com/video/rocket-fuel-using-ai-to-make-digital-ads-better-pFHwbuOBT6iKh5IcHF27TA.html|title=Rocket Fuel: Using AI to Make Digital Ads Better|publisher=Bloomberg TV|author=|date=|accessdate=3 December 2013}}&lt;/ref&gt;&lt;ref name=AdAge&gt;{{cite web|url=http://adage.com/article/digital/rocket-fuel-marks-successful-ad-tech-ipo-2013/244282/|title=Rocket Fuel IPO Soars on First Day of Trading|publisher=AdAge|author=Tim Peterson|author2=Alex Kantrowitz|date=20 September 2013|accessdate=3 December 2013}}&lt;/ref&gt; Gupta serves as Rocket Fuel’s senior vice president of engineering.&lt;ref name=Forbes4&gt;{{cite web|url=http://www.forbes.com/sites/sanjeevsardana/2013/11/20/bigdata/|title=Big Data: It's Not A Buzzword, It's A Movement|publisher=Forbes|author=Sanjeev Sardana|date=20 November 2013|accessdate=3 December 2013}}&lt;/ref&gt;
 
==History==

Based in [[Redwood City]], California, Rocket Fuel was founded in 2008 by George John, Richard Frankel, and Abhinav Gupta, all Yahoo! alumni.&lt;ref name=&quot;Seeking Alpha&quot;/&gt;&lt;ref name=Bloomberg/&gt;&lt;ref name=Forbes&gt;{{cite web|url=http://www.forbes.com/companies/rocket-fuel/|title=America's Most Promising Companies|publisher=Forbes|author=|date=|accessdate=3 December 2013}}&lt;/ref&gt;&lt;ref name=Reuters&gt;{{cite web|url=http://www.reuters.com/article/2013/09/21/us-rocket-fuel-ipo-idUSBRE98J0JR20130921|title=Ad tech provider Rocket Fuel shares nearly double in debut|publisher=Reuters|author=Avik Das|date=21 September 2013|accessdate=3 December 2013}}&lt;/ref&gt;

In October 2012, Rocket Fuel expanded into Japan through an alliance with cyber communications inc. (cci).&lt;ref name=ClickZ&gt;{{cite web|url=http://www.clickz.com/clickz/column/2282017/japan-leads-asia-in-rtb-spending-and-other-datadriven-marketing-updates|title=Japan Leads Asia in RTB Spending and other Data-Driven Marketing Updates|publisher=ClickZ|author=Adaline Lau|date=6 November 2012|accessdate=3 December 2013}}&lt;/ref&gt; cci is Tokyo-based Dentsu Inc.’s digital subsidiary.&lt;ref name=ClickZ/&gt;

Rocket Fuel successfully completed its initial public offering in September 2013.&lt;ref name=DealBook/&gt;&lt;ref name=AdAge/&gt;&lt;ref name=Reuters/&gt;&lt;ref&gt;{{cite web|url=http://www.adotas.com/2014/02/rocket-fuel-exec-offers-reasons-for-optimism-about-the-future-of-mobile-advertising/|title=Rocket Fuel Exec Offers Reasons For Optimism About The Future Of Mobile Advertising|work=Adotas|accessdate=9 March 2014}}&lt;/ref&gt;

In September 2012, Rocket Fuel, among other advertising companies, partnered with Facebook’s FBX (Facebook Exchange), bringing users customized ads based on their web surfing habits on other sites.&lt;ref name=AdWeek&gt;{{cite web|url=http://www.adweek.com/news/technology/buyers-bullish-facebook-exchange-143658|title=Buyers Bullish on Facebook Exchange 15 ad tech firms on board; most rave about platform's potential|author=Tim Peterson|date=13 September 2012|accessdate=3 December 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.mediapost.com/publications/article/219994/rocket-fuel-partners-with-27-companies-adds-mobil.html|title=Rocket Fuel Partners With 27 Companies, Adds Mobile To Its Package|work=RTM Daily|accessdate=9 March 2014}}&lt;/ref&gt;

==References==
{{reflist|33em}}

==Further reading==
*[http://video.cnbc.com/gallery/?video=3000241603 RocketFuel CEO: Google fantastic player]
*[http://www.bloomberg.com/news/2014-03-06/ipo-dot-com-bubble-echo-seen-muted-as-older-companies-go-public.html IPO Dot-Com Bubble Echo Seen Muted as Older Companies Debut]

==External links==
*{{Official site|http://www.rocketfuel.com}}
*[http://www.cci.co.jp/en/index.html Cyber Communications Inc. site]

[[Category:Big data]]
[[Category:Technology companies established in 2008]]
[[Category:2008 establishments in California]]
[[Category:Companies based in Redwood City, California]]
[[Category:Companies listed on NASDAQ]]
[[Category:Internet advertising]]</text>
      <sha1>qtz5i3f9wqicru7hoj1vud1rx8x227b</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>RelateIQ</title>
    <ns>0</ns>
    <id>39853660</id>
    <revision>
      <id>601243193</id>
      <parentid>601242933</parentid>
      <timestamp>2014-03-25T20:08:20Z</timestamp>
      <contributor>
        <ip>64.124.196.130</ip>
      </contributor>
      <text xml:space="preserve" bytes="3780">{{Infobox company
| name             = RelateIQ
| logo             = [[File:RelateIQLogoJuly2013.png]]
| foundation       = 2011
| founder          = Adam Evans, Steve Loughlin
| location_city    = [[Palo Alto, California]] 
| location_country = [[United States]]
| industry         = [[Customer Relationship Management]]
| products         = [[Software as a service|Cloud-Based CRM]]
| homepage         = [http://relateiq.com RelateIQ.com]
}}

'''RelateIQ''' is an American enterprise software start-up based in [[Palo Alto, California]]. The company's software is a relationship intelligence platform that combines data from email systems, smartphone calls, and appointments to augment or replace standard relationship management tools or database solutions. It scans &quot;about 10,000 emails, calendar entries, and other data points per minute at first run.&lt;ref&gt;{{cite news|last=Rusli|first=Evelyn|title=Your New Secretary: An Algorithm|url=http://online.wsj.com/article/SB10001424127887323949904578539983425941490.html?KEYWORDS=relateiq|accessdate=7/8/13|newspaper=The Wall Street Journal|date=6/12/13}}&lt;/ref&gt;&quot;

== Team ==
The company was founded in July 2011 by Adam Evans and Steve Loughlin. At the end of July 2013, the company made headlines by bringing on data scientist DJ Patil as VP of Product.&lt;ref&gt;{{cite web|author= |url=http://techcrunch.com/2013/07/31/greylocks-data-scientist-in-residence-dj-patil-joins-enterprise-relationship-manager-relateiq-as-vp-of-product/ |title=Greylock’s Data Scientist In Residence DJ Patil Joins Enterprise Relationship Manager RelateIQ As VP Of Product |publisher=TechCrunch |date=2013-07-31 |accessdate=2013-10-02}}&lt;/ref&gt; The team also includes Armando Mann, the recent head of sales at Dropbox and a former Google executive.

== Advisers ==
Advisors include Bob Cohn, Bill Campbell, Ping Li, and Joe Lonsdale.&lt;ref&gt;{{cite news|last=Empson|first=Rip|title=RelateIQ Launches With $29M From Formation 8, Dustin Moskovitz And More To Be Your Next-Gen Relationship Manager|url=http://techcrunch.com/2013/06/12/relateiq-launches-with-29m-from-formation-8-dustin-moskovitz-and-more-to-be-your-next-gen-relationship-manager/|accessdate=7/8/13|newspaper=TechCrunch|date=6/12/13}}&lt;/ref&gt;

== Technology ==
Unlike traditional relationship management systems, which rely on data input by users to keep their teams informed and run predictive analytics, RelateIQ's platform automatically isolates and analyzes a user's professional emails and other interactions. By combining this data with information gleaned from other sources, such as LinkedIn and Facebook, RelateIQ leverages data science to &quot;to comb through emails, analyze them, and offer reminders and suggestions to busy salespeople.&quot;&lt;ref&gt;{{cite news|last=Novet|first=Jordan|title=Startups fatten up Salesforce.com with analytics and data|url=http://venturebeat.com/2013/12/03/startups-fatten-up-salesforce-com-with-analytics-and-data/|accessdate=12/10/13|newspaper=VentureBeat|date=12/3/13}}&lt;/ref&gt;

In March 2014, the company released &quot;Closest Connections,&quot; a feature that &quot;automates a normally time-consuming—and potentially erroneous—sales process&quot; by identifying warm introductions for potential prospects based on real activity happening within a team's email inboxes, calendars, and social networks.&lt;ref&gt;{{cite news|last=Novet|first=Jordan|title=RelateIQ identifies the best employee to contact each new sales lead|url=http://venturebeat.com/2014/03/22/relateiq-identifies-the-best-employee-to-contact-each-new-sales-lead/|newspaper=VentureBeat|date=3/22/14}}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Big data]]
[[Category:Data management software]]
[[Category:Enterprise software]]
[[Category:Business software]]
{{Catimprove|date=December 2013}}


{{software-company-stub}}</text>
      <sha1>fkf1sd4qdmwz9y2s17j4z3g6vmmnhi1</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Sessionization</title>
    <ns>0</ns>
    <id>41499984</id>
    <revision>
      <id>590599545</id>
      <parentid>588682365</parentid>
      <timestamp>2014-01-14T01:31:35Z</timestamp>
      <contributor>
        <username>Starcheerspeaksnewslostwars</username>
        <id>11554556</id>
      </contributor>
      <comment>−[[Category:Behavioral analytics]]; −[[Category:Business analytics]]; −[[Category:Sessions]]; ±[[Category:Data Mining]]→[[Category:Data mining]] using [[WP:HC|HotCat]]</comment>
      <text xml:space="preserve" bytes="5131">'''Sessionization''' is a common analytic operation in [[big data]] analysis and is used to measure user behavior.  [[Behavioral analytics]] focuses on how and why users of eCommerce platforms, online games and web applications behave by grouping certain events into sessions. The sessionization operation consists of a group of interactions that take place on a web or mobile platform during a certain time frame.   This operation identifies users' web browsing sessions by storing recorded [[Event (computing)|events]] and grouping them from each user, based on the time-intervals between each and every event.  Conceptually, if two events from the same user are made too far apart in time, they will be treated as coming from two browsing sessions.  A session can be as short as a few seconds or as long as several hours.&lt;ref&gt;Liu, Bing. Web Data Mining: Exploring Hyperlinks, Contents, and Usage Data. Berlin: Springer, 2007. Print&lt;/ref&gt;

Web usage patterns from users are becoming more complex as more data is being generated in web servers. Thus, many applications have been focusing on various ways to analyze them in order to recognize the usage patterns of users and other meaningful patterns.   
Valuable click patterns are detected from each session, such click patterns are often used for fraud detection, ads promotion, revenue prediction, and other applications.&lt;ref&gt;Brusilovsky, Peter, Alfred Kobsa, and W. Nejdl. The Adaptive Web: Methods and Strategies of Web Personalization. Berlin: Springer, 2007. Print&lt;/ref&gt;

==Description==
A [[session ID]] or session token is a piece of data that is used in network communications to identify a session and is usually granted to a visitor on his first visit to a site.  The session information is stored on the web server using the session identifier Session ID generated as a result of the first (sometimes the first authenticated) request from the end user running a web browser. The &quot;storage&quot; of Session IDs and the associated session data (user name, account number, etc.) on the web server is accomplished using a variety of techniques including, but not limited to, local memory, flat files, and databases.  
The Session ID is valid while the user is active and then when he or she stops being active, the session expires along with its session ID.  When a user decides to log in to a web platform, an identity is attached and a user ID is formed. You can then associate the user ID with its session ID, and begin collecting data about that certain user.  
Each session has its own characteristics such as overall time, amount of events, country IP, etc.  Events are stored under each session ID, which is linked to a user ID and then data is collected and analyzed.  One limitation is the time out threshold.  A single session can contain multiple pageviews, events, social interactions, custom variables, and e-commerce transactions.&lt;ref&gt;{{cite book|last=Arregoces|first=Mauricio|title=Data Center Fundamentals|year=2004|publisher=Cisco Press|url=http://books.google.co.il/books?id=DRIryrLoxKkC&amp;pg=PA380&amp;dq=session+id&amp;hl=iw&amp;sa=X&amp;ei=yH3CUuCwN6jC7AaH6oHABw&amp;redir_esc=y#v=onepage&amp;q=session%20id&amp;f=false}}&lt;/ref&gt;

==Types of Sessionization==
 [[File:Personalized Sessionization.png|400px|thumbnail|Personalized Sessionization]]
*Reactive Sessionization. Sessions that are obtained from log files are anonymous because web user identity does not appear explicitly in the registers.  This complicates the identification of a web users trail.  Web users with a similar profile could be accessing the same part of a site at the same time resulting in registers that appeared shuffled in logs.

*Sessions in Dynamic Environments. Different users may have different styles and preferences for internet browsing, and therefore the same time-out threshold may not accurately identify sessions for all users. For example, user A is a slower web-surfer than an average user and the typical interval between two events in same domain is 1 minute.  User B is a fast web surfer and the typical interval between two events in same domain is 15 seconds. For this example lets say that if an average user does not perform events in a particular web domain in 30 seconds, it indicates the end of a session.  These two users behave differently and so the sessions wont be accurately defined. A better solution would be to adaptively determine the session timeout threshold of user A, or B, based on their recent browsing behavior. In this case the threshold is tracked between each event along with how long the session was and then real-time averages of the sessions are created.&lt;ref&gt;{{cite book|last=D. Velasquez|first=Juan|title=Advanced Techniques in Web Intelligence|year=2010|url=http://books.google.co.il/books?id=l0cF55YkcKAC&amp;pg=PA37&amp;dq=sessionization&amp;hl=iw&amp;sa=X&amp;ei=Y4DCUvrjKKnA7AaSqYC4CQ&amp;redir_esc=y#v=onepage&amp;q=sessionization&amp;f=false}}&lt;/ref&gt;  In this way a very accurate picture of how the certain user behaves both during the session and between sessions is received.

==References==
{{Reflist}}

[[Category:Business intelligence]]
[[Category:Data mining]]
[[Category:Big data]]</text>
      <sha1>snm45jvo3ei7590crrm3wvmz24d2vom</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>AdNear</title>
    <ns>0</ns>
    <id>39585396</id>
    <revision>
      <id>592432591</id>
      <parentid>592430992</parentid>
      <timestamp>2014-01-26T05:09:58Z</timestamp>
      <contributor>
        <ip>198.72.150.41</ip>
      </contributor>
      <comment>Added company infobox</comment>
      <text xml:space="preserve" bytes="3270">{{Infobox company
| name             = AdNear
| logo             = 
| caption          = 
| type             = [[Private company|Private]]
| genre            = 
| fate             = 
| predecessor      = 
| successor        = 
| foundation       = [[Bangalore]], [[India]] ({{Start date|2012}})
| founder          = Anil Mathews
| defunct          = 
| location_city    = 
| location_country = [[Singapore]]
| location         = 
| locations        = [[Singapore]], [[Bangalore]], [[Mumbai]], [[Sydney]], [[Jakarta]]
| area_served      = [[Earth|Global]]
| key_people       = 
| industry         = Big Data
| products         = 
| services         = 
| revenue          = 
| operating_income = 
| net_income       = 
| aum              = 
| assets           = 
| equity           = 
| owner            = 
| num_employees    = 45
| parent           = 
| divisions        = 
| subsid           = 
| homepage         =  {{URL|www.adnear.com}}
| footnotes        = 
| intl             = 
}}

'''AdNear''' (formerly '''Imere Technologies''')&lt;ref&gt;{{cite web |url=http://www.adnear.com/about |title=About AdNear at adnear.com}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.linkedin.com/company/2811352?trk=tyah&amp;trkInfo=tas%3Aadnear%2Cidx%3A1-1-1|title=AdNear Linkedin Profile}}&lt;/ref&gt; is a [[Privately held company|privately held]] big data company founded in Nov 2012 having direct presence in [[Singapore]], [[India]], [[Indonesia]], and [[Australia]]. It is funded by venture capital firms [[Sequoia Capital]]&lt;ref&gt;{{cite web |url=http://www.sequoiacap.com/india/adnear/info |title=Adnear : Sequoia Capital : India}}&lt;/ref&gt; and [[Canaan Partners]].&lt;ref&gt;{{cite news |url=http://techcircle.vccircle.com/2012/11/28/location-based-mobile-advertising-platform-adnear-raises-6-3m-from-sequoia-canaan-partners/ |title=AdNear raises $6.3M from Sequoia &amp; Canaan Partners}}&lt;/ref&gt;

It is a big data company that uses location data to drive superior ad targeting across mobile devices. It's advertising platform is built on top of proprietary hybrid geo-location platform, giving it the strength of location awareness on phones without the need of GPS or operator assistance. AdNear enables brands reach audience relevant to them across Asia &amp; Australia.&lt;ref name=&quot;investvine&quot;&gt;{{cite web|url=http://investvine.com/mobile-advertising-platform-targets-asean/|title=Mobile advertising platform targets ASEAN|first=Todd|last=Watson|work=Inside Investor|date=30 July 2013|accessdate=31 July 2013}}&lt;/ref&gt; AdNear's platform is used by major brands such as [[P&amp;G]], [[Unilever]], [[Google]], [[Intel]], [[Microsoft]], [[Nimbuzz]], [[SONY]], [[Audi]], [[Toyota]], [[Ford]], [[Titan]], [[Pizza Hut]], [[KFC]], [[McDonalds]], [[Red Bull]], [[Samsung]], [[Nokia]], [[Bharti Airtel|Airtel]], [[Vodafone]], [[Amobee]], [[MAC Cosmetics|MAC]], [[Toblerone]], [[Ambi Pur]], [[Starhub]], and [[Magnum]].&lt;ref&gt;{{cite news |url=http://www.moneycontrol.com/news/business/adnear-woos-advertiserslocation-based-advertising_884110.html |title=AdNear woos advertisers with location-based advertising}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
* [http://www.crunchbase.com/company/adnear AdNear | Crunbase profile]
* [http://www.adnear.com/ AdNear] (company website)

{{Tech-company-stub}}

[[Category:Big data]]</text>
      <sha1>19dkzsa7nbfcaf6dv1qtu1tpc9a8hgc</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Datafication</title>
    <ns>0</ns>
    <id>41731546</id>
    <revision>
      <id>592164813</id>
      <parentid>592164769</parentid>
      <timestamp>2014-01-24T12:26:53Z</timestamp>
      <contributor>
        <username>Alexdruk</username>
        <id>5336120</id>
      </contributor>
      <text xml:space="preserve" bytes="1121">'''Datafication''' is a modern technological trend turning many aspects of our life into computerised data &lt;ref name=&quot;CukierMayer-Schoenberger2013&quot;&gt;{{cite journal | last = Cukier | first =Kenneth | last2 = Mayer-Schoenberger | first2 = Viktor  | title =The Rise of Big Data | journal =Foreign Affairs, | issue =May/June | pages = 28-40. | date =2013  | url = http://www.foreignaffairs.com/articles/139104/kenneth-neil-cukier-and-viktor-mayer-schoenberger/the-rise-of-big-data | accessdate = 24 January 2014}}&lt;/ref&gt; and transforming this information into new forms of value. 
&lt;ref name=&quot;SchuttOneil2014&quot;&gt;
{{cite book
 | last = O'Neil | first =Cathy
 | last2 =Schutt
 | first2 = Rachel
| title =Doing Data Science
 | publisher =O’Reilly Media
 | date =2013
 | pages =406
  | isbn =978-1-4493-5865-5
 }}
&lt;/ref&gt;
Examples of datafication are how [[Twitter]] datafies stray thoughts or datafication of [[Human_resource_management|HR]] by [[LinkedIn]] and others.  
==References==
{{Reflist}}
[[Category:Information science]]
[[Category:Technology forecasting]]
[[Category:Data management]]
[[Category:Big data]]
{{Tech-stub}}</text>
      <sha1>sqg2bmvy2r2oudov6vtqj3c4tzwz30k</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>MonetDB</title>
    <ns>0</ns>
    <id>1862647</id>
    <revision>
      <id>602436825</id>
      <parentid>601175723</parentid>
      <timestamp>2014-04-02T16:26:33Z</timestamp>
      <contributor>
        <ip>194.171.31.45</ip>
      </contributor>
      <comment>/* External links */ Remove non-existing wiki link</comment>
      <text xml:space="preserve" bytes="10376">{{Infobox software|
  name = MonetDB |
  logo =  [[Image:Monetdb-logo.png|181px|MonetDB logo]] |
  developer = MonetDB Developer Team |
  latest_release_version = Jan2014-SP1 |
  latest_release_date = {{release date |2014|03|10}}|
  programming language   = [[C (programming language)|C]] |
  operating_system = [[Cross-platform]] |
  genre = [[Database management system]] |
  license = MonetDB License (based on the [[Mozilla Public License|MPL]] 1.1) |
  website = {{URL |www.monetdb.org}}
}}

'''MonetDB''' is an [[open source]] [[Column-oriented DBMS|column-oriented]] [[database management system]] developed at the [[Centrum Wiskunde &amp; Informatica]] (CWI) in the [[Netherlands]].
It was designed to provide high performance on complex queries against large databases, such as combining [[table (database)|tables]] with hundreds of columns and multi-million rows.
MonetDB has been applied in high-performance applications for
[[data mining]], [[online analytical processing]] (OLAP), [[geographic information system]]s,&lt;ref name=gis&gt;{{cite web | url = http://www.monetdb.org/Documentation/Extensions/GIS | title = GeoSpatial - MonetDB | date = 4 March 2014}}&lt;/ref&gt; [[Resource Description Framework|RDF]],&lt;ref name=rdf&gt;{{cite web | url = http://lod2.eu/Project/MonetDB.html | title = MonetDB - LOD2 - Creating Knowledge out of Interlined Data | date = 6 March 2014}}&lt;/ref&gt; streaming data processing,&lt;ref name=datacell&gt;{{cite web | url = http://www.monetdb.org/Documentation/Extensions/Streams | title = Streaming - MonetDB | date = 4 March 2014}}&lt;/ref&gt; text and [[multimedia]] retrieval.

== History ==

MonetDB (initially only called Monet) was first created by 2002 doctoral student [http://www.cwi.nl/~boncz/ Peter Alexander Boncz] and professor [http://www.cwi.nl/~mk/ Martin L. Kersten] as part of the 1990s MAGNUM research project at [[University of Amsterdam]].&lt;ref&gt;{{Cite book |title= Monet: A Next-Generation DBMS Kernel For Query-Intensive Applications |work= Ph.D. Thesis |publisher= Universiteit van Amsterdam |date= May 2002 |url= http://oai.cwi.nl/oai/asset/14832/14832A.pdf }}&lt;/ref&gt; The first version under an [[open-source software]] license (a modified version of the [[Mozilla Public License]]) was released on September 30, 2004.&lt;ref&gt;[https://www.monetdb.org/AboutUs MonetDB historic background]&lt;/ref&gt;

Data mining projects in the 1990's required for better analytical database support. This resulted in a [[CWI]] the [[Corporate spin-off|spin-off]] called Data Distilleries, which used early MonetDB implementations in its analytical suite. Data Distilleries eventually became a subsidiary of [[SPSS]] in 2003, which in turn was acquired by [[IBM]] in 2009.&lt;ref name=datacell&gt;{{cite web | url = http://www.monetdb.org/AboutUs | title = A short history about us - MonetDB | date = 6 March 2014}}&lt;/ref&gt;

MonetDB introduced innovations at all [[database management system]] layers: a storage model based on vertical fragmentation, a modern [[CPU]]-tuned query execution architecture that often gave MonetDB a speed advantage on the same [[algorithm]] over a typical [[interpreter (computing)|interpreter-based]] [[RDBMS]].
MonetDB is one of the first database systems to focus its query optimization effort on exploiting [[CPU cache]]s. MonetDB also features automatic and self-tuning indexes, run-time query optimization, and a modular software architecture.&lt;ref&gt;{{Cite journal |author= Stefan Manegold |title= An Empirical Evaluation of XQuery Processors |work= Proceedings of the International Workshop on Performance and Evaluation of Data Management Systems (ExpDB) |publisher= ACM |date= June 2006 |url= http://oai.cwi.nl/oai/asset/19337/19337B.pdf |accessdate= December 11, 2013 |doi= 10.1016/j.is.2007.05.004 }}&lt;/ref&gt;&lt;ref&gt;P. A. Boncz, T. Grust, M. van Keulen, S. Manegold, J. Rittinger, J. Teubner. [http://www.cwi.nl/htbin/ins1/publications?request=pdf&amp;key=BoGrKeMaRiTe:SIGMOD:06 MonetDB/XQuery: A Fast XQuery Processor Powered by a Relational Engine]. In Proceedings of the ACM SIGMOD International Conference on Management of Data, Chicago, IL, USA, June 2006.&lt;/ref&gt;

By 2008, a follow-on project called X100 (MonetDB/X100) started, which evolved into the [[Vectorwise|VectorWise]] technology. VectorWise was acquired by [[Actian Corporation]], integrated with the [[Ingres (database)|Ingres database]] and sold as a commercial product.&lt;ref&gt;{{Cite journal |title= From x100 to vectorwise: opportunities, challenges and things most researchers do not think about |authors=  Marcin Zukowski and Peter Boncz |publisher= ACM |work= Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data |date= May 20, 2012 |pages= 861â€“862 |doi= 10.1145/2213836.2213967 |isbn= 978-1-4503-1247-9 }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | title = Integration of VectorWise with Ingres | authors =  Inkster, D. and Zukowski, M. and Boncz, P. A. |publisher= ACM | work= ACM SIGMOD Record |date= September 20, 2011 | url = http://www.sigmod.org/publications/sigmod-record/1109/pdfs/08.industry.inkster.pdf }}&lt;/ref&gt;

== Architecture ==

MonetDB architecture is represented in three layers, each with its own set of optimizers.&lt;ref name=monetdb-20&gt;{{Cite journal |author= Idreos, S. and Groffen, F. E. and Nes, N. J. and Manegold, S. and Mullender, K. S. and Kersten, M. L. |title= MonetDB: Two Decades of Research in Column-oriented Database Architectures |work= IEEE Data Engineering Bulletin |publisher= IEEE |date= March 2012 |url= http://oai.cwi.nl/oai/asset/19929/19929B.pdf |accessdate= March 6, 2014 |pages=  40–45}}&lt;/ref&gt;
The front-end is the top layer, providing query interfaces for [[SQL]], SciQL, [[SPARQL]] and [[Jaql|JAQL]].&lt;ref&gt;[http://www.monetdb.org/Documentation/Extensions/JAQL Jaqueline JAQL Extension of MonetDB]&lt;/ref&gt; Queries are parsed into domain-specific representations, like relational algebra for SQL, and optimized. The generated logical execution plans are then translated into MonetDB Assembly Language (MAL) instructions, which are passed to the next layer. The middle or back-end layer provides a number of cost-based optimizers for the MAL. The bottom layer is the database kernel, which provides access to the data stored in Binary Association Tables (BATs). Each BAT is a table consisting of a Object-identifier and value columns, representing a single column in the database.&lt;ref name=monetdb-20 /&gt;

MonetDB internal data representation also relies on the memory addressing ranges of contemporary CPUs using [[demand paging]] of memory mapped files, and thus departing from traditional DBMS designs involving complex management of large data stores in limited memory.

== Components ==
A number of extensions exist for MonetDB that extend the functionality of the database engine. Due to the three-layer architecture, top-level query interfaces can benefit from optimizations one in the backend and kernel layers.

=== SQL ===
MonetDB/SQL is an top-level extension, wich provides complete support for transactions in compliance with the [[SQL:2003]] standard.&lt;ref name=monetdb-20 /&gt;

=== GIS ===
MonetDB/GIS is an extension to MonetDB/SQL with support for the [[Simple Features]] standard of [[OpenGIS]].&lt;ref name=gis /&gt;

=== SciQL ===
SciQL an SQL-based query language for science applications with arrays as first class citizens. SciQL allows MonetDB to effectively function as an [[Array DBMS|array database]]. It is used in the [http://www.scilens.org SciLens] and will be further extended for the [[Human Brain Project]].&lt;ref&gt;{{cite web | url = http://www.sciql.org | title = SCIQL.ORG | date = 4 March 2014}}&lt;/ref&gt;

=== DataCell ===
MonetDB/DataCell adds stream processing facilities on top of the column-store architecture of MonetDB. It provides facilities for data analysis of-the-fly with the database system itself.&lt;ref name=datacell /&gt;
&lt;ref name=monetdb-20 /&gt;

=== RDF/SPARQL ===
MonetDB/RDF is a [[SPARQL]]-based extension for working linked data, which adds support for [[RDF]] and allowing MonetDB to function as a [[triplestore]]. Under development for the [[LOD2]] project.&lt;ref name=rdf /&gt;

== See also ==
* [[List of relational database management systems]]
* [[Database management system]]
* [[Column-oriented DBMS]]
* [[Array DBMS]]

== External links ==
* [https://www.monetdb.org/ Official homepage of MonetDB]
* [https://www.monetdbsolutions.com/ MonetDB Solutions - MonetDB's professional services company]
* [https://www.monetdb.org/Documentation In-depth technical information about MonetDB]
* [http://monetr.r-forge.r-project.org MonetDB.R - MonetDB to R Connector]
* [https://github.com/hannesmuehleisen/monetdb-hadoop-pig-loader A Pig/Hadoop loader for MonetDB] 
* [https://metacpan.org/module/DBD::monetdb MonetDB's DBD driver] for the [[Perl DBI]]
* [http://www.monetdb.org/Home/ProjectGallery List of scientific projects using MonetDB]
** [http://www.scilens.org/ SciLens] - SciLens project aims at becoming the portal for database technology to advance science
** [http://www.earthobservatory.eu/ TELEIOS] - Creating a A Virtual Earth Observatory for the [[TerraSAR-X]] satellite archive of [[DLR]].&lt;ref&gt;{{cite web |url=http://www.earthobservatory.eu | title=TELEIOS - Virtual Observatory Infrastructure for Earth Observation Data | date = 4 March 2014}}&lt;/ref&gt;
** [http://lod2.eu/WikiArticle/Project.html LOD2] - Developing new technologies for enabling scalable management of [[Linked Open Data]]
** [https://www.humanbrainproject.eu Human Brain Project] - The project aims to simulate the complete human brain on supercomputers to better understand how it functions.
** [http://coherentpaas.eu/ CohernetPaaS] - The project aims to provide a full ACID-coherent Database-as-a-Service integrating SQL, NoSQL and CEP data technologies

== References ==
{{Reflist|2}}

{{FOLDOC}}

{{Data warehouse}}

{{DEFAULTSORT:Monetdb}}
[[Category:Free database management systems]]
[[Category:Database management systems]]
[[Category:Data warehousing products]]
[[Category:Big data]]
[[Category:Dutch inventions]]
[[Category:Cross-platform software]]
[[Category:Client-server database management systems]]
[[Category:Linux database-related software]]
[[Category:Free software programmed in C]]
[[Category:Cross-platform free software]]
[[Category:Relational database management systems]]
[[Category:Products introduced in 2004]]
[[Category:Database engines]]
[[Category:Structured storage]]</text>
      <sha1>2535fglmfu70vfzatlau99ya4aiwpxt</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Sogamo</title>
    <ns>0</ns>
    <id>41729652</id>
    <revision>
      <id>600033857</id>
      <parentid>599923246</parentid>
      <timestamp>2014-03-17T17:11:26Z</timestamp>
      <contributor>
        <username>Vegaswikian</username>
        <id>214427</id>
      </contributor>
      <comment>Disambiguated: [[Apps]] → [[Mobile app]]</comment>
      <text xml:space="preserve" bytes="3937">{{Orphan|date=February 2014}}

:''This article is about the software company.''
{{Infobox company
|name=Sogamo ソガモ
|logo=[[Image:Sogamo logo.png|250px|Sogamo's current company logo]]
|type=[[Big Data]] and App Analytics
|foundation=[[Singapore]] (June 26, 2008)
|CEO=Nelson Allen
|founder=Elvin Li
|location=[[Singapore]]
|area_served= International
|key_people=
|industry=[[Mobile phone|Mobile]], Games, [[Mobile app|App]]s, [[World Wide Web|Web]]
|products=''Sogamo''
|services=
|revenue=
|operating_income=
|net_income=
|assets=
|equity=
|owner=ZelRealm Interactive Pte Ltd
|num_employees=
|parent=ZelRealm Interactive Pte Ltd
|divisions=
|subsid=
|caption=
|homepage=[http://www.sogamo.com www.sogamo.com]
|footnotes=
|intl=
}}

'''Sogamo''' is a Singapore app analytics founded in June 2008. The company is best known for its specialisation in data analytics for mobile, games and web applications; it is also recognised in Asia for providing personalised marketing tools for mobile and web developers, including a big data analytics platform, targeted messaging tool and smart offer engine in real-time to support user acquisition, retention and monetization.

==History==
Sogamo, originally known as ZelRealm, was established in June 2008 by Elvin Li when he was still a student at the [[National University of Singapore]].

The company initially focused on developing a virtual goods market place for [[Massively multiplayer online game]]s. In 2009, it released ZelTrade an online virtual goods platform working with Korean game developers, which was then overtaken by much more well-funded American competitors such as LiverGamer and PlaySpan. After several years of reiterating the product through working closely with the market, the plan of Sogamo was born in late 2011 and the platform was later released in March 2013.

Today Sogamo's focus is on Asia, where half the world’s mobile app users will be by 2017. The company has been expanding into other market verticals to provide real-time personalised marketing; starting from its current client base from the mobile gaming space to e-commerce space and subsequently the financial industry.

The company received seed investment in 2012 from IncuVest Asia,&lt;ref name=&quot;The Bridge&quot;&gt;{{cite web|title=ソーシャルゲーム分析とパーソナライズエンジンのSogamoがIncuVestから資金調達|publisher=The Bridge|url=http://thebridge.jp/2013/01/social-games-analytics-and-recommendation-engine-sogamo-raises-funds-from-incuvest|accessdate=2014-01-23|date=2013-01-16}}&lt;/ref&gt;&lt;ref name=&quot;5point&quot;&gt;{{cite web|title=数据解释比数据分析更重要|publisher=5Point|url=http://www.5point.cn/2076.html|accessdate=2014-01-23|date=2012-12-24}}&lt;/ref&gt; and grew to 10 full-time staff. In September 2013, it received recognition in Asia as one of Red Herring's Top 100 Asia Finalist,&lt;ref name=&quot;Red Herring&quot;&gt;{{cite web|title=2013 Red Herring Asia: Finalists|publisher=[[Red Herring (magazine)]]|url=http://www.redherring.com/events/red-herring-asia/finalists2013/|accessdate=2014-01-23|date=2013-09-03}}&lt;/ref&gt; which lead to a partnership with SAP HANA.&lt;ref name=&quot;The Star&quot;&gt;{{cite web|title=SAP wants to co-innovate with its partners|publisher=The Star|url=http://www.thestar.com.my/Tech/Tech-News/2013/08/23/SAP-wants-to-coinnovate-with-its-parners/|accessdate=2014-01-23|date=2013-08-23}}&lt;/ref&gt;

In 2013, Nelson Allen, a former Microsoft and Samsung executive, joined as CEO to drive the next growth stage.&lt;ref name=&quot;Crunchbase&quot;&gt;{{cite web|title=Sogamo's Crunchbase profile|publisher=[[Crunchbase]]|url=http://www.crunchbase.com/company/sogamo|accessdate=2014-01-23|date=2014-01-07}}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Internet in Singapore]]
[[Category:Windows Phone software]]
[[Category:IOS software]]
[[Category:Android (operating system) software]]
[[Category:Cross-platform software]]
[[Category:Big data]]
[[Category:Companies based in Singapore]]</text>
      <sha1>mdwdpqed7urkvq110p3gyv7m5y9vti3</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>CVidya</title>
    <ns>0</ns>
    <id>31890204</id>
    <revision>
      <id>595107508</id>
      <parentid>594948918</parentid>
      <timestamp>2014-02-12T07:31:04Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fix for #61.  Punctuation goes before References. Do [[Wikipedia:GENFIXES|general fixes]] if a problem exists. - using [[Project:AWB|AWB]] (9936)</comment>
      <text xml:space="preserve" bytes="3034">{{Use dmy dates|date=November 2011}}
{{Infobox company
| name = cVidya Networks
| logo = Image:Cvidya_Logo.gif
| type = Private
| industry = Telecommunication, [[Service (economics)|Services]], [[Consultant|Consulting]], [[Revenue Management]], [[Revenue assurance|Revenue Assurance]], [[Fraud|Fraud Management]], [[Dealership Management System|Dealer management]]
| foundation = 2001
| founder = [http://www.cvidya.com/Management.html Alon Aginsky]
| location = [[Herzlia]], Israel
| employment = 300 (2010)

| homepage = [http://www.cvidya.com/ www.cvidya.com]}}

'''cVidya Networks''' is a provider of revenue intelligence [[solution]]s for [[communications]] and [[Telecommunications service provider|digital service providers]].

cVidya's investors include [[Battery Ventures]], [[Carmel Ventures]], Hyperion, StageOne, Saints Capital and Plenus.

==History==

cVidya was founded in 2001 by Alon Aginsky in the US and saw successful projects at [[Telecom Italia]] and [[Bezeq Israel]] within its first couple of years.

Between 2004–2010 cVidya became one of the leading vendors in the [[Revenue Assurance]] domain with the MoneyMap® product suite, and gained global market penetration which began with their entrance into Europe followed closely by LATAM, APAC, North America and Africa.

cVidya’s acquisition of ECtel in 2010 &lt;ref&gt;[http://www.billingworld.com/news/2010/01/cvidya-wraps-ectel-acquisition.aspx cVidya Wraps ECtel Acquisition]. Billingworld.com&lt;/ref&gt; was a major milestone for the company. By acquiring the larger, publicly traded company and through consolidating their product portfolios cVidya added the FraudView® [[Fraud Management]] product and integrated it to its Revenue Analytics solutions portfolio.

In 2011 cVidya adds [[Risk Management]] product to its portfolio and launches the Education Center service, offering Revenue Assurance and Fraud Management [[eLearning]] courses to industry professionals.&lt;ref&gt;[http://www.telecomasia.net/content/cvidya-launches-elearning-education-center?src=related cVidya launches eLearning education center]. TelecomAsia.net&lt;/ref&gt; In that same year, [[Gartner]] Ranks cVidya as the global market leader.&lt;ref&gt;[http://www.billingworld.com/news/2011/04/gartner-puts-cvidya-at-top-of-revenue-assurance-m.aspx Gartner Puts cVidya at Top of Revenue Assurance Market]. Billingworld.com&lt;/ref&gt;

In 2013 cVidya expands its portfolio, adding [[Big Data Analytics]] capabilities and [[Marketing Analytics]] solution suite.&lt;ref&gt;[http://www.telcoprofessionals.com/pressreleases/611/c-vidya-launches-powerful-marketi cVidya Launches Powerful Marketing Analytics Suite]. TelcoProfessionals.com&lt;/ref&gt;

==Pictures==
&lt;gallery&gt;
:Image:Cvidya_Logo.gif|cVidya Logo
&lt;/gallery&gt;

==External links==
* [http://www.cvidya.com cVidya website]

== References ==
{{reflist|colwidth=30em}}

[[Category:Revenue assurance]]
[[Category:Big data]]
[[Category:Software companies of Israel]]
[[Category:Business intelligence companies]]
[[Category:Revenue services]]
[[Category:Consulting firms]]</text>
      <sha1>csmzqxs6hjq4suufltpdir2ds5yhkqg</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Alpine Data Labs</title>
    <ns>0</ns>
    <id>42149032</id>
    <revision>
      <id>601947514</id>
      <parentid>601946932</parentid>
      <timestamp>2014-03-30T11:49:51Z</timestamp>
      <contributor>
        <username>Shrimp Called Jake</username>
        <id>20936306</id>
      </contributor>
      <comment>logo</comment>
      <text xml:space="preserve" bytes="10009">{{Infobox company
| name             = Alpine Data Labs
| logo             = File:Alpine_only_logo.jpg
| logo_caption     = 
| image            = 
| image_caption    = 
| type             = Private
| industry         = 
| fate             = 
| predecessor      = 
| successor        = 
| foundation       = [[Beijing]], [[China]] 2010
| co-founders          = Anderson Wong &amp; Yi-Ling Chen
| defunct          = &lt;!-- {{End date|YYYY|MM|DD}} --&gt;
| location_city    = San Francisco, California
| location_country = USA
| locations        = &lt;!-- Number of locations, stores, offices, etc. --&gt;
| area_served      = 
| key_people       = Joe Otto, President &amp; CEO
| services         = Advanced Analytics on Hadoop and Big Data
| num_employees    = 45 (As of October 2013)
| parent           = 
| divisions        = 
| subsid           = 
| homepage         = {{URL|http://www.alpinenow.com}}
}}

'''Alpine Data Labs''' is an advanced analytics interface working with [[Apache Hadoop]] and [[big data]].&lt;ref&gt;{{cite web|url=http://sandhill.com/article/sand-hill-50-swift-and-strong-in-big-data/|title=Sand Hill 50 “Swift and Strong” in Big Data|publisher=Sand Hill|date=1/8/14|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref name=expand1&gt;{{cite web|url=http://insideanalysis.com/2014/01/10-companies-and-technologies-to-watch-in-2014/|title=10 Companies and Technologies to Watch in 2014|publisher=Inside Analysis|author=Robin Bloor|date=1/6/14|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://datadoodle.com/2013/12/04/alpine-data-and-goliath/|title=Alpine Data and Goliath|publisher=Data Doodle|author=Ted Cuzzillo|date=12/4/13|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://venturebeat.com/2013/10/30/big-data-little-companies-these-six-startups-want-to-disrupt-the-data-world/|title=Big data, little companies: These six startups want to disrupt the data world|publisher=Venture Beat|author=Eric Blattberg|date=10/30/13|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.allanalytics.com/author.asp?section_id=2220&amp;doc_id=268388|title=Big-Data Draws Attention at Interop New York|publisher=All Analytics|author=Noreen Seebacher|date=10/2/13|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.forbes.com/sites/emc/2013/12/03/making-big-data-work-for-your-business/|title=Making Big Data Work For Your Business|publisher=Forbes|author=Scott Koegler|date=12/3/13|accessdate=3/8/14}}&lt;/ref&gt; It provides a collaborative, visual environment to create and deploy analytics workflow and predictive models.&lt;ref&gt;{{cite web|url=http://www.analystone.com/the-analyst-one-top-technologies-list/|title=The breakthrough technologies every analyst should know about|publisher=Analyst One|author=Bob Gourley|date=10/17/13|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://siliconangle.com/blog/2013/10/29/2014-the-year-of-big-data-applications-bigdatanyc-2013/|title=2014, the year of Big Data applications|publisher=Silicon Angle|author=Valentina Craft|date=10/29/13|accessdate=3/8/14}}&lt;/ref&gt; This aims to make analytics more suitable for business analyst level staff, like sales and other departments using the data, rather than requiring a &quot;data scientist&quot; who understands languages like [[MapReduce]] or [[Pig (programming tool)|Pig]].&lt;ref name=expand1/&gt;&lt;ref name=expand2&gt;{{cite web|url=http://venturebeat.com/2013/11/22/alpine-data-labs-gets-16m-to-ensure-companies-wont-fail-with-hadoop/|title=Alpine Data Labs gets $16M to ensure companies ‘won’t fail’ with big data analytics|publisher=Venture Beat|author=Christina Farr|date=11/22/13|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref name=expand3&gt;{{cite web|url=http://gigaom.com/2013/11/22/alpine-data-labs-raises-16-million-for-its-visual-approach-to-data-science/|title=Alpine Data Labs raises $16 million for its visual approach to data science|publisher=Gigaom|author=Derrick Harris|date=11/22/13|accessdate=3/8/14}}&lt;/ref&gt;

Co-founded by Anderson Wong and Yi-Ling Chen, Joe Otto serves as president and [[CEO]] of Alpine Data Labs.&lt;ref&gt;{{cite web|url=http://www.forbes.com/sites/gilpress/2013/09/24/whats-a-cmo-to-do-alpine-data-labs-otto-and-aziza-on-the-digital-marketing-landscape/|title=What's A CMO To Do? Alpine Data Labs' Otto And Aziza On The Digital Marketing Landscape|publisher=Forbes|author=Gil Press|date=9/24/13|accessdate=3/8/14}}&lt;/ref&gt;

==History==

[[Greenplum]] commissioned its then employees Anderson Wong and Yi-Ling Chen to develop an [[application software|app]] that could work with [[database|databases]].&lt;ref name=expand6&gt;{{cite web|url=http://gigaom.com/2011/07/20/greenplum-protege-brings-predictive-muscle-to-exadata/|title=Greenplum protégé brings predictive muscle to Exadata|publisher=Gigaom|author=Derrick Harris|date=7/20/11|accessdate=3/8/14}}&lt;/ref&gt; Greenplum was acquired by [[EMC Corporation]] and Alpine Data Labs was co-founded by Wong and Chen in 2010.&lt;ref name=expand4&gt;{{cite web|url=http://www.datanami.com/datanami/2013-10-29/alpine_demos_big_data_analytics_from_an_ipad.html|title=Alpine Demos Big Data Analytics from an iPad|publisher=Datanami|author=Alex Woodie|date=10/29/13|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://techcrunch.com/2011/05/11/alpine-data-labs-scores-7-5-million-to-help-companies-analyze-troves-of-data/|title=Alpine Data Labs Scores $7.5 Million To Help Companies Analyze Troves Of Data|publisher=TechCrunch|author=Robin Wauters|date=5/11/11|accessdate=3/8/14}}&lt;/ref&gt; The site launched in May 2011 with Wong serving as CEO.&lt;ref name=expand5&gt;{{cite web|url=http://www.bizjournals.com/sanfrancisco/news/2011/05/11/alpine-labs-gets-75m-to-mine-big-data.html|title=Alpine Labs gets $7.5M to mine Big Data|publisher=San Francisco Business Times|author=Patrick Hoge|date=5/11/11|accessdate=3/8/14}}&lt;/ref&gt; That month, Alpine raised 7.5 million in [[Series A round]] funding from EMC Greenplum, Sierra Ventures, Mission Ventures, and Sumitomo Corp. Equity Asia.&lt;ref name=expand5/&gt; The funding was used in part to move Alpine’s headquarters from [[Beijing]] to [[San Mateo, California]].&lt;ref&gt;{{cite web|url=http://www.dbta.com/Editorial/News-Flashes/Alpine-Data-Labs-Raises-75-Million-in-Series-A-Funding-and-Formally-Launches-in-the-US-75490.aspx|title=Alpine Data Labs Raises $7.5 Million in Series A Funding and Formally Launches in the U.S.|publisher=Database Trends and Applications|date=5/13/11|accessdate=3/8/14}}&lt;/ref&gt; It’s core product then, Alpine Miner, allowed for data non-data scientists to create predictive analytics data models without using code and used an &quot;In-Database&quot; model.&lt;ref&gt;{{cite web|url=http://siliconangle.com/blog/2011/05/23/alpine-data-labs-offers-visualization-tools-to-create-in-database-analytics-models/|title=Alpine Data Labs Offers Visualization Tools to Create In-Database Analytics Models|publisher=Silicon Angle|author=Jeffrey Kelly|date=5/23/11|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.datanami.com/datanami/2012-02-16/alpine_data_climbs_analytics_mountain.html|title=Alpine Data Climbs Analytics Mountain|publisher=Datanami|author=Robert Gelber|date=2/16/12|accessdate=3/8/14}}&lt;/ref&gt; In June 2011, Alpine Miner 2.0 for [[Oracle Database]] was released.&lt;ref name=expand6/&gt;

Tom Ryan was appointed CEO and president of Alpine Data Labs in January 2012 and served until April 2013. The following month, Joe Otto was appointed to serve as CEO and president.&lt;ref&gt;{{cite web|url=http://www.pehub.com/2013/03/alpine-data-labs-names-joe-otto-ceo/|title=Alpine Data Labs Names Joe Otto CEO|publisher=Reuters PE HUB|date=5/3/13|accessdate=3/8/14}}&lt;/ref&gt; In November 2013, Alpine Data Labs raised $16 million in Series B venture funding.&lt;ref name=expand2/&gt;&lt;ref name=expand3/&gt; Investors included Sierra Ventures, Mission Ventures, UMC Capital, and [[Robert Bosch GmbH|Robert Bosch Venture Capital GmbH]].&lt;ref&gt;{{cite web|url=http://www.forbes.com/sites/alexkonrad/2013/11/22/alpine-data-labs-raises-series-b/|title=This Startup Just Raised $16M To Help Barclays, Nike And Havas Play With Big Data|publisher=Forbes|author=Alex Konrad|date=11/22/13|accessdate=3/8/14}}&lt;/ref&gt; That same month, it also released Alpine 3.0, which introduced a [[drag and drop]] interface and access to data from any device that with internet capabilities, including [[tablet computer|tablets]] and phones.&lt;ref name=expand4/&gt;&lt;ref&gt;{{cite web|url=http://www.networkworld.com/slideshow/127869/products-of-the-week-111113.html#slide16|title=Products of the week 11.11.13|publisher=Network World|author=Brandon Butler|date=11/11/13|accessdate=3/8/14}}&lt;/ref&gt; This makes it possible for analysts to access data on Hadoop, and other databases and data warehouses, without IT having to move the data into another interface.&lt;ref&gt;{{cite web|url=http://www.itbusinessedge.com/blogs/it-unmasked/alpine-data-analytics-app-works-directly-against-hadoop.html|title=Alpine Data Analytics App Works Directly Against Hadoop|publisher=IT Business Edge|author=Mike Vizard|date=11/7/13|accessdate=3/8/14}}&lt;/ref&gt; Alpine also moved its headquarters from San Mateo to [[San Francisco]] in November 2013. In February 2014, Alpine Data Labs was added to the [[Gartner Magic Quadrant]] as a &quot;Niche Player.&quot;&lt;ref&gt;{{cite web|url=http://www.kdnuggets.com/2014/02/gartner-mq-for-advanced-analytics-platforms.html|title=SAS, IBM, RapidMiner, Knime leaders in Gartner MQ for Advanced Analytics Platforms|publisher=KDnuggets|author=Gregory Piatetsky|date=2/24/14|accessdate=3/8/14}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.gartner.com/doc/2667527/magic-quadrant-advanced-analytics-platforms|title=Magic Quadrant for Advanced Analytics Platforms|publisher=Gartner|date=2/19/14|accessdate=3/8/14}}&lt;/ref&gt;

In March 2014, Alpine Data Labs was certified by Databricks on [[Apache Spark]].&lt;ref&gt;{{cite web|url=http://inside-bigdata.com/2014/03/18/databricks-certifies-alpine-data-labs-spark/|title=Databricks Certifies Alpine Data Labs on Spark|publisher=Inside Big Data|author=Daniel Gutierrez|date=3/18/14|accessdate=3/30/14}}&lt;/ref&gt;

==References==
{{reflist|2}}
[[Category:Big data]]
[[Category:Companies established in 2010]]</text>
      <sha1>cv97jb935s4thih3aa078i9j2vol0q3</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
  <page>
    <title>Platfora</title>
    <ns>0</ns>
    <id>41715854</id>
    <revision>
      <id>601480237</id>
      <parentid>600289567</parentid>
      <timestamp>2014-03-27T08:56:11Z</timestamp>
      <contributor>
        <username>Spartaz</username>
        <id>1676211</id>
      </contributor>
      <comment>[[Wikipedia:Articles for deletion/Platfora]] closed as no consensus</comment>
      <text xml:space="preserve" bytes="5605">


{{Infobox company
|name             = Platfora
|industry         = [[Data analytics|Big Data Analytics]]
|founder          = Ben Werther ([[CEO]])
|location_city    = [[San Mateo, California]],
|num_employees    = 75
|homepage         = {{URL|http://www.platfora.com|Platfora.com}}
}}

'''Platfora, Inc.''' is a [[big data analytics]] company based in [[San Mateo, California]]. The firm’s software works in tandem with the [[open-source software]] framework [[Apache Hadoop]] to assist companies and government organizations with rapid data analysis, [[data visualization]], and sharing.&lt;ref name=&quot;1 Business Insider&quot;&gt;{{cite web|last=Bort|first=Julie|title=Larry Page's College Friend Has Launched a Cool New Startup|url=http://www.businessinsider.com/platfora-ben-werther-2012-10|publisher=Business Insider|accessdate=22 January 2014}}&lt;/ref&gt;&lt;ref name=&quot;2 WSJ&quot;&gt;{{cite web|last=Gage|first=Deborah|title=Platfora Founder Goes in Search of Big-Data Answers|url=http://blogs.wsj.com/venturecapital/2013/04/15/platfora-founder-goes-in-search-of-big-data-answers/|publisher=Wall Street Journal|accessdate=22 January 2014}}&lt;/ref&gt;&lt;ref name=&quot;4 Bloomberg&quot;&gt;{{cite web|last=Vance|first=Ashlee|title=Big Data for Dummies or at Least Product Managers|url=http://www.businessweek.com/articles/2013-09-12/big-data-for-dummies-or-at-least-product-managers|publisher=Bloomberg Businessweek|accessdate=22 January 2014}}&lt;/ref&gt;
== History ==

Platfora was founded in 2011 by Ben Werther. Werther studied [[computer science]] at [[Stanford University]].&lt;ref name=&quot;1 Business Insider&quot; /&gt; Prior to founding Platfora, he worked at There Inc., [[Siebel Systems]], [[Microsoft]], and [[Greenplum]].&lt;ref name=&quot;1 Business Insider&quot; /&gt;&lt;ref name=&quot;5 Information Week&quot;&gt;{{cite web|last=Henschen|first=Doug|title=Big Data's Big Picture: An Insider's View|url=http://www.informationweek.com/big-data/big-data-analytics/big-datas-big-picture-an-insiders-view/d/d-id/899326|publisher=Information Week|accessdate=22 January 2014}}&lt;/ref&gt;
In 2011, Werther met regularly with former coworkers John Eshleman and SriSatish Ambati in a café in downtown San Mateo, California.&lt;ref name=&quot;2 WSJ&quot; /&gt; At one of these meetings, while discussing the technical process of big data analysis, Werther realized that he could develop software that paired with Hadoop to greatly speed up data analysis and visualization. Werther sought financing to start Platfora; Eshleman was initially an adviser, and later joined Platfora as founding vice president of technology. Ambati formed his own company, Oxdata.&lt;ref name=&quot;2 WSJ&quot; /&gt; 

Platfora received funding from [[Andreessen Horowitz]], [[Battery Ventures]], [[Sutter Hill Ventures]], and [[In-Q-Tel]], the venture fund of the [[Central Intelligence Agency]]. Series A funding was $7.2 million. As of January, 2014, total funding stood at $27.2 million.&lt;ref name=&quot;1 Business Insider&quot; /&gt;&lt;ref name=&quot;2 WSJ&quot; /&gt;&lt;ref name=&quot;4 Bloomberg&quot; /&gt; 

Platfora is one of several new big data analytics companies that industry analysts expect to compete with established firms including [[SAP AG|SAP]], [[IBM]], [[SAS Institute|SAS]], and [[Oracle]], whose older methods of data analysis and visualization are currently more time consuming.&lt;ref&gt;{{cite web|last=Bort|first=Julie|title=How Big Data Startups Could Kill a $30 Billion Industry|url=http://www.businessinsider.com/how-new-big-data-startups-will-kill-this-30-billion-industry-2012-8|publisher=Business Insider|accessdate=22 January 2014}}&lt;/ref&gt;&lt;ref name=&quot;4 Bloomberg&quot; /&gt;&lt;ref&gt;{{cite web|last=Harris|first=Derrick|title=Visualization is the future: 6 startups re-imagining how we consume data|url=http://gigaom.com/2013/05/13/visualization-is-the-future-6-startups-re-imagining-how-we-consume-data/|publisher=Gigaom|accessdate=22 January 2014}}&lt;/ref&gt;
== Product ==

Platfora’s software works with the open-source software framework Apache Hadoop; when a user queries a database, the product delivers answers in [[real time]] via a [[graphical user interface]]. [[Bloomberg Businessweek]] called it “Big Data for Dummies.”&lt;ref name=&quot;4 Bloomberg&quot; /&gt; A corporate or government data analyst can use the interface to filter results, or drag and drop fields to create graphs, overlays, and other visualizations of the data. The analyst can then share those data visualizations and answers with others.&lt;ref name=&quot;2 WSJ&quot; /&gt;&lt;ref name=&quot;4 Bloomberg&quot; /&gt;

== Awards and recognition ==

[[CRN Magazine]] named Platfora’s software one of “The 10 Coolest Big Data Products of 2013.”&lt;ref&gt;{{cite web|last=Whiting|first=Rick|title=The 10 Coolest Big Data Products Of 2013|url=http://www.crn.com/slide-shows/applications-os/240164423/the-10-coolest-big-data-products-of-2013.htm|publisher=CRN Magazine|accessdate=22 January 2014}}&lt;/ref&gt;  Platfora’s software was also voted a winner at the Big Data Festival in Kansas City, where it was used to analyze the city’s crime data.&lt;ref&gt;{{cite web|title=Platfora and DST Win Big at Big Data Kansas City Festival|url=http://finance.yahoo.com/news/platfora-dst-win-big-big-140000848.html|publisher=Yahoo Finance|accessdate=22 January 2014}}&lt;/ref&gt;
== See also ==

* [[Big data]]
* [[Data science]]
* [[Apache Hadoop]]
* [[Map Reduce]]
* [[In-Q-Tel]]

== References ==

{{reflist}}

== External links ==

* [http://www.platfora.com Official website]
* [http://patents.justia.com/inventor/john-glenn-eshleman Patents by John Eshleman]
* [http://www.bigdatafestival.co Big Data Festival website]


[[Category:Big Data Analytics]]
[[Category:Big data]]
[[Category:Enterprise Software]]
[[Category:Software Companies based in the Bay Area]]</text>
      <sha1>ttop2173qifjjtvzyr0oowbfw1s87v5</sha1>
      <model>wikitext</model>
      <format>text/x-wiki</format>
    </revision>
  </page>
</mediawiki>
